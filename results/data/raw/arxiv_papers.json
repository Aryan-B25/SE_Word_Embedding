[
  {
    "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural\n  Chunking via Abstract Syntax Tree",
    "abstract": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.",
    "text": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural\n  Chunking via Abstract Syntax Tree Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088384
  },
  {
    "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses",
    "abstract": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.",
    "text": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0884008
  },
  {
    "title": "Uncovering Intention through LLM-Driven Code Snippet Description\n  Generation",
    "abstract": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.",
    "text": "Uncovering Intention through LLM-Driven Code Snippet Description\n  Generation Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088414
  },
  {
    "title": "Large Language Models for Unit Testing: A Systematic Literature Review",
    "abstract": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.",
    "text": "Large Language Models for Unit Testing: A Systematic Literature Review Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0884259
  },
  {
    "title": "Advanced approach for Agile/Scrum Process: RetroAI++",
    "abstract": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.",
    "text": "Advanced approach for Agile/Scrum Process: RetroAI++ In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088437
  },
  {
    "title": "Enhancement Report Approval Prediction: A Comparative Study of Large\n  Language Models",
    "abstract": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.",
    "text": "Enhancement Report Approval Prediction: A Comparative Study of Large\n  Language Models Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088449
  },
  {
    "title": "Program Feature-based Fuzzing Benchmarking",
    "abstract": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.",
    "text": "Program Feature-based Fuzzing Benchmarking Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0884638
  },
  {
    "title": "An Empirical Study of Bugs in Data Visualization Libraries",
    "abstract": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.",
    "text": "An Empirical Study of Bugs in Data Visualization Libraries Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088473
  },
  {
    "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
    "abstract": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "text": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088484
  },
  {
    "title": "Unified Software Engineering agent as AI Software Engineer",
    "abstract": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
    "text": "Unified Software Engineering agent as AI Software Engineer The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088495
  },
  {
    "title": "Issue Retrieval and Verification Enhanced Supplementary Code Comment\n  Generation",
    "abstract": "Issue reports have been recognized to contain rich information for\nretrieval-augmented code comment generation. However, how to minimize\nhallucinations in the generated comments remains significant challenges. In\nthis paper, we propose IsComment, an issue-based LLM retrieval and verification\napproach for generating method's design rationale, usage directives, and so on\nas supplementary code comments. We first identify five main types of code\nsupplementary information that issue reports can provide through\ncode-comment-issue analysis. Next, we retrieve issue sentences containing these\ntypes of supplementary information and generate candidate code comments. To\nreduce hallucinations, we filter out those candidate comments that are\nirrelevant to the code or unverifiable by the issue report, making the code\ncomment generation results more reliable. Our experiments indicate that\ncompared with LLMs, IsComment increases the coverage of manual supplementary\ncomments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and\nfrom 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can\ngenerate richer and more useful supplementary code comments for programming\nunderstanding, which is quantitatively evaluated through the MESIA metric on\nboth methods with and without manual code comments.",
    "text": "Issue Retrieval and Verification Enhanced Supplementary Code Comment\n  Generation Issue reports have been recognized to contain rich information for\nretrieval-augmented code comment generation. However, how to minimize\nhallucinations in the generated comments remains significant challenges. In\nthis paper, we propose IsComment, an issue-based LLM retrieval and verification\napproach for generating method's design rationale, usage directives, and so on\nas supplementary code comments. We first identify five main types of code\nsupplementary information that issue reports can provide through\ncode-comment-issue analysis. Next, we retrieve issue sentences containing these\ntypes of supplementary information and generate candidate code comments. To\nreduce hallucinations, we filter out those candidate comments that are\nirrelevant to the code or unverifiable by the issue report, making the code\ncomment generation results more reliable. Our experiments indicate that\ncompared with LLMs, IsComment increases the coverage of manual supplementary\ncomments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and\nfrom 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can\ngenerate richer and more useful supplementary code comments for programming\nunderstanding, which is quantitatively evaluated through the MESIA metric on\nboth methods with and without manual code comments.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088505
  },
  {
    "title": "Navigating the growing field of research on AI for software testing --\n  the taxonomy for AI-augmented software testing and an ontology-driven\n  literature survey",
    "abstract": "In industry, software testing is the primary method to verify and validate\nthe functionality, performance, security, usability, and so on, of\nsoftware-based systems. Test automation has gained increasing attention in\nindustry over the last decade, following decades of intense research into test\nautomation and model-based testing. However, designing, developing, maintaining\nand evolving test automation is a considerable effort. Meanwhile, AI's\nbreakthroughs in many engineering fields are opening up new perspectives for\nsoftware testing, for both manual and automated testing. This paper reviews\nrecent research on AI augmentation in software test automation, from no\nautomation to full automation. It also discusses new forms of testing made\npossible by AI. Based on this, the newly developed taxonomy, ai4st, is\npresented and used to classify recent research and identify open research\nquestions.",
    "text": "Navigating the growing field of research on AI for software testing --\n  the taxonomy for AI-augmented software testing and an ontology-driven\n  literature survey In industry, software testing is the primary method to verify and validate\nthe functionality, performance, security, usability, and so on, of\nsoftware-based systems. Test automation has gained increasing attention in\nindustry over the last decade, following decades of intense research into test\nautomation and model-based testing. However, designing, developing, maintaining\nand evolving test automation is a considerable effort. Meanwhile, AI's\nbreakthroughs in many engineering fields are opening up new perspectives for\nsoftware testing, for both manual and automated testing. This paper reviews\nrecent research on AI augmentation in software test automation, from no\nautomation to full automation. It also discusses new forms of testing made\npossible by AI. Based on this, the newly developed taxonomy, ai4st, is\npresented and used to classify recent research and identify open research\nquestions.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088515
  },
  {
    "title": "ACM Survey Draft on Formalising Software Requirements with Large\n  Language Models",
    "abstract": "This draft is a working document, having a summary of nighty-four (94) papers\nwith additional sections on Traceability of Software Requirements (Section 4),\nFormal Methods and Its Tools (Section 5), Unifying Theories of Programming\n(UTP) and Theory of Institutions (Section 6). Please refer to abstract of\n[7,8]. Key difference of this draft from our recently anticipated ones with\nsimilar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:\n  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted\non 18th of March, 2025, it went through the light-weight blind review and\naccepted for poster presentation. Conference was held on 15th of May, 2025.\n  [8] is a nine page paper with additional nine pages of references and summary\ntables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April,\n2025. It went through rigorous review process. The uploaded version on\narXiv.org [8] is the improved one of the submission, after addressing the\nspecific suggestions to improve the paper.",
    "text": "ACM Survey Draft on Formalising Software Requirements with Large\n  Language Models This draft is a working document, having a summary of nighty-four (94) papers\nwith additional sections on Traceability of Software Requirements (Section 4),\nFormal Methods and Its Tools (Section 5), Unifying Theories of Programming\n(UTP) and Theory of Institutions (Section 6). Please refer to abstract of\n[7,8]. Key difference of this draft from our recently anticipated ones with\nsimilar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:\n  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted\non 18th of March, 2025, it went through the light-weight blind review and\naccepted for poster presentation. Conference was held on 15th of May, 2025.\n  [8] is a nine page paper with additional nine pages of references and summary\ntables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April,\n2025. It went through rigorous review process. The uploaded version on\narXiv.org [8] is the improved one of the submission, after addressing the\nspecific suggestions to improve the paper.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088525
  },
  {
    "title": "Low-code to fight climate change: the Climaborough project",
    "abstract": "The EU-funded Climaborough project supports European cities to achieve carbon\nneutrality by 2030. Eleven cities in nine countries will deploy in real\nconditions products and services fostering climate transition in their local\nenvironment. The Climaborough City Platform is being developed to monitor the\ncities' overall progress towards their climate goals by aggregating historic\nand real-time data and displaying the results in user-friendly dashboards that\nwill be used by non-technical experts to evaluate the effectiveness of local\nexperimental initiatives, identify those that yield significant impact, and\nassess the potential consequences of scaling them up to a broader level. In\nthis paper, we explain how we have put in place a low-code/no-code strategy in\nClimaborough in response to the project's aim to quickly deploy climate\ndashboards. A low-code strategy is used to accelerate the development of the\ndashboards. The dashboards embed a no-code philosophy that enables all types of\ncitizen profiles to configure and adapt the dashboard to their specific needs.",
    "text": "Low-code to fight climate change: the Climaborough project The EU-funded Climaborough project supports European cities to achieve carbon\nneutrality by 2030. Eleven cities in nine countries will deploy in real\nconditions products and services fostering climate transition in their local\nenvironment. The Climaborough City Platform is being developed to monitor the\ncities' overall progress towards their climate goals by aggregating historic\nand real-time data and displaying the results in user-friendly dashboards that\nwill be used by non-technical experts to evaluate the effectiveness of local\nexperimental initiatives, identify those that yield significant impact, and\nassess the potential consequences of scaling them up to a broader level. In\nthis paper, we explain how we have put in place a low-code/no-code strategy in\nClimaborough in response to the project's aim to quickly deploy climate\ndashboards. A low-code strategy is used to accelerate the development of the\ndashboards. The dashboards embed a no-code philosophy that enables all types of\ncitizen profiles to configure and adapt the dashboard to their specific needs.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088535
  },
  {
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "abstract": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "text": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088546
  },
  {
    "title": "Automatic Qiskit Code Refactoring Using Large Language Models",
    "abstract": "As quantum software frameworks evolve, developers face increasing challenges\nin maintaining compatibility with rapidly changing APIs. In this work, we\npresent a novel methodology for refactoring Qiskit code using large language\nmodels (LLMs). We begin by extracting a taxonomy of migration scenarios from\nthe different sources of official Qiskit documentation (such as release notes),\ncapturing common patterns such as migration of functionality to different\nmodules and deprecated usage. This taxonomy, along with the original Python\nsource code, is provided as input to an LLM, which is then tasked with\nidentifying instances of migration scenarios in the code and suggesting\nappropriate refactoring solutions. Our approach is designed to address the\ncontext length limitations of current LLMs by structuring the input and\nreasoning process in a targeted, efficient manner. The results demonstrate that\nLLMs, when guided by domain-specific migration knowledge, can effectively\nassist in automating Qiskit code migration. This work contributes both a set of\nproven prompts and taxonomy for Qiskit code migration from earlier versions to\nversion 0.46 and a methodology to asses the capabilities of LLMs to assist in\nthe migration of quantum code.",
    "text": "Automatic Qiskit Code Refactoring Using Large Language Models As quantum software frameworks evolve, developers face increasing challenges\nin maintaining compatibility with rapidly changing APIs. In this work, we\npresent a novel methodology for refactoring Qiskit code using large language\nmodels (LLMs). We begin by extracting a taxonomy of migration scenarios from\nthe different sources of official Qiskit documentation (such as release notes),\ncapturing common patterns such as migration of functionality to different\nmodules and deprecated usage. This taxonomy, along with the original Python\nsource code, is provided as input to an LLM, which is then tasked with\nidentifying instances of migration scenarios in the code and suggesting\nappropriate refactoring solutions. Our approach is designed to address the\ncontext length limitations of current LLMs by structuring the input and\nreasoning process in a targeted, efficient manner. The results demonstrate that\nLLMs, when guided by domain-specific migration knowledge, can effectively\nassist in automating Qiskit code migration. This work contributes both a set of\nproven prompts and taxonomy for Qiskit code migration from earlier versions to\nversion 0.46 and a methodology to asses the capabilities of LLMs to assist in\nthe migration of quantum code.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088556
  },
  {
    "title": "AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph\n  Representations on Code Clone Detection",
    "abstract": "As one of the most detrimental code smells, code clones significantly\nincrease software maintenance costs and heighten vulnerability risks, making\ntheir detection a critical challenge in software engineering. Abstract Syntax\nTrees (ASTs) dominate deep learning-based code clone detection due to their\nprecise syntactic structure representation, but they inherently lack semantic\ndepth. Recent studies address this by enriching AST-based representations with\nsemantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs\n(DFGs). However, the effectiveness of various enriched AST-based\nrepresentations and their compatibility with different graph-based machine\nlearning techniques remains an open question, warranting further investigation\nto unlock their full potential in addressing the complexities of code clone\ndetection. In this paper, we present a comprehensive empirical study to\nrigorously evaluate the effectiveness of AST-based hybrid graph representations\nin Graph Neural Network (GNN)-based code clone detection. We systematically\ncompare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs\n(FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid\nrepresentations impact GNNs differently: while AST+CFG+DFG consistently\nenhances accuracy for convolution- and attention-based models (Graph\nConvolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST\nfrequently introduces structural complexity that harms performance. Notably,\nGMN outperforms others even with standard AST representations, highlighting its\nsuperior cross-code similarity detection and reducing the need for enriched\nstructures.",
    "text": "AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph\n  Representations on Code Clone Detection As one of the most detrimental code smells, code clones significantly\nincrease software maintenance costs and heighten vulnerability risks, making\ntheir detection a critical challenge in software engineering. Abstract Syntax\nTrees (ASTs) dominate deep learning-based code clone detection due to their\nprecise syntactic structure representation, but they inherently lack semantic\ndepth. Recent studies address this by enriching AST-based representations with\nsemantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs\n(DFGs). However, the effectiveness of various enriched AST-based\nrepresentations and their compatibility with different graph-based machine\nlearning techniques remains an open question, warranting further investigation\nto unlock their full potential in addressing the complexities of code clone\ndetection. In this paper, we present a comprehensive empirical study to\nrigorously evaluate the effectiveness of AST-based hybrid graph representations\nin Graph Neural Network (GNN)-based code clone detection. We systematically\ncompare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs\n(FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid\nrepresentations impact GNNs differently: while AST+CFG+DFG consistently\nenhances accuracy for convolution- and attention-based models (Graph\nConvolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST\nfrequently introduces structural complexity that harms performance. Notably,\nGMN outperforms others even with standard AST representations, highlighting its\nsuperior cross-code similarity detection and reducing the need for enriched\nstructures.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088567
  },
  {
    "title": "Varanus: Runtime Verification for CSP",
    "abstract": "Autonomous systems are often used in changeable and unknown environments,\nwhere traditional verification may not be suitable. Runtime Verification (RV)\nchecks events performed by a system against a formal specification of its\nintended behaviour, making it highly suitable for ensuring that an autonomous\nsystem is obeying its specification at runtime. Communicating Sequential\nProcesses (CSP) is a process algebra usually used in static verification, which\ncaptures behaviour as a trace of events, making it useful for RV as well.\nFurther, CSP has more recently been used to specify autonomous and robotic\nsystems. Though CSP is supported by two extant model checkers, so far it has no\nRV tool. This paper presents Varanus, an RV tool that monitors a system against\nan oracle built from a CSP specification. This approach enables the reuse\nwithout modifications of a specification that was built, e.g during the\nsystem's design. We describe the tool, apply it to a simulated autonomous\nrobotic rover inspecting a nuclear waste store, empirically comparing its\nperformance to two other RV tools using different languages, and demonstrate\nhow it can detect violations of the specification. Varanus can synthesise a\nmonitor from a CSP process in roughly linear time, with respect to the number\nof states and transitions in the model; and checks each event in roughly\nconstant time.",
    "text": "Varanus: Runtime Verification for CSP Autonomous systems are often used in changeable and unknown environments,\nwhere traditional verification may not be suitable. Runtime Verification (RV)\nchecks events performed by a system against a formal specification of its\nintended behaviour, making it highly suitable for ensuring that an autonomous\nsystem is obeying its specification at runtime. Communicating Sequential\nProcesses (CSP) is a process algebra usually used in static verification, which\ncaptures behaviour as a trace of events, making it useful for RV as well.\nFurther, CSP has more recently been used to specify autonomous and robotic\nsystems. Though CSP is supported by two extant model checkers, so far it has no\nRV tool. This paper presents Varanus, an RV tool that monitors a system against\nan oracle built from a CSP specification. This approach enables the reuse\nwithout modifications of a specification that was built, e.g during the\nsystem's design. We describe the tool, apply it to a simulated autonomous\nrobotic rover inspecting a nuclear waste store, empirically comparing its\nperformance to two other RV tools using different languages, and demonstrate\nhow it can detect violations of the specification. Varanus can synthesise a\nmonitor from a CSP process in roughly linear time, with respect to the number\nof states and transitions in the model; and checks each event in roughly\nconstant time.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088577
  },
  {
    "title": "Defining the Game Producer: A Mapping of Key Characteristics and\n  Differentiators of the Professional Behind Digital Game Production",
    "abstract": "Introduction: As digital games grow in complexity, the role of the Game\nProducer becomes increasingly relevant for aligning creative, technical, and\nbusiness dimensions. Objective: This study aimed to identify and map the main\ncharacteristics, skills, and competencies that define the Digital Game Producer\nprofile. Methodology: A qualitative investigation was conducted with 11\nsemi-structured interviews, analyzed through Grounded Theory to build\ncategories grounded in professional practice. Results: The study produced a\nstructured set of personal characteristics, practical skills, and strategic\ncompetencies considered essential for Game Producers. Communication,\nadaptability, and project management emerged as central elements across the\nsample. Conclusion: The resulting model offers a foundation for professional\ntraining, recruitment strategies, and future research on leadership roles in\ngame development.",
    "text": "Defining the Game Producer: A Mapping of Key Characteristics and\n  Differentiators of the Professional Behind Digital Game Production Introduction: As digital games grow in complexity, the role of the Game\nProducer becomes increasingly relevant for aligning creative, technical, and\nbusiness dimensions. Objective: This study aimed to identify and map the main\ncharacteristics, skills, and competencies that define the Digital Game Producer\nprofile. Methodology: A qualitative investigation was conducted with 11\nsemi-structured interviews, analyzed through Grounded Theory to build\ncategories grounded in professional practice. Results: The study produced a\nstructured set of personal characteristics, practical skills, and strategic\ncompetencies considered essential for Game Producers. Communication,\nadaptability, and project management emerged as central elements across the\nsample. Conclusion: The resulting model offers a foundation for professional\ntraining, recruitment strategies, and future research on leadership roles in\ngame development.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088587
  },
  {
    "title": "Agile and Student-Centred Teaching of Agile/Scrum Concepts",
    "abstract": "In this paper, we discuss our experience in designing and teaching a course\non Software Engineering Project Management, where the focus is on Agile/Scrum\ndevelopment and Requirement Engineering activities. The course has undergone\nfundamental changes since 2020 to make the teaching approach more\nstudent-centred and flexible. As many universities abandoned having\nface-to-face exams at the end of the semester, authentic assessments now play\nan even more important role than before. This makes assessment of students'\nwork even more challenging, especially if we are dealing with large cohorts of\nstudents. The complexity is not only in dealing with diversity in the student\ncohorts when elaborating the assessment tasks, but also in being able to\nprovide feedback and marks in a timely and fairly. We report our lessons\nlearned, which might provide useful insights for teaching Agile/Scrum concepts\nto undergraduate and postgraduate students. We also analyse what course\nstructure might be effective to support a blended learning approach, as well as\nwhat could be a reasonable structure of online assessments, to keep them both\nauthentic and scalable for large cohorts of students.",
    "text": "Agile and Student-Centred Teaching of Agile/Scrum Concepts In this paper, we discuss our experience in designing and teaching a course\non Software Engineering Project Management, where the focus is on Agile/Scrum\ndevelopment and Requirement Engineering activities. The course has undergone\nfundamental changes since 2020 to make the teaching approach more\nstudent-centred and flexible. As many universities abandoned having\nface-to-face exams at the end of the semester, authentic assessments now play\nan even more important role than before. This makes assessment of students'\nwork even more challenging, especially if we are dealing with large cohorts of\nstudents. The complexity is not only in dealing with diversity in the student\ncohorts when elaborating the assessment tasks, but also in being able to\nprovide feedback and marks in a timely and fairly. We report our lessons\nlearned, which might provide useful insights for teaching Agile/Scrum concepts\nto undergraduate and postgraduate students. We also analyse what course\nstructure might be effective to support a blended learning approach, as well as\nwhat could be a reasonable structure of online assessments, to keep them both\nauthentic and scalable for large cohorts of students.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0885959
  },
  {
    "title": "Quality Assessment of Python Tests Generated by Large Language Models",
    "abstract": "The manual generation of test scripts is a time-intensive, costly, and\nerror-prone process, indicating the value of automated solutions. Large\nLanguage Models (LLMs) have shown great promise in this domain, leveraging\ntheir extensive knowledge to produce test code more efficiently. This study\ninvestigates the quality of Python test code generated by three LLMs: GPT-4o,\nAmazon Q, and LLama 3.3. We evaluate the structural reliability of test suites\ngenerated under two distinct prompt contexts: Text2Code (T2C) and Code2Code\n(C2C). Our analysis includes the identification of errors and test smells, with\na focus on correlating these issues to inadequate design patterns. Our findings\nreveal that most test suites generated by the LLMs contained at least one error\nor test smell. Assertion errors were the most common, comprising 64% of all\nidentified errors, while the test smell Lack of Cohesion of Test Cases was the\nmost frequently detected (41%). Prompt context significantly influenced test\nquality; textual prompts with detailed instructions often yielded tests with\nfewer errors but a higher incidence of test smells. Among the evaluated LLMs,\nGPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C),\nwhereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For\ntest smells, Amazon Q had fewer detections in the C2C context (9%), while LLama\n3.3 performed best in the T2C context (10%). Additionally, we observed a strong\nrelationship between specific errors, such as assertion or indentation issues,\nand test case cohesion smells. These findings demonstrate opportunities for\nimproving the quality of test generation by LLMs and highlight the need for\nfuture research to explore optimized generation scenarios and better prompt\nengineering strategies.",
    "text": "Quality Assessment of Python Tests Generated by Large Language Models The manual generation of test scripts is a time-intensive, costly, and\nerror-prone process, indicating the value of automated solutions. Large\nLanguage Models (LLMs) have shown great promise in this domain, leveraging\ntheir extensive knowledge to produce test code more efficiently. This study\ninvestigates the quality of Python test code generated by three LLMs: GPT-4o,\nAmazon Q, and LLama 3.3. We evaluate the structural reliability of test suites\ngenerated under two distinct prompt contexts: Text2Code (T2C) and Code2Code\n(C2C). Our analysis includes the identification of errors and test smells, with\na focus on correlating these issues to inadequate design patterns. Our findings\nreveal that most test suites generated by the LLMs contained at least one error\nor test smell. Assertion errors were the most common, comprising 64% of all\nidentified errors, while the test smell Lack of Cohesion of Test Cases was the\nmost frequently detected (41%). Prompt context significantly influenced test\nquality; textual prompts with detailed instructions often yielded tests with\nfewer errors but a higher incidence of test smells. Among the evaluated LLMs,\nGPT-4o produced the fewest errors in both contexts (10% in C2C and 6% in T2C),\nwhereas Amazon Q had the highest error rates (19% in C2C and 28% in T2C). For\ntest smells, Amazon Q had fewer detections in the C2C context (9%), while LLama\n3.3 performed best in the T2C context (10%). Additionally, we observed a strong\nrelationship between specific errors, such as assertion or indentation issues,\nand test case cohesion smells. These findings demonstrate opportunities for\nimproving the quality of test generation by LLMs and highlight the need for\nfuture research to explore optimized generation scenarios and better prompt\nengineering strategies.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088607
  },
  {
    "title": "Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity\n  Effects",
    "abstract": "The primary goal of bug prediction is to optimize testing efforts by focusing\non software fragments, i.e., classes, methods, commits (JIT), or lines of code,\nmost likely to be buggy. However, these predicted fragments already contain\nbugs. Thus, the current bug prediction approaches support fixing rather than\nprevention. The aim of this paper is to introduce and evaluate Ticket-Level\nPrediction (TLP), an approach to identify tickets that will introduce bugs once\nimplemented. We analyze TLP at three temporal points, each point represents a\nticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1)\nTLP accuracy increases as tickets progress towards the closed stage due to\nimproved feature reliability over time, and (2) the predictive power of\nfeatures changes across these temporal points. Our TLP approach leverages 72\nfeatures belonging to six different families: code, developer, external\ntemperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our\nTLP evaluation uses a sliding-window approach, balancing feature selection and\nthree machine-learning bug prediction classifiers on about 10,000 tickets of\ntwo Apache open-source projects. Our results show that TLP accuracy increases\nwith proximity, confirming the expected trade-off between early prediction and\naccuracy. Regarding the prediction power of feature families, no single feature\nfamily dominates across stages; developer-centric signals are most informative\nearly, whereas code and JIT metrics prevail near closure, and temperature-based\nfeatures provide complementary value throughout. Our findings complement and\nextend the literature on bug prediction at the class, method, or commit level\nby showing that defect prediction can be effectively moved upstream, offering\nopportunities for risk-aware ticket triaging and developer assignment before\nany code is written.",
    "text": "Anticipating Bugs: Ticket-Level Bug Prediction and Temporal Proximity\n  Effects The primary goal of bug prediction is to optimize testing efforts by focusing\non software fragments, i.e., classes, methods, commits (JIT), or lines of code,\nmost likely to be buggy. However, these predicted fragments already contain\nbugs. Thus, the current bug prediction approaches support fixing rather than\nprevention. The aim of this paper is to introduce and evaluate Ticket-Level\nPrediction (TLP), an approach to identify tickets that will introduce bugs once\nimplemented. We analyze TLP at three temporal points, each point represents a\nticket lifecycle stage: Open, In Progress, or Closed. We conjecture that: (1)\nTLP accuracy increases as tickets progress towards the closed stage due to\nimproved feature reliability over time, and (2) the predictive power of\nfeatures changes across these temporal points. Our TLP approach leverages 72\nfeatures belonging to six different families: code, developer, external\ntemperature, internal temperature, intrinsic, ticket to tickets, and JIT. Our\nTLP evaluation uses a sliding-window approach, balancing feature selection and\nthree machine-learning bug prediction classifiers on about 10,000 tickets of\ntwo Apache open-source projects. Our results show that TLP accuracy increases\nwith proximity, confirming the expected trade-off between early prediction and\naccuracy. Regarding the prediction power of feature families, no single feature\nfamily dominates across stages; developer-centric signals are most informative\nearly, whereas code and JIT metrics prevail near closure, and temperature-based\nfeatures provide complementary value throughout. Our findings complement and\nextend the literature on bug prediction at the class, method, or commit level\nby showing that defect prediction can be effectively moved upstream, offering\nopportunities for risk-aware ticket triaging and developer assignment before\nany code is written.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088618
  },
  {
    "title": "Designing a Custom Chaos Engineering Framework for Enhanced System\n  Resilience at Softtech",
    "abstract": "Chaos Engineering is a discipline which enhances software resilience by\nintroducing faults to observe and improve system behavior intentionally. This\npaper presents a design proposal for a customized Chaos Engineering framework\ntailored for Softtech, a leading software development company serving the\nfinancial sector. It outlines foundational concepts and activities for\nintroducing Chaos Engineering within Softtech, while considering financial\nsector regulations. Building on these principles, the framework aims to be\niterative and scalable, enabling development teams to progressively improve\ntheir practices. The study addresses two primary questions: how Softtech's\nunique infrastructure, business priorities, and organizational context shape\nthe customization of its Chaos Engineering framework and what key activities\nand components are necessary for creating an effective framework tailored to\nSofttech's needs.",
    "text": "Designing a Custom Chaos Engineering Framework for Enhanced System\n  Resilience at Softtech Chaos Engineering is a discipline which enhances software resilience by\nintroducing faults to observe and improve system behavior intentionally. This\npaper presents a design proposal for a customized Chaos Engineering framework\ntailored for Softtech, a leading software development company serving the\nfinancial sector. It outlines foundational concepts and activities for\nintroducing Chaos Engineering within Softtech, while considering financial\nsector regulations. Building on these principles, the framework aims to be\niterative and scalable, enabling development teams to progressively improve\ntheir practices. The study addresses two primary questions: how Softtech's\nunique infrastructure, business priorities, and organizational context shape\nthe customization of its Chaos Engineering framework and what key activities\nand components are necessary for creating an effective framework tailored to\nSofttech's needs.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0886269
  },
  {
    "title": "The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity,\n  and Inclusion in Software Engineering",
    "abstract": "Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top\npriority for leading software companies. However, in a short period, a wave of\nbacklash has led many firms to re-assess their DEI strategies. Responding to\nthis DEI backlash is crucial in academic research, especially because,\ncurrently, little scholarly research has been done on it. In this paper,\ntherefore, we have set forth the following research question (RQ): \"How have\nleading software companies changed their DEI strategies in recent years?\" Given\nthe novelty of the RQ and, consequently, the lack of scholarly research on it,\nwe are conducting a grey literature study, examining the current state of DEI\ninitiatives in 10 leading software companies. Based on our analysis, we have\nclassified companies into categories based on their shift in commitment to DEI.\nWe can identify that companies are indeed responding to the backlash by\nrethinking their strategy, either by reducing, increasing, or renaming their\nDEI initiatives. In contrast, some companies keep on with their DEI strategy,\nat least so far, despite the challenging political climate. To illustrate these\nchanges, we introduce the DEI Universe Map, a visual representation of software\nindustry trends in DEI commitment and actions.",
    "text": "The Tech DEI Backlash -- The Changing Landscape of Diversity, Equity,\n  and Inclusion in Software Engineering Not long ago, Diversity, Equity, and Inclusion (DEI) initiatives were a top\npriority for leading software companies. However, in a short period, a wave of\nbacklash has led many firms to re-assess their DEI strategies. Responding to\nthis DEI backlash is crucial in academic research, especially because,\ncurrently, little scholarly research has been done on it. In this paper,\ntherefore, we have set forth the following research question (RQ): \"How have\nleading software companies changed their DEI strategies in recent years?\" Given\nthe novelty of the RQ and, consequently, the lack of scholarly research on it,\nwe are conducting a grey literature study, examining the current state of DEI\ninitiatives in 10 leading software companies. Based on our analysis, we have\nclassified companies into categories based on their shift in commitment to DEI.\nWe can identify that companies are indeed responding to the backlash by\nrethinking their strategy, either by reducing, increasing, or renaming their\nDEI initiatives. In contrast, some companies keep on with their DEI strategy,\nat least so far, despite the challenging political climate. To illustrate these\nchanges, we introduce the DEI Universe Map, a visual representation of software\nindustry trends in DEI commitment and actions.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0886421
  },
  {
    "title": "Mobile Application Review Summarization using Chain of Density Prompting",
    "abstract": "Mobile app users commonly rely on app store ratings and reviews to find apps\nthat suit their needs. However, the sheer volume of reviews available on app\nstores can lead to information overload, thus impeding users' ability to make\ninformed app selection decisions. To address this challenge, we leverage Large\nLanguage Models (LLMs) to summarize mobile app reviews. In particular, we use\nthe Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate\nabstractive, semantically dense, and easily interpretable summaries of mobile\napp reviews. The CoD prompt is engineered to iteratively extract salient\nentities from the source text and fuse them into a fixed-length summary. We\nevaluate the performance of our approach using a large dataset of mobile app\nreviews. We further conduct an empirical evaluation with 48 study participants\nto assess the readability of the generated summaries. Our results demonstrate\nthat adapting the CoD prompt to focus on app features improves its ability to\nextract key themes from user reviews and generate natural language summaries\ntailored for end-user consumption. The prompt also manages to maintain the\nreadability of the generated summaries while increasing their semantic density.\nOur work in this paper aims to improve mobile app users' experience by\nproviding an effective mechanism for summarizing important user feedback in the\nreview stream.",
    "text": "Mobile Application Review Summarization using Chain of Density Prompting Mobile app users commonly rely on app store ratings and reviews to find apps\nthat suit their needs. However, the sheer volume of reviews available on app\nstores can lead to information overload, thus impeding users' ability to make\ninformed app selection decisions. To address this challenge, we leverage Large\nLanguage Models (LLMs) to summarize mobile app reviews. In particular, we use\nthe Chain of Density (CoD) prompt to guide OpenAI GPT-4 to generate\nabstractive, semantically dense, and easily interpretable summaries of mobile\napp reviews. The CoD prompt is engineered to iteratively extract salient\nentities from the source text and fuse them into a fixed-length summary. We\nevaluate the performance of our approach using a large dataset of mobile app\nreviews. We further conduct an empirical evaluation with 48 study participants\nto assess the readability of the generated summaries. Our results demonstrate\nthat adapting the CoD prompt to focus on app features improves its ability to\nextract key themes from user reviews and generate natural language summaries\ntailored for end-user consumption. The prompt also manages to maintain the\nreadability of the generated summaries while increasing their semantic density.\nOur work in this paper aims to improve mobile app users' experience by\nproviding an effective mechanism for summarizing important user feedback in the\nreview stream.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0886521
  },
  {
    "title": "A Quantum Annealing Approach for Solving Optimal Feature Selection and\n  Next Release Problems",
    "abstract": "Search-based software engineering (SBSE) addresses critical optimization\nchallenges in software engineering, including the next release problem (NRP)\nand feature selection problem (FSP). While traditional heuristic approaches and\ninteger linear programming (ILP) methods have demonstrated efficacy for small\nto medium-scale problems, their scalability to large-scale instances remains\nunknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling\nmulti-objective SBSE problems, leveraging the computational potential of\nquantum systems. We propose two QA-based algorithms tailored to different\nproblem scales. For small-scale problems, we reformulate multi-objective\noptimization (MOO) as single-objective optimization (SOO) using penalty-based\nmappings for quantum processing. For large-scale problems, we employ a\ndecomposition strategy guided by maximum energy impact (MEI), integrating QA\nwith a steepest descent method to enhance local search efficiency. Applied to\nNRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and\nthe ILP-based $\\epsilon$-constraint method. Experimental results reveal that\nwhile our methods produce fewer non-dominated solutions than\n$\\epsilon$-constraint, they achieve significant reductions in execution time.\nMoreover, compared to NSGA-II, our methods deliver more non-dominated solutions\nwith superior computational efficiency. These findings underscore the potential\nof QA in advancing scalable and efficient solutions for SBSE challenges.",
    "text": "A Quantum Annealing Approach for Solving Optimal Feature Selection and\n  Next Release Problems Search-based software engineering (SBSE) addresses critical optimization\nchallenges in software engineering, including the next release problem (NRP)\nand feature selection problem (FSP). While traditional heuristic approaches and\ninteger linear programming (ILP) methods have demonstrated efficacy for small\nto medium-scale problems, their scalability to large-scale instances remains\nunknown. Here, we introduce quantum annealing (QA) as a subroutine to tackling\nmulti-objective SBSE problems, leveraging the computational potential of\nquantum systems. We propose two QA-based algorithms tailored to different\nproblem scales. For small-scale problems, we reformulate multi-objective\noptimization (MOO) as single-objective optimization (SOO) using penalty-based\nmappings for quantum processing. For large-scale problems, we employ a\ndecomposition strategy guided by maximum energy impact (MEI), integrating QA\nwith a steepest descent method to enhance local search efficiency. Applied to\nNRP and FSP, our approaches are benchmarked against the heuristic NSGA-II and\nthe ILP-based $\\epsilon$-constraint method. Experimental results reveal that\nwhile our methods produce fewer non-dominated solutions than\n$\\epsilon$-constraint, they achieve significant reductions in execution time.\nMoreover, compared to NSGA-II, our methods deliver more non-dominated solutions\nwith superior computational efficiency. These findings underscore the potential\nof QA in advancing scalable and efficient solutions for SBSE challenges.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088663
  },
  {
    "title": "Characterising Bugs in Jupyter Platform",
    "abstract": "As a representative literate programming platform, Jupyter is widely adopted\nby developers, data analysts, and researchers for replication, data sharing,\ndocumentation, interactive data visualization, and more. Understanding the bugs\nin the Jupyter platform is essential for ensuring its correctness, security,\nand robustness. Previous studies focused on code reuse, restoration, and repair\nexecution environment for Jupyter notebooks. However, the bugs in Jupyter\nnotebooks' hosting platform Jupyter are not investigated. In this paper, we\ninvestigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified\ninto 11 root causes and 11 bug symptoms. We identify 14 major findings for\ndevelopers. More importantly, our study opens new directions in building tools\nfor detecting and fixing bugs in the Jupyter platform.",
    "text": "Characterising Bugs in Jupyter Platform As a representative literate programming platform, Jupyter is widely adopted\nby developers, data analysts, and researchers for replication, data sharing,\ndocumentation, interactive data visualization, and more. Understanding the bugs\nin the Jupyter platform is essential for ensuring its correctness, security,\nand robustness. Previous studies focused on code reuse, restoration, and repair\nexecution environment for Jupyter notebooks. However, the bugs in Jupyter\nnotebooks' hosting platform Jupyter are not investigated. In this paper, we\ninvestigate 387 bugs in the Jupyter platform. These Jupyter bugs are classified\ninto 11 root causes and 11 bug symptoms. We identify 14 major findings for\ndevelopers. More importantly, our study opens new directions in building tools\nfor detecting and fixing bugs in the Jupyter platform.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088672
  },
  {
    "title": "TUM Teleoperation: Open Source Software for Remote Driving and\n  Assistance of Automated Vehicles",
    "abstract": "Teleoperation is a key enabler for future mobility, supporting Automated\nVehicles in rare and complex scenarios beyond the capabilities of their\nautomation. Despite ongoing research, no open source software currently\ncombines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance\nthrough high-level interaction with automated driving software modules, and\nintegration with a real-world vehicle for practical testing. To address this\ngap, we present a modular, open source teleoperation software stack that can\ninteract with an automated driving software, e.g., Autoware, enabling Remote\nAssistance and Remote Driving. The software featuresstandardized interfaces for\nseamless integration with various real-world and simulation platforms, while\nallowing for flexible design of the human-machine interface. The system is\ndesigned for modularity and ease of extension, serving as a foundation for\ncollaborative development on individual software components as well as\nrealistic testing and user studies. To demonstrate the applicability of our\nsoftware, we evaluated the latency and performance of different vehicle\nplatforms in simulation and real-world. The source code is available on GitHub",
    "text": "TUM Teleoperation: Open Source Software for Remote Driving and\n  Assistance of Automated Vehicles Teleoperation is a key enabler for future mobility, supporting Automated\nVehicles in rare and complex scenarios beyond the capabilities of their\nautomation. Despite ongoing research, no open source software currently\ncombines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance\nthrough high-level interaction with automated driving software modules, and\nintegration with a real-world vehicle for practical testing. To address this\ngap, we present a modular, open source teleoperation software stack that can\ninteract with an automated driving software, e.g., Autoware, enabling Remote\nAssistance and Remote Driving. The software featuresstandardized interfaces for\nseamless integration with various real-world and simulation platforms, while\nallowing for flexible design of the human-machine interface. The system is\ndesigned for modularity and ease of extension, serving as a foundation for\ncollaborative development on individual software components as well as\nrealistic testing and user studies. To demonstrate the applicability of our\nsoftware, we evaluated the latency and performance of different vehicle\nplatforms in simulation and real-world. The source code is available on GitHub",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088683
  },
  {
    "title": "How Does LLM Reasoning Work for Code? A Survey and a Call to Action",
    "abstract": "The rise of large language models (LLMs) has led to dramatic improvements\nacross a wide range of natural language tasks. These advancements have extended\ninto the domain of code, facilitating complex tasks such as code generation,\ntranslation, summarization, and repair. However, their utility for real-world\ndeployment in-the-wild has only recently been studied, particularly on software\nengineering (SWE) tasks such as GitHub issue resolution. In this study, we\nexamine the code reasoning techniques that underlie the ability to perform such\ntasks, and examine the paradigms used to drive their performance. Our\ncontributions in this paper are: (1) the first dedicated survey on code\nreasoning for code tasks, highlighting overarching strategies, hybrid and\nagentic approaches; (2) a taxonomy of various techniques used to drive code\nreasoning; (3) a comprehensive overview of performance on common benchmarks and\na showcase of new, under-explored benchmarks with high potential in SWE; (4) an\nexploration on how core properties of code can be used to explain different\nreasoning techniques; and (5) gaps and potentially under-explored areas for\nfuture research.",
    "text": "How Does LLM Reasoning Work for Code? A Survey and a Call to Action The rise of large language models (LLMs) has led to dramatic improvements\nacross a wide range of natural language tasks. These advancements have extended\ninto the domain of code, facilitating complex tasks such as code generation,\ntranslation, summarization, and repair. However, their utility for real-world\ndeployment in-the-wild has only recently been studied, particularly on software\nengineering (SWE) tasks such as GitHub issue resolution. In this study, we\nexamine the code reasoning techniques that underlie the ability to perform such\ntasks, and examine the paradigms used to drive their performance. Our\ncontributions in this paper are: (1) the first dedicated survey on code\nreasoning for code tasks, highlighting overarching strategies, hybrid and\nagentic approaches; (2) a taxonomy of various techniques used to drive code\nreasoning; (3) a comprehensive overview of performance on common benchmarks and\na showcase of new, under-explored benchmarks with high potential in SWE; (4) an\nexploration on how core properties of code can be used to explain different\nreasoning techniques; and (5) gaps and potentially under-explored areas for\nfuture research.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088692
  },
  {
    "title": "Towards Bug-Free Distributed Go Programs",
    "abstract": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.",
    "text": "Towards Bug-Free Distributed Go Programs Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088703
  },
  {
    "title": "DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with\n  Large Language Models",
    "abstract": "Multimodal large language models (MLLMs) have streamlined front-end interface\ndevelopment by automating code generation. However, these models also introduce\nchallenges in ensuring code quality. Existing approaches struggle to maintain\nboth visual consistency and functional completeness in the generated\ncomponents. Moreover, they lack mechanisms to assess the fidelity and\ncorrectness of the rendered pages. To address these issues, we propose\nDesignCoder, a novel hierarchical-aware and self-correcting automated code\ngeneration framework. Specifically, we introduce UI Grouping Chains, which\nenhance MLLMs' capability to understand and predict complex nested UI\nhierarchies. Subsequently, DesignCoder employs a hierarchical\ndivide-and-conquer approach to generate front-end code. Finally, we incorporate\na self-correction mechanism to improve the model's ability to identify and\nrectify errors in the generated code. Extensive evaluations on a dataset of UI\nmockups collected from both open-source communities and industry projects\ndemonstrate that DesignCoder outperforms state-of-the-art baselines in React\nNative, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,\n12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and\nsignificantly improves code structure similarity in terms of TreeBLEU,\nContainer Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,\nwe conducted a user study with professional developers to assess the quality\nand practicality of the generated code. Results indicate that DesignCoder\naligns with industry best practices, demonstrating high usability, readability,\nand maintainability. Our approach provides an efficient and practical solution\nfor agile front-end development, enabling development teams to focus more on\ncore functionality and product innovation.",
    "text": "DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with\n  Large Language Models Multimodal large language models (MLLMs) have streamlined front-end interface\ndevelopment by automating code generation. However, these models also introduce\nchallenges in ensuring code quality. Existing approaches struggle to maintain\nboth visual consistency and functional completeness in the generated\ncomponents. Moreover, they lack mechanisms to assess the fidelity and\ncorrectness of the rendered pages. To address these issues, we propose\nDesignCoder, a novel hierarchical-aware and self-correcting automated code\ngeneration framework. Specifically, we introduce UI Grouping Chains, which\nenhance MLLMs' capability to understand and predict complex nested UI\nhierarchies. Subsequently, DesignCoder employs a hierarchical\ndivide-and-conquer approach to generate front-end code. Finally, we incorporate\na self-correction mechanism to improve the model's ability to identify and\nrectify errors in the generated code. Extensive evaluations on a dataset of UI\nmockups collected from both open-source communities and industry projects\ndemonstrate that DesignCoder outperforms state-of-the-art baselines in React\nNative, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,\n12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and\nsignificantly improves code structure similarity in terms of TreeBLEU,\nContainer Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,\nwe conducted a user study with professional developers to assess the quality\nand practicality of the generated code. Results indicate that DesignCoder\naligns with industry best practices, demonstrating high usability, readability,\nand maintainability. Our approach provides an efficient and practical solution\nfor agile front-end development, enabling development teams to focus more on\ncore functionality and product innovation.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088714
  },
  {
    "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and\n  Maintainability of MCP Servers",
    "abstract": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP\nservers. Using state-of-the-art health metrics and a hybrid analysis pipeline,\ncombining a general-purpose static analysis tool with an MCP-specific scanner,\nwe evaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities -- only three overlapping with\ntraditional software vulnerabilities. Additionally, 7.2% of servers contain\ngeneral vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping with traditional open-source software projects. These findings\nhighlight the need for MCP-specific vulnerability detection techniques while\nreaffirming the value of traditional analysis and refactoring practices.",
    "text": "Model Context Protocol (MCP) at First Glance: Studying the Security and\n  Maintainability of MCP Servers Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP\nservers. Using state-of-the-art health metrics and a hybrid analysis pipeline,\ncombining a general-purpose static analysis tool with an MCP-specific scanner,\nwe evaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities -- only three overlapping with\ntraditional software vulnerabilities. Additionally, 7.2% of servers contain\ngeneral vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping with traditional open-source software projects. These findings\nhighlight the need for MCP-specific vulnerability detection techniques while\nreaffirming the value of traditional analysis and refactoring practices.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088725
  },
  {
    "title": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency\n  Without Compromising Accuracy",
    "abstract": "The reliability of machine learning (ML) software systems is heavily\ninfluenced by changes in data over time. For that reason, ML systems require\nregular maintenance, typically based on model retraining. However, retraining\nrequires significant computational demand, which makes it energy-intensive and\nraises concerns about its environmental impact. To understand which retraining\ntechniques should be considered when designing sustainable ML applications, in\nthis work, we study the energy consumption of common retraining techniques.\nSince the accuracy of ML systems is also essential, we compare retraining\ntechniques in terms of both energy efficiency and accuracy. We showcase that\nretraining with only the most recent data, compared to all available data,\nreduces energy consumption by up to 25\\%, being a sustainable alternative to\nthe status quo. Furthermore, our findings show that retraining a model only\nwhen there is evidence that updates are necessary, rather than on a fixed\nschedule, can reduce energy consumption by up to 40\\%, provided a reliable data\nchange detector is in place. Our findings pave the way for better\nrecommendations for ML practitioners, guiding them toward more energy-efficient\nretraining techniques when designing sustainable ML software systems.",
    "text": "Sustainable Machine Learning Retraining: Optimizing Energy Efficiency\n  Without Compromising Accuracy The reliability of machine learning (ML) software systems is heavily\ninfluenced by changes in data over time. For that reason, ML systems require\nregular maintenance, typically based on model retraining. However, retraining\nrequires significant computational demand, which makes it energy-intensive and\nraises concerns about its environmental impact. To understand which retraining\ntechniques should be considered when designing sustainable ML applications, in\nthis work, we study the energy consumption of common retraining techniques.\nSince the accuracy of ML systems is also essential, we compare retraining\ntechniques in terms of both energy efficiency and accuracy. We showcase that\nretraining with only the most recent data, compared to all available data,\nreduces energy consumption by up to 25\\%, being a sustainable alternative to\nthe status quo. Furthermore, our findings show that retraining a model only\nwhen there is evidence that updates are necessary, rather than on a fixed\nschedule, can reduce energy consumption by up to 40\\%, provided a reliable data\nchange detector is in place. Our findings pave the way for better\nrecommendations for ML practitioners, guiding them toward more energy-efficient\nretraining techniques when designing sustainable ML software systems.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0887349
  },
  {
    "title": "Tady: A Neural Disassembler without Structural Constraint Violations",
    "abstract": "Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy.",
    "text": "Tady: A Neural Disassembler without Structural Constraint Violations Disassembly is a crucial yet challenging step in binary analysis. While\nemerging neural disassemblers show promise for efficiency and accuracy, they\nfrequently generate outputs violating fundamental structural constraints, which\nsignificantly compromise their practical usability. To address this critical\nproblem, we regularize the disassembly solution space by formalizing and\napplying key structural constraints based on post-dominance relations. This\napproach systematically detects widespread errors in existing neural\ndisassemblers' outputs. These errors often originate from models' limited\ncontext modeling and instruction-level decoding that neglect global structural\nintegrity. We introduce Tady, a novel neural disassembler featuring an improved\nmodel architecture and a dedicated post-processing algorithm, specifically\nengineered to address these deficiencies. Comprehensive evaluations on diverse\nbinaries demonstrate that Tady effectively eliminates structural constraint\nviolations and functions with high efficiency, while maintaining\ninstruction-level accuracy.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0887449
  },
  {
    "title": "Adopting Use Case Descriptions for Requirements Specification: an\n  Industrial Case Study",
    "abstract": "Context: Use case (UC) descriptions are a prominent format for specifying\nfunctional requirements. Existing literature abounds with recommendations on\nhow to write high-quality UC descriptions but lacks insights into (1) their\nreal-world adoption, (2) whether these recommendations correspond to actual\nquality, and (3) which factors influence the quality of UCs. Objectives: We aim\nto contribute empirical evidence about the adoption of UC descriptions in a\nlarge, globally distributed case company. Methods: We surveyed 1188 business\nrequirements of a case company that were elicited from 2020-01-01 until\n2024-12-31 and contained 1192 UCs in various forms. Among these, we manually\nevaluated the 273 template-style UC descriptions against established quality\nguidelines. We generated descriptive statistics of the format's adoption over\nthe surveyed time frame. Furthermore, we used inferential statistics to\ndetermine (a) how properties of the requirements engineering process affected\nthe UC quality and (b) how UC quality affects subsequent software development\nactivities. Results and Conclusions: Our descriptive results show how the\nadoption of UC descriptions in practice deviates from textbook recommendations.\nHowever, our inferential results suggest that only a few phenomena like\nsolution-orientation show an actual impact in practice. These results can steer\nUC quality research into a more relevant direction.",
    "text": "Adopting Use Case Descriptions for Requirements Specification: an\n  Industrial Case Study Context: Use case (UC) descriptions are a prominent format for specifying\nfunctional requirements. Existing literature abounds with recommendations on\nhow to write high-quality UC descriptions but lacks insights into (1) their\nreal-world adoption, (2) whether these recommendations correspond to actual\nquality, and (3) which factors influence the quality of UCs. Objectives: We aim\nto contribute empirical evidence about the adoption of UC descriptions in a\nlarge, globally distributed case company. Methods: We surveyed 1188 business\nrequirements of a case company that were elicited from 2020-01-01 until\n2024-12-31 and contained 1192 UCs in various forms. Among these, we manually\nevaluated the 273 template-style UC descriptions against established quality\nguidelines. We generated descriptive statistics of the format's adoption over\nthe surveyed time frame. Furthermore, we used inferential statistics to\ndetermine (a) how properties of the requirements engineering process affected\nthe UC quality and (b) how UC quality affects subsequent software development\nactivities. Results and Conclusions: Our descriptive results show how the\nadoption of UC descriptions in practice deviates from textbook recommendations.\nHowever, our inferential results suggest that only a few phenomena like\nsolution-orientation show an actual impact in practice. These results can steer\nUC quality research into a more relevant direction.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088756
  },
  {
    "title": "Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning",
    "abstract": "Incorrectly labelled test cases can adversely affect the training process of\nhuman-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,\na technique designed to identify such mislabelled test cases introduced during\nhuman-in-the-loop oracle learning. This technique can be applied to programs\ntaking numeric inputs. Given a compromised automatic test oracle and its\ntraining test suite, ISONOISE first isolates thetest cases suspected of being\nmislabelled. This task is performed based on the level of disagreement of a\ntest case with respect to the others. An intermediate automatic test oracle is\ntrained based on the slightly disagreeing test cases. Based on the predictions\nof this intermediate oracle, the test cases suspected of being mislabelled are\nsystematically presented for relabelling. When mislabelled test cases are\nfound, the intermediate test oracle is updated. This process repeats until no\nmislabelled test case is found in relabelling. ISONOISE was evaluated within\nthe human-in-the-loop oracle learning method used in LEARN2FIX. Experimental\nresults demonstrate that ISONOISE can identify mislabelled test cases\nintroduced by the human in LEARN2FIX with over 67% accuracy, while requiring\nonly a small number of relabelling queries. These findings highlight the\npotential of ISONOISE to enhance the reliability of human-in-the-loop oracle\nlearning.",
    "text": "Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning Incorrectly labelled test cases can adversely affect the training process of\nhuman-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,\na technique designed to identify such mislabelled test cases introduced during\nhuman-in-the-loop oracle learning. This technique can be applied to programs\ntaking numeric inputs. Given a compromised automatic test oracle and its\ntraining test suite, ISONOISE first isolates thetest cases suspected of being\nmislabelled. This task is performed based on the level of disagreement of a\ntest case with respect to the others. An intermediate automatic test oracle is\ntrained based on the slightly disagreeing test cases. Based on the predictions\nof this intermediate oracle, the test cases suspected of being mislabelled are\nsystematically presented for relabelling. When mislabelled test cases are\nfound, the intermediate test oracle is updated. This process repeats until no\nmislabelled test case is found in relabelling. ISONOISE was evaluated within\nthe human-in-the-loop oracle learning method used in LEARN2FIX. Experimental\nresults demonstrate that ISONOISE can identify mislabelled test cases\nintroduced by the human in LEARN2FIX with over 67% accuracy, while requiring\nonly a small number of relabelling queries. These findings highlight the\npotential of ISONOISE to enhance the reliability of human-in-the-loop oracle\nlearning.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088767
  },
  {
    "title": "Empirical Evaluation of Large Language Models in Automated Program\n  Repair",
    "abstract": "The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems.",
    "text": "Empirical Evaluation of Large Language Models in Automated Program\n  Repair The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088777
  },
  {
    "title": "From Empirical Evaluation to Context-Aware Enhancement: Repairing\n  Regression Errors with LLMs",
    "abstract": "[...] Since then, various APR approaches, especially those leveraging the\npower of large language models (LLMs), have been rapidly developed to fix\ngeneral software bugs. Unfortunately, the effectiveness of these advanced\ntechniques in the context of regression bugs remains largely unexplored. This\ngap motivates the need for an empirical study evaluating the effectiveness of\nmodern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java\nregression bugs. To facilitate our study, we introduce RegMiner4APR, a\nhigh-quality benchmark of Java regression bugs integrated into a framework\ndesigned to facilitate APR research. The current benchmark includes 99\nregression bugs collected from 32 widely used real-world Java GitHub\nrepositories. We begin by conducting an in-depth analysis of the benchmark,\ndemonstrating its diversity and quality. Building on this foundation, we\nempirically evaluate the capabilities of APR to regression bugs by assessing\nboth traditional APR tools and advanced LLM-based APR approaches. Our\nexperimental results show that classical APR tools fail to repair any bugs,\nwhile LLM-based APR approaches exhibit promising potential. Motivated by these\nresults, we investigate impact of incorporating bug-inducing change information\ninto LLM-based APR approaches for fixing regression bugs. Our results highlight\nthat this context-aware enhancement significantly improves the performance of\nLLM-based APR, yielding 1.8x more successful repairs compared to using\nLLM-based APR without such context.",
    "text": "From Empirical Evaluation to Context-Aware Enhancement: Repairing\n  Regression Errors with LLMs [...] Since then, various APR approaches, especially those leveraging the\npower of large language models (LLMs), have been rapidly developed to fix\ngeneral software bugs. Unfortunately, the effectiveness of these advanced\ntechniques in the context of regression bugs remains largely unexplored. This\ngap motivates the need for an empirical study evaluating the effectiveness of\nmodern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java\nregression bugs. To facilitate our study, we introduce RegMiner4APR, a\nhigh-quality benchmark of Java regression bugs integrated into a framework\ndesigned to facilitate APR research. The current benchmark includes 99\nregression bugs collected from 32 widely used real-world Java GitHub\nrepositories. We begin by conducting an in-depth analysis of the benchmark,\ndemonstrating its diversity and quality. Building on this foundation, we\nempirically evaluate the capabilities of APR to regression bugs by assessing\nboth traditional APR tools and advanced LLM-based APR approaches. Our\nexperimental results show that classical APR tools fail to repair any bugs,\nwhile LLM-based APR approaches exhibit promising potential. Motivated by these\nresults, we investigate impact of incorporating bug-inducing change information\ninto LLM-based APR approaches for fixing regression bugs. Our results highlight\nthat this context-aware enhancement significantly improves the performance of\nLLM-based APR, yielding 1.8x more successful repairs compared to using\nLLM-based APR without such context.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.08879
  },
  {
    "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM\n  Approaches",
    "abstract": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.",
    "text": "Querying Large Automotive Software Models: Agentic vs. Direct LLM\n  Approaches Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088802
  },
  {
    "title": "Using LLMs for Security Advisory Investigations: How Far Are We?",
    "abstract": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration.",
    "text": "Using LLMs for Security Advisory Investigations: How Far Are We? Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088812
  },
  {
    "title": "Designing Deep Learning Frameworks for LLMs:Challenges, Expectations,\n  and Opportunities",
    "abstract": "Large language models (LLMs) drive significant advancements in real industry\napplications. LLMs rely on DL frameworks for efficient model construction,\ndistributed execution, and optimized deployment. Their large parameter scale\nand long execution cycles place extreme demands on DL frameworks in terms of\nscalability, stability, and efficiency. Therefore, poor usability, limited\nfunctionality, and subtle bugs in DL frameworks may hinder development\nefficiency and cause severe failures or resource waste. However, a fundamental\nquestion remains underinvestigated, i.e., What challenges do DL frameworks face\nin supporting LLMs? To seek an answer, we investigate these challenges through\na large-scale analysis of issue reports from three major DL frameworks\n(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,\nMegatron). We construct a taxonomy of LLM-centric bugs, requirements, and user\nquestions and enrich it through interviews with 11 LLM users and eight DL\nframework developers, uncovering key technical challenges and misalignments\nbetween user needs and developer priorities. Our contributions are threefold:\n(1) we develop a comprehensive taxonomy comprising four question themes (nine\nsub-themes), four requirement themes (15 sub-themes), and ten bug themes (45\nsub-themes); (2) we assess the perceived importance and priority of these\nchallenges based on practitioner insights; and (3) we identify five key\nfindings across the LLM development and propose five actionable recommendations\nto improve the reliability, usability, and testability of DL frameworks. Our\nresults highlight critical limitations in current DL frameworks and offer\nconcrete guidance for advancing their support for the next generation of LLM\nconstruction and applications.",
    "text": "Designing Deep Learning Frameworks for LLMs:Challenges, Expectations,\n  and Opportunities Large language models (LLMs) drive significant advancements in real industry\napplications. LLMs rely on DL frameworks for efficient model construction,\ndistributed execution, and optimized deployment. Their large parameter scale\nand long execution cycles place extreme demands on DL frameworks in terms of\nscalability, stability, and efficiency. Therefore, poor usability, limited\nfunctionality, and subtle bugs in DL frameworks may hinder development\nefficiency and cause severe failures or resource waste. However, a fundamental\nquestion remains underinvestigated, i.e., What challenges do DL frameworks face\nin supporting LLMs? To seek an answer, we investigate these challenges through\na large-scale analysis of issue reports from three major DL frameworks\n(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,\nMegatron). We construct a taxonomy of LLM-centric bugs, requirements, and user\nquestions and enrich it through interviews with 11 LLM users and eight DL\nframework developers, uncovering key technical challenges and misalignments\nbetween user needs and developer priorities. Our contributions are threefold:\n(1) we develop a comprehensive taxonomy comprising four question themes (nine\nsub-themes), four requirement themes (15 sub-themes), and ten bug themes (45\nsub-themes); (2) we assess the perceived importance and priority of these\nchallenges based on practitioner insights; and (3) we identify five key\nfindings across the LLM development and propose five actionable recommendations\nto improve the reliability, usability, and testability of DL frameworks. Our\nresults highlight critical limitations in current DL frameworks and offer\nconcrete guidance for advancing their support for the next generation of LLM\nconstruction and applications.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0888262
  },
  {
    "title": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development\n  via Automatic Evaluation",
    "abstract": "Large Language Models (LLMs) have made significant strides in front-end code\ngeneration. However, existing benchmarks exhibit several critical limitations:\nmany tasks are overly simplistic, test cases often lack rigor, and end-to-end\nvalidation is absent. These issues hinder the accurate assessment of model\nperformance. To address these challenges, we present FrontendBench, a benchmark\nco-developed by humans and LLMs. FrontendBench categorizes tasks based on code\nfunctionality and incorporates interactive test scenarios, enabling a more\ncomprehensive and practical evaluation of front-end code generation\ncapabilities. The benchmark comprises 148 meticulously crafted prompt-test case\npairs spanning five levels of web components, from basic UI elements to complex\ninteractive features. Each task reflects realistic front-end development\nchallenges. Furthermore, we introduce an automatic evaluation framework that\nexecutes generated code within a sandbox environment and assesses outcomes\nusing predefined test scripts. This framework achieves a 90.54% agreement rate\nwith expert human evaluations, demonstrating high reliability. We benchmark\nseveral state-of-the-art LLMs on FrontendBench and observe substantial\nperformance disparities in handling real-world front-end tasks. These results\nhighlight FrontendBench as a reliable and scalable benchmark, supporting\nconsistent multimodal evaluation and providing a robust foundation for future\nresearch in front-end code generation. Our data and code will be released soon.",
    "text": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development\n  via Automatic Evaluation Large Language Models (LLMs) have made significant strides in front-end code\ngeneration. However, existing benchmarks exhibit several critical limitations:\nmany tasks are overly simplistic, test cases often lack rigor, and end-to-end\nvalidation is absent. These issues hinder the accurate assessment of model\nperformance. To address these challenges, we present FrontendBench, a benchmark\nco-developed by humans and LLMs. FrontendBench categorizes tasks based on code\nfunctionality and incorporates interactive test scenarios, enabling a more\ncomprehensive and practical evaluation of front-end code generation\ncapabilities. The benchmark comprises 148 meticulously crafted prompt-test case\npairs spanning five levels of web components, from basic UI elements to complex\ninteractive features. Each task reflects realistic front-end development\nchallenges. Furthermore, we introduce an automatic evaluation framework that\nexecutes generated code within a sandbox environment and assesses outcomes\nusing predefined test scripts. This framework achieves a 90.54% agreement rate\nwith expert human evaluations, demonstrating high reliability. We benchmark\nseveral state-of-the-art LLMs on FrontendBench and observe substantial\nperformance disparities in handling real-world front-end tasks. These results\nhighlight FrontendBench as a reliable and scalable benchmark, supporting\nconsistent multimodal evaluation and providing a robust foundation for future\nresearch in front-end code generation. Our data and code will be released soon.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0888362
  },
  {
    "title": "Distributed Computing From First Principles",
    "abstract": "This book on Distributed Computing aims to benefit a diverse audience,\nranging from aspiring engineers, and seasoned researchers, to a wide range of\nprofessionals. Driven by my passion for making the core concepts of distributed\ncomputing accessible, this work is a significant undertaking designed to\nempower individuals from all backgrounds to gain valuable insight. Have you\never wondered how a typical distributed system works under the hood? Are you\nlooking for a pedagogical guide with complete implementations? In this work, we\nhave implemented several foundational algorithms in Distributed Computing.\nWhether your expertise lies in the theoretical foundations or the practical\napplications of the principles of Distributed Systems, this book is for you.",
    "text": "Distributed Computing From First Principles This book on Distributed Computing aims to benefit a diverse audience,\nranging from aspiring engineers, and seasoned researchers, to a wide range of\nprofessionals. Driven by my passion for making the core concepts of distributed\ncomputing accessible, this work is a significant undertaking designed to\nempower individuals from all backgrounds to gain valuable insight. Have you\never wondered how a typical distributed system works under the hood? Are you\nlooking for a pedagogical guide with complete implementations? In this work, we\nhave implemented several foundational algorithms in Distributed Computing.\nWhether your expertise lies in the theoretical foundations or the practical\napplications of the principles of Distributed Systems, this book is for you.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088845
  },
  {
    "title": "Towards Operation Proof Obligation Generation for VDM",
    "abstract": "All formalisms have the ability to ensure that their models are internally\nconsistent. Potential inconsistencies are generally highlighted by assertions\ncalled proof obligations, and the generation of these obligations is an\nimportant role of the tools that support the method. This capability has been\navailable for VDM tools for many years. However, support for obligation\ngeneration for explicit operation bodies has always been limited. This work\ndescribes the current state of work to address this, showing the capabilities\nso far and highlighting the work remaining.",
    "text": "Towards Operation Proof Obligation Generation for VDM All formalisms have the ability to ensure that their models are internally\nconsistent. Potential inconsistencies are generally highlighted by assertions\ncalled proof obligations, and the generation of these obligations is an\nimportant role of the tools that support the method. This capability has been\navailable for VDM tools for many years. However, support for obligation\ngeneration for explicit operation bodies has always been limited. This work\ndescribes the current state of work to address this, showing the capabilities\nso far and highlighting the work remaining.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088855
  },
  {
    "title": "The Journey of CodeLab: How University Hackathons Built a Community of\n  Engaged Students",
    "abstract": "This paper presents the journey of CodeLab: a student-organized initiative\nfrom the University of S\\~ao Paulo that has grown thanks to university\nhackathons. It summarizes patterns, challenges, and lessons learned over 15\ncompetitions organized by the group from 2015 to 2020. By describing these\nexperiences, this report aims to help CodeLab to resume its events after the\nCOVID-19 pandemic, and foster similar initiatives around the world.",
    "text": "The Journey of CodeLab: How University Hackathons Built a Community of\n  Engaged Students This paper presents the journey of CodeLab: a student-organized initiative\nfrom the University of S\\~ao Paulo that has grown thanks to university\nhackathons. It summarizes patterns, challenges, and lessons learned over 15\ncompetitions organized by the group from 2015 to 2020. By describing these\nexperiences, this report aims to help CodeLab to resume its events after the\nCOVID-19 pandemic, and foster similar initiatives around the world.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088864
  },
  {
    "title": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library\n  Scenarios",
    "abstract": "Code debugging is a crucial task in software engineering, which attracts\nincreasing attention. While remarkable success has been made in the era of\nlarge language models (LLMs), current research still focuses on the simple\nno-library or single-library setting, ignoring the complex multi-library\nscenario in real-world applications. To address this limitation, we make the\nfirst attempt to introduce MLDebugging (Multi-Library Debugging), a\ncomprehensive benchmark designed to assess debugging challenges within\nmulti-library Python code. Specifically, MLDebugging encompasses 126 distinct\nPython libraries, covering a wide range of multi-library code issues,\ncategorized into seven distinct types. Furthermore, we conduct a thorough\nevaluation of MLDebugging using both mainstream open-source and closed-source\nLLMs and highlight that current LLMs still struggle to correctly perform code\ndebugging across multi-library scenarios. We hope this work can uncover the\npotential of LLMs in multi-library debugging scenario and offer insights for\nfuture research.",
    "text": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library\n  Scenarios Code debugging is a crucial task in software engineering, which attracts\nincreasing attention. While remarkable success has been made in the era of\nlarge language models (LLMs), current research still focuses on the simple\nno-library or single-library setting, ignoring the complex multi-library\nscenario in real-world applications. To address this limitation, we make the\nfirst attempt to introduce MLDebugging (Multi-Library Debugging), a\ncomprehensive benchmark designed to assess debugging challenges within\nmulti-library Python code. Specifically, MLDebugging encompasses 126 distinct\nPython libraries, covering a wide range of multi-library code issues,\ncategorized into seven distinct types. Furthermore, we conduct a thorough\nevaluation of MLDebugging using both mainstream open-source and closed-source\nLLMs and highlight that current LLMs still struggle to correctly perform code\ndebugging across multi-library scenarios. We hope this work can uncover the\npotential of LLMs in multi-library debugging scenario and offer insights for\nfuture research.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088874
  },
  {
    "title": "IDOL: Improved Different Optimization Levels Testing for Solidity\n  Compilers",
    "abstract": "As blockchain technology continues to evolve and mature, smart contracts have\nbecome a key driving force behind the digitization and automation of\ntransactions. Smart contracts greatly simplify and refine the traditional\nbusiness transaction processes, and thus have had a profound impact on various\nindustries such as finance and supply chain management. However, because smart\ncontracts cannot be modified once deployed, any vulnerabilities or design flaws\nwithin the contract cannot be easily fixed, potentially leading to significant\nfinancial losses or even legal issues. The compiler, as a critical component in\nthe development process, directly affects the quality and security of smart\ncontracts. This paper innovatively proposes a method, known as the Improved\nDifferent Optimization Levels (IDOL), for testing the Solidity compiler. The\nkey idea behind IDOL is to perform reverse optimization transformations (i.e.,\nchange optimized form into unoptimized form) to generate semantically\nequivalent variants of the smart contracts under test, aiming to maximize the\nopportunities to trigger the optimization logic of compilers. We conducted a\npreliminary evaluation of IDOL and three confirmed compiler optimization bugs\nhave been uncovered at the time of writing.",
    "text": "IDOL: Improved Different Optimization Levels Testing for Solidity\n  Compilers As blockchain technology continues to evolve and mature, smart contracts have\nbecome a key driving force behind the digitization and automation of\ntransactions. Smart contracts greatly simplify and refine the traditional\nbusiness transaction processes, and thus have had a profound impact on various\nindustries such as finance and supply chain management. However, because smart\ncontracts cannot be modified once deployed, any vulnerabilities or design flaws\nwithin the contract cannot be easily fixed, potentially leading to significant\nfinancial losses or even legal issues. The compiler, as a critical component in\nthe development process, directly affects the quality and security of smart\ncontracts. This paper innovatively proposes a method, known as the Improved\nDifferent Optimization Levels (IDOL), for testing the Solidity compiler. The\nkey idea behind IDOL is to perform reverse optimization transformations (i.e.,\nchange optimized form into unoptimized form) to generate semantically\nequivalent variants of the smart contracts under test, aiming to maximize the\nopportunities to trigger the optimization logic of compilers. We conducted a\npreliminary evaluation of IDOL and three confirmed compiler optimization bugs\nhave been uncovered at the time of writing.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0888839
  },
  {
    "title": "Role, cost, and complexity of software in the real-world: a case for\n  formal methods",
    "abstract": "In this chapter we outline the role that software has in modern society,\nalong with the staggering costs of poor software quality. To lay this bare, we\nrecall the costs of some of the major software failures that happened during\nthe last~$40$ years. We argue that these costs justify researching, studying\nand applying formal software verification and in particular program analysis.\nThis position is supported by successful industrial experiences.",
    "text": "Role, cost, and complexity of software in the real-world: a case for\n  formal methods In this chapter we outline the role that software has in modern society,\nalong with the staggering costs of poor software quality. To lay this bare, we\nrecall the costs of some of the major software failures that happened during\nthe last~$40$ years. We argue that these costs justify researching, studying\nand applying formal software verification and in particular program analysis.\nThis position is supported by successful industrial experiences.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088892
  },
  {
    "title": "Shelter Soul: Bridging Shelters and Adopters Through Technology",
    "abstract": "Pet adoption processes often face inefficiencies, including limited\naccessibility, lack of real-time information, and mismatched expectations\nbetween shelters and adopters. To address these challenges, this study presents\nShelter Soul, a technology-based solution designed to streamline pet adoption\nthrough an integrated, web-based platform. Developed using the MERN stack and\nGraphQL, Shelter Soul is a prototype system built to improve pet matching\naccuracy, shelter management efficiency, and secure online donations. The\nsystem includes modules for intelligent pet matching, shelter administration,\ndonation processing, volunteer coordination, and analytics. Prototype testing\n(performance load tests, usability studies, and security assessments)\ndemonstrated that the system meets its design goals: it handled 500 concurrent\nusers with a 99.2% transaction success rate and an average response time of 250\nms, and usability feedback rated the interface highly (4.5/5). These results\nindicate Shelter Soul's potential as a practical solution to enhance animal\nshelter operations and adoption outcomes.",
    "text": "Shelter Soul: Bridging Shelters and Adopters Through Technology Pet adoption processes often face inefficiencies, including limited\naccessibility, lack of real-time information, and mismatched expectations\nbetween shelters and adopters. To address these challenges, this study presents\nShelter Soul, a technology-based solution designed to streamline pet adoption\nthrough an integrated, web-based platform. Developed using the MERN stack and\nGraphQL, Shelter Soul is a prototype system built to improve pet matching\naccuracy, shelter management efficiency, and secure online donations. The\nsystem includes modules for intelligent pet matching, shelter administration,\ndonation processing, volunteer coordination, and analytics. Prototype testing\n(performance load tests, usability studies, and security assessments)\ndemonstrated that the system meets its design goals: it handled 500 concurrent\nusers with a 99.2% transaction success rate and an average response time of 250\nms, and usability feedback rated the interface highly (4.5/5). These results\nindicate Shelter Soul's potential as a practical solution to enhance animal\nshelter operations and adoption outcomes.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088902
  },
  {
    "title": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository\n  Issue Resolution",
    "abstract": "LLMs demonstrate strong performance in auto-mated software engineering,\nparticularly for code generation and issue resolution. While proprietary models\nlike GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,\ncost, and privacy concerns limit adoption. Open-source alternatives offer\ntransparency but underperform in complex tasks, especially sub-100B parameter\nmodels. Although quality Chain-of-Thought (CoT) data can enhance reasoning,\ncurrent methods face two critical flaws: (1) weak rejection sampling reduces\ndata quality, and (2) inadequate step validation causes error accumulation.\nThese limitations lead to flawed reasoning chains that impair LLMs'ability to\nlearn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced\nMonte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and\noptimizes intermediate reasoning steps through a rigorous rejection sampling\nstrategy, generating high-quality CoT data to improve LLM performance in issue\nresolution tasks. Key innovations include: (1) augmenting MCTS with a\nreflection mechanism that corrects errors via rejection sampling and\nrefinement, (2) decomposing issue resolution into three subtasks-File\nLocalization, Fault Localization, and Patch Generation-each with clear\nground-truth criteria, and (3) enforcing a strict sampling protocol where\nintermediate outputs must exactly match verified developer patches, ensuring\ncorrectness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench\nVerified demonstrate that LLMs fine-tuned with our CoT dataset achieve\nsubstantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves\n28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline\nSWE-Fixer-Qwen-72B with the same parameter scale, which only reached\n24.7%(Lite) and 32.8%(Verified).",
    "text": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository\n  Issue Resolution LLMs demonstrate strong performance in auto-mated software engineering,\nparticularly for code generation and issue resolution. While proprietary models\nlike GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,\ncost, and privacy concerns limit adoption. Open-source alternatives offer\ntransparency but underperform in complex tasks, especially sub-100B parameter\nmodels. Although quality Chain-of-Thought (CoT) data can enhance reasoning,\ncurrent methods face two critical flaws: (1) weak rejection sampling reduces\ndata quality, and (2) inadequate step validation causes error accumulation.\nThese limitations lead to flawed reasoning chains that impair LLMs'ability to\nlearn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced\nMonte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and\noptimizes intermediate reasoning steps through a rigorous rejection sampling\nstrategy, generating high-quality CoT data to improve LLM performance in issue\nresolution tasks. Key innovations include: (1) augmenting MCTS with a\nreflection mechanism that corrects errors via rejection sampling and\nrefinement, (2) decomposing issue resolution into three subtasks-File\nLocalization, Fault Localization, and Patch Generation-each with clear\nground-truth criteria, and (3) enforcing a strict sampling protocol where\nintermediate outputs must exactly match verified developer patches, ensuring\ncorrectness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench\nVerified demonstrate that LLMs fine-tuned with our CoT dataset achieve\nsubstantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves\n28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline\nSWE-Fixer-Qwen-72B with the same parameter scale, which only reached\n24.7%(Lite) and 32.8%(Verified).",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0889118
  },
  {
    "title": "Structured Program Synthesis using LLMs: Results and Insights from the\n  IPARC Challenge",
    "abstract": "The IPARC Challenge, inspired by ARC, provides controlled program synthesis\ntasks over synthetic images to evaluate automatic program construction,\nfocusing on sequence, selection, and iteration. This set of 600 tasks has\nresisted automated solutions. This paper presents a structured inductive\nprogramming approach with LLMs that successfully solves tasks across all IPARC\ncategories. The controlled nature of IPARC reveals insights into LLM-based code\ngeneration, including the importance of prior structuring, LLMs' ability to aid\nstructuring (requiring human refinement), the need to freeze correct code, the\nefficiency of code reuse, and how LLM-generated code can spark human\ncreativity. These findings suggest valuable mechanisms for human-LLM\ncollaboration in tackling complex program synthesis.",
    "text": "Structured Program Synthesis using LLMs: Results and Insights from the\n  IPARC Challenge The IPARC Challenge, inspired by ARC, provides controlled program synthesis\ntasks over synthetic images to evaluate automatic program construction,\nfocusing on sequence, selection, and iteration. This set of 600 tasks has\nresisted automated solutions. This paper presents a structured inductive\nprogramming approach with LLMs that successfully solves tasks across all IPARC\ncategories. The controlled nature of IPARC reveals insights into LLM-based code\ngeneration, including the importance of prior structuring, LLMs' ability to aid\nstructuring (requiring human refinement), the need to freeze correct code, the\nefficiency of code reuse, and how LLM-generated code can spark human\ncreativity. These findings suggest valuable mechanisms for human-LLM\ncollaboration in tackling complex program synthesis.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088922
  },
  {
    "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest\n  Code Competition?",
    "abstract": "Code generation is a core capability of large language models (LLMs), yet\nmainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with\nmedium-level difficulty and pose no challenge to advanced LLMs. To better\nreflected the advanced reasoning and code generation ability, We introduce\nHumanity's Last Code Exam (HLCE), comprising 235 most challenging problems from\nthe International Collegiate Programming Contest (ICPC World Finals) and the\nInternational Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of\nHLCE, we design a harmonized online-offline sandbox that guarantees fully\nreproducible evaluation. Through our comprehensive evaluation, we observe that\neven the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve\npass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a\nnovel \"self-recognition\" task to measure LLMs' awareness of their own\ncapabilities. Results indicate that LLMs' self-recognition abilities are not\nproportionally correlated with their code generation performance. Finally, our\nempirical validation of test-time scaling laws reveals that current advanced\nLLMs have substantial room for improvement on complex programming tasks. We\nexpect HLCE to become a milestone challenge for code generation and to catalyze\nadvances in high-performance reasoning and human-AI collaborative programming.\nOur code and dataset are also public\navailable(https://github.com/Humanity-s-Last-Code-Exam/HLCE).",
    "text": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest\n  Code Competition? Code generation is a core capability of large language models (LLMs), yet\nmainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with\nmedium-level difficulty and pose no challenge to advanced LLMs. To better\nreflected the advanced reasoning and code generation ability, We introduce\nHumanity's Last Code Exam (HLCE), comprising 235 most challenging problems from\nthe International Collegiate Programming Contest (ICPC World Finals) and the\nInternational Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of\nHLCE, we design a harmonized online-offline sandbox that guarantees fully\nreproducible evaluation. Through our comprehensive evaluation, we observe that\neven the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve\npass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a\nnovel \"self-recognition\" task to measure LLMs' awareness of their own\ncapabilities. Results indicate that LLMs' self-recognition abilities are not\nproportionally correlated with their code generation performance. Finally, our\nempirical validation of test-time scaling laws reveals that current advanced\nLLMs have substantial room for improvement on complex programming tasks. We\nexpect HLCE to become a milestone challenge for code generation and to catalyze\nadvances in high-performance reasoning and human-AI collaborative programming.\nOur code and dataset are also public\navailable(https://github.com/Humanity-s-Last-Code-Exam/HLCE).",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088932
  },
  {
    "title": "Get on the Train or be Left on the Station: Using LLMs for Software\n  Engineering Research",
    "abstract": "The adoption of Large Language Models (LLMs) is not only transforming\nsoftware engineering (SE) practice but is also poised to fundamentally disrupt\nhow research is conducted in the field. While perspectives on this\ntransformation range from viewing LLMs as mere productivity tools to\nconsidering them revolutionary forces, we argue that the SE research community\nmust proactively engage with and shape the integration of LLMs into research\npractices, emphasizing human agency in this transformation. As LLMs rapidly\nbecome integral to SE research - both as tools that support investigations and\nas subjects of study - a human-centric perspective is essential. Ensuring human\noversight and interpretability is necessary for upholding scientific rigor,\nfostering ethical responsibility, and driving advancements in the field.\nDrawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI\nin SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze\nthe impact of LLMs on SE research. Through this theoretical lens, we examine\nhow LLMs enhance research capabilities through accelerated ideation and\nautomated processes, make some traditional research practices obsolete,\nretrieve valuable aspects of historical research approaches, and risk reversal\neffects when taken to extremes. Our analysis reveals opportunities for\ninnovation and potential pitfalls that require careful consideration. We\nconclude with a call to action for the SE research community to proactively\nharness the benefits of LLMs while developing frameworks and guidelines to\nmitigate their risks, to ensure continued rigor and impact of research in an\nAI-augmented future.",
    "text": "Get on the Train or be Left on the Station: Using LLMs for Software\n  Engineering Research The adoption of Large Language Models (LLMs) is not only transforming\nsoftware engineering (SE) practice but is also poised to fundamentally disrupt\nhow research is conducted in the field. While perspectives on this\ntransformation range from viewing LLMs as mere productivity tools to\nconsidering them revolutionary forces, we argue that the SE research community\nmust proactively engage with and shape the integration of LLMs into research\npractices, emphasizing human agency in this transformation. As LLMs rapidly\nbecome integral to SE research - both as tools that support investigations and\nas subjects of study - a human-centric perspective is essential. Ensuring human\noversight and interpretability is necessary for upholding scientific rigor,\nfostering ethical responsibility, and driving advancements in the field.\nDrawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI\nin SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze\nthe impact of LLMs on SE research. Through this theoretical lens, we examine\nhow LLMs enhance research capabilities through accelerated ideation and\nautomated processes, make some traditional research practices obsolete,\nretrieve valuable aspects of historical research approaches, and risk reversal\neffects when taken to extremes. Our analysis reveals opportunities for\ninnovation and potential pitfalls that require careful consideration. We\nconclude with a call to action for the SE research community to proactively\nharness the benefits of LLMs while developing frameworks and guidelines to\nmitigate their risks, to ensure continued rigor and impact of research in an\nAI-augmented future.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088943
  },
  {
    "title": "Towards Lean Research Inception: Assessing Practical Relevance of\n  Formulated Research Problems",
    "abstract": "[Context] The lack of practical relevance in many Software Engineering (SE)\nresearch contributions is often rooted in oversimplified views of industrial\npractice, weak industry connections, and poorly defined research problems.\nClear criteria for evaluating SE research problems can help align their value,\nfeasibility, and applicability with industrial needs. [Goal] In this paper, we\nintroduce the Lean Research Inception (LRI) framework, designed to support the\nformulation and assessment of practically relevant research problems in SE. We\ndescribe its initial evaluation strategy conducted in a workshop with a network\nof SE researchers experienced in industry-academia collaboration and report the\nevaluation of its three assessment criteria (valuable, feasible, and\napplicable) regarding their importance in assessing practical relevance.\n[Method] We applied LRI retroactively to a published research paper, engaging\nworkshop participants in discussing and assessing the research problem by\napplying the proposed criteria using a semantic differential scale.\nParticipants provided feedback on the criteria's importance and completeness,\ndrawn from their own experiences in industry-academia collaboration. [Results]\nThe findings reveal an overall agreement on the importance of the three\ncriteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for\naligning research problems with industrial needs. Qualitative feedback\nsuggested adjustments in terminology with a clearer distinction between\nfeasible and applicable, and refinements for valuable by more clearly\nconsidering business value, ROI, and originality. [Conclusion] While LRI\nconstitutes ongoing research and requires further evaluation, our results\nstrengthen our confidence that the three criteria applied using the semantic\ndifferential scale can already help the community assess the practical\nrelevance of SE research problems.",
    "text": "Towards Lean Research Inception: Assessing Practical Relevance of\n  Formulated Research Problems [Context] The lack of practical relevance in many Software Engineering (SE)\nresearch contributions is often rooted in oversimplified views of industrial\npractice, weak industry connections, and poorly defined research problems.\nClear criteria for evaluating SE research problems can help align their value,\nfeasibility, and applicability with industrial needs. [Goal] In this paper, we\nintroduce the Lean Research Inception (LRI) framework, designed to support the\nformulation and assessment of practically relevant research problems in SE. We\ndescribe its initial evaluation strategy conducted in a workshop with a network\nof SE researchers experienced in industry-academia collaboration and report the\nevaluation of its three assessment criteria (valuable, feasible, and\napplicable) regarding their importance in assessing practical relevance.\n[Method] We applied LRI retroactively to a published research paper, engaging\nworkshop participants in discussing and assessing the research problem by\napplying the proposed criteria using a semantic differential scale.\nParticipants provided feedback on the criteria's importance and completeness,\ndrawn from their own experiences in industry-academia collaboration. [Results]\nThe findings reveal an overall agreement on the importance of the three\ncriteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for\naligning research problems with industrial needs. Qualitative feedback\nsuggested adjustments in terminology with a clearer distinction between\nfeasible and applicable, and refinements for valuable by more clearly\nconsidering business value, ROI, and originality. [Conclusion] While LRI\nconstitutes ongoing research and requires further evaluation, our results\nstrengthen our confidence that the three criteria applied using the semantic\ndifferential scale can already help the community assess the practical\nrelevance of SE research problems.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088953
  },
  {
    "title": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via\n  Web Search-Augmented Agentic Generative AI Foundation Models",
    "abstract": "Generative AI foundation models offer transformative potential for processing\nstructured biological data, particularly in single-cell RNA sequencing, where\ndatasets are rapidly scaling toward billions of cells. We propose the use of\nagentic foundation models with real-time web search to automate the labeling of\nexperimental data, achieving up to 82.5% accuracy. This addresses a key\nbottleneck in supervised learning for structured omics data by increasing\nannotation throughput without manual curation and human error. Our approach\nenables the development of virtual cell foundation models capable of downstream\ntasks such as cell-typing and perturbation prediction. As data volume grows,\nthese models may surpass human performance in labeling, paving the way for\nreliable inference in large-scale perturbation screens. This application\ndemonstrates domain-specific innovation in health monitoring and diagnostics,\naligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.",
    "text": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via\n  Web Search-Augmented Agentic Generative AI Foundation Models Generative AI foundation models offer transformative potential for processing\nstructured biological data, particularly in single-cell RNA sequencing, where\ndatasets are rapidly scaling toward billions of cells. We propose the use of\nagentic foundation models with real-time web search to automate the labeling of\nexperimental data, achieving up to 82.5% accuracy. This addresses a key\nbottleneck in supervised learning for structured omics data by increasing\nannotation throughput without manual curation and human error. Our approach\nenables the development of virtual cell foundation models capable of downstream\ntasks such as cell-typing and perturbation prediction. As data volume grows,\nthese models may surpass human performance in labeling, paving the way for\nreliable inference in large-scale perturbation screens. This application\ndemonstrates domain-specific innovation in health monitoring and diagnostics,\naligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088963
  },
  {
    "title": "Social Media Reactions to Open Source Promotions: AI-Powered GitHub\n  Projects on Hacker News",
    "abstract": "Social media platforms have become more influential than traditional news\nsources, shaping public discourse and accelerating the spread of information.\nWith the rapid advancement of artificial intelligence (AI), open-source\nsoftware (OSS) projects can leverage these platforms to gain visibility and\nattract contributors. In this study, we investigate the relationship between\nHacker News, a social news site focused on computer science and\nentrepreneurship, and the extent to which it influences developer activity on\nthe promoted GitHub AI projects.\n  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments\nover a two-year period. Our findings reveal that at least 19\\% of AI developers\npromoted their GitHub projects on Hacker News, often receiving positive\nengagement from the community. By tracking activity on the associated 1,814\nGitHub repositories after they were shared on Hacker News, we observed a\nsignificant increase in forks, stars, and contributors. These results suggest\nthat Hacker News serves as a viable platform for AI-powered OSS projects, with\nthe potential to gain attention, foster community engagement, and accelerate\nsoftware development.",
    "text": "Social Media Reactions to Open Source Promotions: AI-Powered GitHub\n  Projects on Hacker News Social media platforms have become more influential than traditional news\nsources, shaping public discourse and accelerating the spread of information.\nWith the rapid advancement of artificial intelligence (AI), open-source\nsoftware (OSS) projects can leverage these platforms to gain visibility and\nattract contributors. In this study, we investigate the relationship between\nHacker News, a social news site focused on computer science and\nentrepreneurship, and the extent to which it influences developer activity on\nthe promoted GitHub AI projects.\n  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments\nover a two-year period. Our findings reveal that at least 19\\% of AI developers\npromoted their GitHub projects on Hacker News, often receiving positive\nengagement from the community. By tracking activity on the associated 1,814\nGitHub repositories after they were shared on Hacker News, we observed a\nsignificant increase in forks, stars, and contributors. These results suggest\nthat Hacker News serves as a viable platform for AI-powered OSS projects, with\nthe potential to gain attention, foster community engagement, and accelerate\nsoftware development.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088973
  },
  {
    "title": "Signal-First Architectures: Rethinking Front-End Reactivity",
    "abstract": "Modern front-end frameworks face escalating reactivity management challenges,\nincluding performance degradation from complex observable chains and\nunpredictable re-renders. This paper introduces Signal-First Architecture--a\nnovel paradigm where granular, dependency-tracked signals are the atomic unit\nof reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces\nreactive flows from explicit signal declarations, with derived values via\ncomputed() and side effects scoped to effect(). This model ensures\ndeterministic behavior by eliminating implicit subscriptions and optimizing\nreactive graph evaluation.\n  We present a comparative analysis of three Angular reactivity models: RxJS\nservice-based, NgRx global stores, and pure Signal-First implementations.\nThrough controlled benchmarking, including Chrome DevTools performance tracing,\nmemory heap snapshots, and Lighthouse audits, this study quantifies\nSignal-First advantages.",
    "text": "Signal-First Architectures: Rethinking Front-End Reactivity Modern front-end frameworks face escalating reactivity management challenges,\nincluding performance degradation from complex observable chains and\nunpredictable re-renders. This paper introduces Signal-First Architecture--a\nnovel paradigm where granular, dependency-tracked signals are the atomic unit\nof reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces\nreactive flows from explicit signal declarations, with derived values via\ncomputed() and side effects scoped to effect(). This model ensures\ndeterministic behavior by eliminating implicit subscriptions and optimizing\nreactive graph evaluation.\n  We present a comparative analysis of three Angular reactivity models: RxJS\nservice-based, NgRx global stores, and pure Signal-First implementations.\nThrough controlled benchmarking, including Chrome DevTools performance tracing,\nmemory heap snapshots, and Lighthouse audits, this study quantifies\nSignal-First advantages.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088982
  },
  {
    "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
    "abstract": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
    "text": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.088992
  },
  {
    "title": "A Mapping Study About Training in Industry Context in Software\n  Engineering",
    "abstract": "Context: Corporate training plays a strategic role in the continuous\ndevelopment of professionals in the software engineering industry. However,\nthere is a lack of systematized understanding of how training initiatives are\ndesigned, implemented, and evaluated within this domain.\n  Objective: This study aims to map the current state of research on corporate\ntraining in software engineering in industry settings, using Eduardo Salas'\ntraining framework as an analytical lens.\n  Method: A systematic mapping study was conducted involving the selection and\nanalysis of 26 primary studies published in the field. Each study was\ncategorized according to Salas' four key areas: Training Needs Analysis,\nAntecedent Training Conditions, Training Methods and Instructional Strategies,\nand Post-Training Conditions.\n  Results: The findings show a predominance of studies focusing on Training\nMethods and Instructional Strategies. Significant gaps were identified in other\nareas, particularly regarding Job/Task Analysis and Simulation-based Training\nand Games. Most studies were experience reports, lacking methodological rigor\nand longitudinal assessment.\n  Conclusions: The study offers a structured overview of how corporate training\nis approached in software engineering, revealing underexplored areas and\nproposing directions for future research. It contributes to both academic and\npractical communities by highlighting challenges, methodological trends, and\nopportunities for designing more effective training programs in industry.",
    "text": "A Mapping Study About Training in Industry Context in Software\n  Engineering Context: Corporate training plays a strategic role in the continuous\ndevelopment of professionals in the software engineering industry. However,\nthere is a lack of systematized understanding of how training initiatives are\ndesigned, implemented, and evaluated within this domain.\n  Objective: This study aims to map the current state of research on corporate\ntraining in software engineering in industry settings, using Eduardo Salas'\ntraining framework as an analytical lens.\n  Method: A systematic mapping study was conducted involving the selection and\nanalysis of 26 primary studies published in the field. Each study was\ncategorized according to Salas' four key areas: Training Needs Analysis,\nAntecedent Training Conditions, Training Methods and Instructional Strategies,\nand Post-Training Conditions.\n  Results: The findings show a predominance of studies focusing on Training\nMethods and Instructional Strategies. Significant gaps were identified in other\nareas, particularly regarding Job/Task Analysis and Simulation-based Training\nand Games. Most studies were experience reports, lacking methodological rigor\nand longitudinal assessment.\n  Conclusions: The study offers a structured overview of how corporate training\nis approached in software engineering, revealing underexplored areas and\nproposing directions for future research. It contributes to both academic and\npractical communities by highlighting challenges, methodological trends, and\nopportunities for designing more effective training programs in industry.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089003
  },
  {
    "title": "Sharp Tools: How Developers Wield Agentic AI in Real Software\n  Engineering Tasks",
    "abstract": "Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents.",
    "text": "Sharp Tools: How Developers Wield Agentic AI in Real Software\n  Engineering Tasks Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089013
  },
  {
    "title": "The Foundation Cracks: A Comprehensive Study on Bugs and Testing\n  Practices in LLM Libraries",
    "abstract": "Large Language Model (LLM) libraries have emerged as the foundational\ninfrastructure powering today's AI revolution, serving as the backbone for LLM\ndeployment, inference optimization, fine-tuning, and production serving across\ndiverse applications. Despite their critical role in the LLM ecosystem, these\nlibraries face frequent quality issues and bugs that threaten the reliability\nof AI systems built upon them. To address this knowledge gap, we present the\nfirst comprehensive empirical investigation into bug characteristics and\ntesting practices in modern LLM libraries. We examine 313 bug-fixing commits\nextracted across two widely-adopted LLM libraries: HuggingFace Transformers and\nvLLM.Through rigorous manual analysis, we establish comprehensive taxonomies\ncategorizing bug symptoms into 5 types and root causes into 14 distinct\ncategories.Our primary discovery shows that API misuse has emerged as the\npredominant root cause (32.17%-48.19%), representing a notable transition from\nalgorithm-focused defects in conventional deep learning frameworks toward\ninterface-oriented problems. Additionally, we examine 7,748 test functions to\nidentify 7 distinct test oracle categories employed in current testing\napproaches, with predefined expected outputs (such as specific tensors and text\nstrings) being the most common strategy. Our assessment of existing testing\neffectiveness demonstrates that the majority of bugs escape detection due to\ninadequate test cases (41.73%), lack of test drivers (32.37%), and weak test\noracles (25.90%). Drawing from these findings, we offer some recommendations\nfor enhancing LLM library quality assurance.",
    "text": "The Foundation Cracks: A Comprehensive Study on Bugs and Testing\n  Practices in LLM Libraries Large Language Model (LLM) libraries have emerged as the foundational\ninfrastructure powering today's AI revolution, serving as the backbone for LLM\ndeployment, inference optimization, fine-tuning, and production serving across\ndiverse applications. Despite their critical role in the LLM ecosystem, these\nlibraries face frequent quality issues and bugs that threaten the reliability\nof AI systems built upon them. To address this knowledge gap, we present the\nfirst comprehensive empirical investigation into bug characteristics and\ntesting practices in modern LLM libraries. We examine 313 bug-fixing commits\nextracted across two widely-adopted LLM libraries: HuggingFace Transformers and\nvLLM.Through rigorous manual analysis, we establish comprehensive taxonomies\ncategorizing bug symptoms into 5 types and root causes into 14 distinct\ncategories.Our primary discovery shows that API misuse has emerged as the\npredominant root cause (32.17%-48.19%), representing a notable transition from\nalgorithm-focused defects in conventional deep learning frameworks toward\ninterface-oriented problems. Additionally, we examine 7,748 test functions to\nidentify 7 distinct test oracle categories employed in current testing\napproaches, with predefined expected outputs (such as specific tensors and text\nstrings) being the most common strategy. Our assessment of existing testing\neffectiveness demonstrates that the majority of bugs escape detection due to\ninadequate test cases (41.73%), lack of test drivers (32.37%), and weak test\noracles (25.90%). Drawing from these findings, we offer some recommendations\nfor enhancing LLM library quality assurance.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089024
  },
  {
    "title": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason",
    "abstract": "As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce a diagnostic\ntask: file path identification from issue descriptions alone, to probe models'\nunderlying knowledge. We present empirical evidence that performance gains on\nSWE-Bench-Verified may be partially driven by memorization rather than genuine\nproblem-solving. We show that state-of-the-art models achieve up to 76%\naccuracy in identifying buggy file paths using only issue descriptions, without\naccess to repository structure. This performance is merely up to 53% on tasks\nfrom repositories not included in SWE-Bench, pointing to possible data\ncontamination or memorization. These findings raise concerns about the validity\nof existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.",
    "text": "The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of\n  Reason As large language models (LLMs) become increasingly capable and widely\nadopted, benchmarks play a central role in assessing their practical utility.\nFor example, SWE-Bench Verified has emerged as a critical benchmark for\nevaluating LLMs' software engineering abilities, particularly their aptitude\nfor resolving real-world GitHub issues. Recent LLMs show impressive performance\non SWE-Bench, leading to optimism about their capacity for complex coding\ntasks. However, current evaluation protocols may overstate these models' true\ncapabilities. It is crucial to distinguish LLMs' generalizable problem-solving\nability and other learned artifacts. In this work, we introduce a diagnostic\ntask: file path identification from issue descriptions alone, to probe models'\nunderlying knowledge. We present empirical evidence that performance gains on\nSWE-Bench-Verified may be partially driven by memorization rather than genuine\nproblem-solving. We show that state-of-the-art models achieve up to 76%\naccuracy in identifying buggy file paths using only issue descriptions, without\naccess to repository structure. This performance is merely up to 53% on tasks\nfrom repositories not included in SWE-Bench, pointing to possible data\ncontamination or memorization. These findings raise concerns about the validity\nof existing results and underscore the need for more robust,\ncontamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089034
  },
  {
    "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
    "abstract": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
    "text": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0890439
  },
  {
    "title": "code_transformed: The Influence of Large Language Models on Code",
    "abstract": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
    "text": "code_transformed: The Influence of Large Language Models on Code Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0890539
  },
  {
    "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
    "abstract": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
    "text": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming? Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089065
  },
  {
    "title": "A Short Survey on Formalising Software Requirements using Large Language\n  Models",
    "abstract": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements.",
    "text": "A Short Survey on Formalising Software Requirements using Large Language\n  Models This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0890749
  },
  {
    "title": "Instruction and Solution Probabilities as Heuristics for Inductive\n  Programming",
    "abstract": "Instruction subsets (ISs) are heuristics that can shrink the size of the\ninductive programming (IP) search space by tens of orders of magnitude. Here,\nwe extend the IS approach by introducing instruction and solution probabilities\nas additional heuristics. Instruction probability reflects the expectation of\nan instruction occurring in a solution, based on the frequency of instruction\noccurrence in a large code sample. The solution probability for a partial or\ncomplete program is simply the product of all constituent instruction\nprobabilities, including duplicates. We treat the minimum solution\nprobabilities observed in code sample program units of different sizes as\nsolution probability thresholds. These thresholds are used to prune the search\nspace as partial solutions are constructed, thereby eliminating any branches\ncontaining unlikely combinations of instructions. The new approach has been\nevaluated using a large sample of human code. We tested two formulations of\ninstruction probability: one based on instruction occurrence across the entire\ncode sample and another that measured the distribution separately for each IS.\nOur results show that both variants produce substantial further reductions in\nthe IP search space size of up to tens of orders of magnitude, depending on\nsolution size. In combination with IS, reductions of over 100 orders of\nmagnitude can be achieved. We also carried out cross-validation testing to show\nthat the heuristics should work effectively with unseen code. The approach is\ndescribed and the results and some ideas for future work are discussed.",
    "text": "Instruction and Solution Probabilities as Heuristics for Inductive\n  Programming Instruction subsets (ISs) are heuristics that can shrink the size of the\ninductive programming (IP) search space by tens of orders of magnitude. Here,\nwe extend the IS approach by introducing instruction and solution probabilities\nas additional heuristics. Instruction probability reflects the expectation of\nan instruction occurring in a solution, based on the frequency of instruction\noccurrence in a large code sample. The solution probability for a partial or\ncomplete program is simply the product of all constituent instruction\nprobabilities, including duplicates. We treat the minimum solution\nprobabilities observed in code sample program units of different sizes as\nsolution probability thresholds. These thresholds are used to prune the search\nspace as partial solutions are constructed, thereby eliminating any branches\ncontaining unlikely combinations of instructions. The new approach has been\nevaluated using a large sample of human code. We tested two formulations of\ninstruction probability: one based on instruction occurrence across the entire\ncode sample and another that measured the distribution separately for each IS.\nOur results show that both variants produce substantial further reductions in\nthe IP search space size of up to tens of orders of magnitude, depending on\nsolution size. In combination with IS, reductions of over 100 orders of\nmagnitude can be achieved. We also carried out cross-validation testing to show\nthat the heuristics should work effectively with unseen code. The approach is\ndescribed and the results and some ideas for future work are discussed.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089086
  },
  {
    "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant",
    "abstract": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment.",
    "text": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0890949
  },
  {
    "title": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs",
    "abstract": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora.",
    "text": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891058
  },
  {
    "title": "SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments",
    "abstract": "The increasing complexity of software has led to the steady growth of\nvulnerabilities. Vulnerability repair investigates how to fix software\nvulnerabilities. Manual vulnerability repair is labor-intensive and\ntime-consuming because it relies on human experts, highlighting the importance\nof Automated Vulnerability Repair (AVR). In this SoK, we present the\nsystematization of AVR methods through the three steps of AVR workflow:\nvulnerability analysis, patch generation, and patch validation. We assess AVR\ntools for C/C++ and Java programs as they have been widely studied by the\ncommunity. Since existing AVR tools for C/C++ programs are evaluated with\ndifferent datasets, which often consist of a few vulnerabilities, we construct\nthe first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which\ncontains 144 vulnerabilities as well as their exploits and patches. We use\nVul4C to evaluate seven AVR tools for C/C++ programs and use the third-party\nVul4J dataset to evaluate two AVR tools for Java programs. We also discuss\nfuture research directions.",
    "text": "SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments The increasing complexity of software has led to the steady growth of\nvulnerabilities. Vulnerability repair investigates how to fix software\nvulnerabilities. Manual vulnerability repair is labor-intensive and\ntime-consuming because it relies on human experts, highlighting the importance\nof Automated Vulnerability Repair (AVR). In this SoK, we present the\nsystematization of AVR methods through the three steps of AVR workflow:\nvulnerability analysis, patch generation, and patch validation. We assess AVR\ntools for C/C++ and Java programs as they have been widely studied by the\ncommunity. Since existing AVR tools for C/C++ programs are evaluated with\ndifferent datasets, which often consist of a few vulnerabilities, we construct\nthe first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which\ncontains 144 vulnerabilities as well as their exploits and patches. We use\nVul4C to evaluate seven AVR tools for C/C++ programs and use the third-party\nVul4J dataset to evaluate two AVR tools for Java programs. We also discuss\nfuture research directions.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891159
  },
  {
    "title": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A\n  Feynman-Based Architecture for Continuous Learning Over Streaming Data",
    "abstract": "Real-time continuous learning over streaming data remains a central challenge\nin deep learning and AI systems. Traditional gradient-based models such as\nbackpropagation through time (BPTT) face computational and stability\nlimitations when dealing with temporally unbounded data. In this paper, we\nintroduce a novel architecture, Quantum-Inspired Differentiable Integral Neural\nNetworks (QIDINNs), which leverages the Feynman technique of differentiation\nunder the integral sign to formulate neural updates as integrals over\nhistorical data. This reformulation allows for smoother, more stable learning\ndynamics that are both physically interpretable and computationally tractable.\nInspired by Feynman's path integral formalism and compatible with quantum\ngradient estimation frameworks, QIDINNs open a path toward hybrid\nclassical-quantum neural computation. We demonstrate our model's effectiveness\non synthetic and real-world streaming tasks, and we propose directions for\nquantum extensions and scalable implementations.",
    "text": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A\n  Feynman-Based Architecture for Continuous Learning Over Streaming Data Real-time continuous learning over streaming data remains a central challenge\nin deep learning and AI systems. Traditional gradient-based models such as\nbackpropagation through time (BPTT) face computational and stability\nlimitations when dealing with temporally unbounded data. In this paper, we\nintroduce a novel architecture, Quantum-Inspired Differentiable Integral Neural\nNetworks (QIDINNs), which leverages the Feynman technique of differentiation\nunder the integral sign to formulate neural updates as integrals over\nhistorical data. This reformulation allows for smoother, more stable learning\ndynamics that are both physically interpretable and computationally tractable.\nInspired by Feynman's path integral formalism and compatible with quantum\ngradient estimation frameworks, QIDINNs open a path toward hybrid\nclassical-quantum neural computation. We demonstrate our model's effectiveness\non synthetic and real-world streaming tasks, and we propose directions for\nquantum extensions and scalable implementations.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891268
  },
  {
    "title": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management",
    "abstract": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL.",
    "text": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891368
  },
  {
    "title": "Accelerating Delta Debugging through Probabilistic Monotonicity\n  Assessment",
    "abstract": "Delta debugging assumes search space monotonicity: if a program causes a\nfailure, any supersets of that program will also induce the same failure,\npermitting the exclusion of subsets of non-failure-inducing programs. However,\nthis assumption does not always hold in practice. This paper introduces\nProbabilistic Monotonicity Assessment (PMA), enhancing the efficiency of\nDDMIN-style algorithms without sacrificing effectiveness. PMA dynamically\nmodels and assesses the search space's monotonicity based on prior tests tried\nduring the debugging process and uses a confidence function to quantify\nmonotonicity, thereby enabling the probabilistic exclusion of subsets of\nnon-failure-inducing programs. Our approach significantly reduces redundant\ntests that would otherwise be performed, without compromising the quality of\nthe reduction.\n  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.\nOur findings indicate that PMA cuts processing time by 59.2% compared to\nCHISEL, accelerates the reduction process (i.e., the number of tokens deleted\nper second) by 3.32x, and decreases the sizes of the final reduced programs by\n6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x\nspeedup in the reduction process, and further decreases the sizes of the final\nreduced programs by 3.0%. These findings affirm PMA's role in significantly\nimproving delta debugging's efficiency while maintaining or enhancing its\neffectiveness.",
    "text": "Accelerating Delta Debugging through Probabilistic Monotonicity\n  Assessment Delta debugging assumes search space monotonicity: if a program causes a\nfailure, any supersets of that program will also induce the same failure,\npermitting the exclusion of subsets of non-failure-inducing programs. However,\nthis assumption does not always hold in practice. This paper introduces\nProbabilistic Monotonicity Assessment (PMA), enhancing the efficiency of\nDDMIN-style algorithms without sacrificing effectiveness. PMA dynamically\nmodels and assesses the search space's monotonicity based on prior tests tried\nduring the debugging process and uses a confidence function to quantify\nmonotonicity, thereby enabling the probabilistic exclusion of subsets of\nnon-failure-inducing programs. Our approach significantly reduces redundant\ntests that would otherwise be performed, without compromising the quality of\nthe reduction.\n  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.\nOur findings indicate that PMA cuts processing time by 59.2% compared to\nCHISEL, accelerates the reduction process (i.e., the number of tokens deleted\nper second) by 3.32x, and decreases the sizes of the final reduced programs by\n6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x\nspeedup in the reduction process, and further decreases the sizes of the final\nreduced programs by 3.0%. These findings affirm PMA's role in significantly\nimproving delta debugging's efficiency while maintaining or enhancing its\neffectiveness.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891469
  },
  {
    "title": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis",
    "abstract": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection.",
    "text": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891578
  },
  {
    "title": "Understanding API Usage and Testing: An Empirical Study of C Libraries",
    "abstract": "For library developers, understanding how their Application Programming\nInterfaces (APIs) are used in the field can be invaluable. Knowing how clients\nare using their APIs allows for data-driven decisions on prioritising bug\nreports, feature requests, and testing activities. For example, the priority of\na bug report concerning an API can be partly determined by how widely that API\nis used.\n  In this paper, we present an empirical study in which we analyse API usage\nacross 21 popular open-source C libraries, such as OpenSSL and SQLite, with a\ncombined total of 3,061 C/C++ clients. We compare API usage by clients with how\nwell library test suites exercise the APIs to offer actionable insights for\nlibrary developers. To our knowledge, this is the first study that compares API\nusage and API testing at scale for the C/C++ ecosystem. Our study shows that\nlibrary developers do not prioritise their effort based on how clients use\ntheir API, with popular APIs often poorly tested. For example, in LMDB, a\npopular key-value store, 45% of the APIs are used by clients but not tested by\nthe library test suite. We further show that client test suites can be\nleveraged to improve library testing e.g., improving coverage in LMDB by 14.7%\nwith the important advantage that those tests are representative of how the\nAPIs are used in the field.\n  For our empirical study, we have developed LibProbe, a framework that can be\nused to analyse a large corpus of clients for a given library and produce\nvarious metrics useful to library developers.",
    "text": "Understanding API Usage and Testing: An Empirical Study of C Libraries For library developers, understanding how their Application Programming\nInterfaces (APIs) are used in the field can be invaluable. Knowing how clients\nare using their APIs allows for data-driven decisions on prioritising bug\nreports, feature requests, and testing activities. For example, the priority of\na bug report concerning an API can be partly determined by how widely that API\nis used.\n  In this paper, we present an empirical study in which we analyse API usage\nacross 21 popular open-source C libraries, such as OpenSSL and SQLite, with a\ncombined total of 3,061 C/C++ clients. We compare API usage by clients with how\nwell library test suites exercise the APIs to offer actionable insights for\nlibrary developers. To our knowledge, this is the first study that compares API\nusage and API testing at scale for the C/C++ ecosystem. Our study shows that\nlibrary developers do not prioritise their effort based on how clients use\ntheir API, with popular APIs often poorly tested. For example, in LMDB, a\npopular key-value store, 45% of the APIs are used by clients but not tested by\nthe library test suite. We further show that client test suites can be\nleveraged to improve library testing e.g., improving coverage in LMDB by 14.7%\nwith the important advantage that those tests are representative of how the\nAPIs are used in the field.\n  For our empirical study, we have developed LibProbe, a framework that can be\nused to analyse a large corpus of clients for a given library and produce\nvarious metrics useful to library developers.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891688
  },
  {
    "title": "Further Evidence on a Controversial Topic about Human-Based Experiments:\n  Professionals vs. Students",
    "abstract": "Most Software Engineering (SE) human-based controlled experiments rely on\nstudents as participants, raising concerns about their external validity.\nSpecifically, the realism of results obtained from students and their\napplicability to the software industry remains in question. In this short\npaper, we bring further evidence on this controversial point. To do so, we\ncompare 62 students and 42 software professionals on a bug-fixing task on the\nsame Java program. The students were enrolled in a Bachelor's program in\nComputer Science, while the professionals were employed by two multinational\ncompanies (for one of them, the professionals were from two offices). Some\nvariations in the experimental settings of the two groups (students and\nprofessionals) were present. For instance, the experimental environment of the\nexperiment with professionals was more realistic; i.e., they faced some stress\nfactors such as interruptions during the bug-fixing task. Considering the\ndifferences between the two groups of participants, the gathered data show that\nthe students outperformed the professionals in fixing bugs. This diverges to\nsome extent from past empirical evidence. Rather than presenting definitive\nconclusions, our results aim to catalyze the discussion on the use of students\nin experiments and pave the way for future investigations. Specifically, our\nresults encourage us to examine the complex factors influencing SE tasks,\nmaking experiments as more realistic as possible.",
    "text": "Further Evidence on a Controversial Topic about Human-Based Experiments:\n  Professionals vs. Students Most Software Engineering (SE) human-based controlled experiments rely on\nstudents as participants, raising concerns about their external validity.\nSpecifically, the realism of results obtained from students and their\napplicability to the software industry remains in question. In this short\npaper, we bring further evidence on this controversial point. To do so, we\ncompare 62 students and 42 software professionals on a bug-fixing task on the\nsame Java program. The students were enrolled in a Bachelor's program in\nComputer Science, while the professionals were employed by two multinational\ncompanies (for one of them, the professionals were from two offices). Some\nvariations in the experimental settings of the two groups (students and\nprofessionals) were present. For instance, the experimental environment of the\nexperiment with professionals was more realistic; i.e., they faced some stress\nfactors such as interruptions during the bug-fixing task. Considering the\ndifferences between the two groups of participants, the gathered data show that\nthe students outperformed the professionals in fixing bugs. This diverges to\nsome extent from past empirical evidence. Rather than presenting definitive\nconclusions, our results aim to catalyze the discussion on the use of students\nin experiments and pave the way for future investigations. Specifically, our\nresults encourage us to examine the complex factors influencing SE tasks,\nmaking experiments as more realistic as possible.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089182
  },
  {
    "title": "Retrieval-Augmented Code Review Comment Generation",
    "abstract": "Automated code review comment generation (RCG) aims to assist developers by\nautomatically producing natural language feedback for code changes. Existing\napproaches are primarily either generation-based, using pretrained language\nmodels, or information retrieval-based (IR), reusing comments from similar past\nexamples. While generation-based methods leverage code-specific pretraining on\nlarge code-natural language corpora to learn semantic relationships between\ncode and natural language, they often struggle to generate low-frequency but\nsemantically important tokens due to their probabilistic nature. In contrast,\nIR-based methods excel at recovering such rare tokens by copying from existing\nexamples but lack flexibility in adapting to new code contexts-for example,\nwhen input code contains identifiers or structures not found in the retrieval\ndatabase. To bridge the gap between generation-based and IR-based methods, this\nwork proposes to leverage retrieval-augmented generation (RAG) for RCG by\nconditioning pretrained language models on retrieved code-review exemplars. By\nproviding relevant examples that illustrate how similar code has been\npreviously reviewed, the model is better guided to generate accurate review\ncomments. Our evaluation on the Tufano et al. benchmark shows that RAG-based\nRCG outperforms both generation-based and IR-based RCG. It achieves up to\n+1.67% higher exact match and +4.25% higher BLEU scores compared to\ngeneration-based RCG. It also improves the generation of low-frequency\nground-truth tokens by up to 24.01%. We additionally find that performance\nimproves as the number of retrieved exemplars increases.",
    "text": "Retrieval-Augmented Code Review Comment Generation Automated code review comment generation (RCG) aims to assist developers by\nautomatically producing natural language feedback for code changes. Existing\napproaches are primarily either generation-based, using pretrained language\nmodels, or information retrieval-based (IR), reusing comments from similar past\nexamples. While generation-based methods leverage code-specific pretraining on\nlarge code-natural language corpora to learn semantic relationships between\ncode and natural language, they often struggle to generate low-frequency but\nsemantically important tokens due to their probabilistic nature. In contrast,\nIR-based methods excel at recovering such rare tokens by copying from existing\nexamples but lack flexibility in adapting to new code contexts-for example,\nwhen input code contains identifiers or structures not found in the retrieval\ndatabase. To bridge the gap between generation-based and IR-based methods, this\nwork proposes to leverage retrieval-augmented generation (RAG) for RCG by\nconditioning pretrained language models on retrieved code-review exemplars. By\nproviding relevant examples that illustrate how similar code has been\npreviously reviewed, the model is better guided to generate accurate review\ncomments. Our evaluation on the Tufano et al. benchmark shows that RAG-based\nRCG outperforms both generation-based and IR-based RCG. It achieves up to\n+1.67% higher exact match and +4.25% higher BLEU scores compared to\ngeneration-based RCG. It also improves the generation of low-frequency\nground-truth tokens by up to 24.01%. We additionally find that performance\nimproves as the number of retrieved exemplars increases.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0891929
  },
  {
    "title": "MBSR at Work: Perspectives from an Instructor and Software Developers",
    "abstract": "In this paper, we present the preliminary findings from a qualitative study\n(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction\n(MBSR) program, carried out in the Software Development (SD) working context,\nis perceived by the software developers of a multinational company who\nparticipated in the MBSR program and by the instructor who led it. MBSR is a\ndeeply personal and experiential practice in helping individuals manage stress,\nparticularly in high-pressure environments such as workplaces, healthcare\nsettings, education, and other demanding professional or personal situations.\nAlthough MBSR has been experimented in different working contexts;\nsurprisingly, it has never been studied in the SD working context where there\nare several stress factors that developers experience (e.g., time pressure and\nuncertainty about the content of a particular task and its outcome). In this\nrespect, qualitative research can generate valuable insights into the\napplication of MBSR in the SD working context that cannot be captured by\nstandardized quantitative measures. Being MBSR instructors and software\ndevelopers the key stakeholders in delivering an MBSR program in the SD working\ncontext, understanding their first-hand experiences can provide a more detailed\npicture of the investigated phenomenon. The most important takeaway result of\nour research can be summarized as follows: despite initial skepticism, the\ndevelopers recognized personal improvements due to the MBSR practice, though\nthe integration of MBSR techniques in the working context remained challenging.",
    "text": "MBSR at Work: Perspectives from an Instructor and Software Developers In this paper, we present the preliminary findings from a qualitative study\n(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction\n(MBSR) program, carried out in the Software Development (SD) working context,\nis perceived by the software developers of a multinational company who\nparticipated in the MBSR program and by the instructor who led it. MBSR is a\ndeeply personal and experiential practice in helping individuals manage stress,\nparticularly in high-pressure environments such as workplaces, healthcare\nsettings, education, and other demanding professional or personal situations.\nAlthough MBSR has been experimented in different working contexts;\nsurprisingly, it has never been studied in the SD working context where there\nare several stress factors that developers experience (e.g., time pressure and\nuncertainty about the content of a particular task and its outcome). In this\nrespect, qualitative research can generate valuable insights into the\napplication of MBSR in the SD working context that cannot be captured by\nstandardized quantitative measures. Being MBSR instructors and software\ndevelopers the key stakeholders in delivering an MBSR program in the SD working\ncontext, understanding their first-hand experiences can provide a more detailed\npicture of the investigated phenomenon. The most important takeaway result of\nour research can be summarized as follows: despite initial skepticism, the\ndevelopers recognized personal improvements due to the MBSR practice, though\nthe integration of MBSR techniques in the working context remained challenging.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089204
  },
  {
    "title": "Identifying Helpful Context for LLM-based Vulnerability Repair: A\n  Preliminary Study",
    "abstract": "Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings.",
    "text": "Identifying Helpful Context for LLM-based Vulnerability Repair: A\n  Preliminary Study Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892138
  },
  {
    "title": "Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation",
    "abstract": "In the life-cycle of software development, testing plays a crucial role in\nquality assurance. Proper testing not only increases code coverage and prevents\nregressions but it can also ensure that any potential vulnerabilities in the\nsoftware are identified and effectively fixed. However, creating such tests is\na complex, resource-consuming manual process. To help developers and security\nexperts, this paper explores the automatic unit test generation capability of\none of the most widely used large language models, GPT-4, from the perspective\nof vulnerabilities. We examine a subset of the VUL4J dataset containing real\nvulnerabilities and their corresponding fixes to determine whether GPT-4 can\ngenerate syntactically and/or semantically correct unit tests based on the code\nbefore and after the fixes as evidence of vulnerability mitigation. We focus on\nthe impact of code contexts, the effectiveness of GPT-4's self-correction\nability, and the subjective usability of the generated test cases. Our results\nindicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the\ntime without domain-specific pre-training. Although the semantic correctness of\nthe fixes could be automatically validated in only 7. 5\\% of the cases, our\nsubjective evaluation shows that GPT-4 generally produces test templates that\ncan be further developed into fully functional vulnerability-witnessing tests\nwith relatively minimal manual effort.\n  Therefore, despite the limited data, our initial findings suggest that GPT-4\ncan be effectively used in the generation of vulnerability-witnessing tests. It\nmay not operate entirely autonomously, but it certainly plays a significant\nrole in a partially automated process.",
    "text": "Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation In the life-cycle of software development, testing plays a crucial role in\nquality assurance. Proper testing not only increases code coverage and prevents\nregressions but it can also ensure that any potential vulnerabilities in the\nsoftware are identified and effectively fixed. However, creating such tests is\na complex, resource-consuming manual process. To help developers and security\nexperts, this paper explores the automatic unit test generation capability of\none of the most widely used large language models, GPT-4, from the perspective\nof vulnerabilities. We examine a subset of the VUL4J dataset containing real\nvulnerabilities and their corresponding fixes to determine whether GPT-4 can\ngenerate syntactically and/or semantically correct unit tests based on the code\nbefore and after the fixes as evidence of vulnerability mitigation. We focus on\nthe impact of code contexts, the effectiveness of GPT-4's self-correction\nability, and the subjective usability of the generated test cases. Our results\nindicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the\ntime without domain-specific pre-training. Although the semantic correctness of\nthe fixes could be automatically validated in only 7. 5\\% of the cases, our\nsubjective evaluation shows that GPT-4 generally produces test templates that\ncan be further developed into fully functional vulnerability-witnessing tests\nwith relatively minimal manual effort.\n  Therefore, despite the limited data, our initial findings suggest that GPT-4\ncan be effectively used in the generation of vulnerability-witnessing tests. It\nmay not operate entirely autonomously, but it certainly plays a significant\nrole in a partially automated process.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089225
  },
  {
    "title": "Augmenting the Generality and Performance of Large Language Models for\n  Software Engineering",
    "abstract": "Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising.",
    "text": "Augmenting the Generality and Performance of Large Language Models for\n  Software Engineering Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892348
  },
  {
    "title": "A Procedural Framework for Assessing the Desirability of Process\n  Deviations",
    "abstract": "Conformance checking techniques help process analysts to identify where and\nhow process executions deviate from a process model. However, they cannot\ndetermine the desirability of these deviations, i.e., whether they are\nproblematic, acceptable or even beneficial for the process. Such desirability\nassessments are crucial to derive actions, but process analysts typically\nconduct them in a manual, ad-hoc way, which can be time-consuming, subjective,\nand irreplicable. To address this problem, this paper presents a procedural\nframework to guide process analysts in systematically assessing deviation\ndesirability. It provides a step-by-step approach for identifying which input\nfactors to consider in what order to categorize deviations into mutually\nexclusive desirability categories, each linked to action recommendations. The\nframework is based on a review and conceptualization of existing literature on\ndeviation desirability, which is complemented by empirical insights from\ninterviews with process analysis practitioners and researchers. We evaluate the\nframework through a desirability assessment task conducted with practitioners,\nindicating that the framework effectively enables them to streamline the\nassessment for a thorough yet concise evaluation.",
    "text": "A Procedural Framework for Assessing the Desirability of Process\n  Deviations Conformance checking techniques help process analysts to identify where and\nhow process executions deviate from a process model. However, they cannot\ndetermine the desirability of these deviations, i.e., whether they are\nproblematic, acceptable or even beneficial for the process. Such desirability\nassessments are crucial to derive actions, but process analysts typically\nconduct them in a manual, ad-hoc way, which can be time-consuming, subjective,\nand irreplicable. To address this problem, this paper presents a procedural\nframework to guide process analysts in systematically assessing deviation\ndesirability. It provides a step-by-step approach for identifying which input\nfactors to consider in what order to categorize deviations into mutually\nexclusive desirability categories, each linked to action recommendations. The\nframework is based on a review and conceptualization of existing literature on\ndeviation desirability, which is complemented by empirical insights from\ninterviews with process analysis practitioners and researchers. We evaluate the\nframework through a desirability assessment task conducted with practitioners,\nindicating that the framework effectively enables them to streamline the\nassessment for a thorough yet concise evaluation.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089246
  },
  {
    "title": "VulStamp: Vulnerability Assessment using Large Language Model",
    "abstract": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model.",
    "text": "VulStamp: Vulnerability Assessment using Large Language Model Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089256
  },
  {
    "title": "Enhancing Clinical Decision Support and EHR Insights through LLMs and\n  the Model Context Protocol: An Open-Source MCP-FHIR Framework",
    "abstract": "Enhancing clinical decision support (CDS), reducing documentation burdens,\nand improving patient health literacy remain persistent challenges in digital\nhealth. This paper presents an open-source, agent-based framework that\nintegrates Large Language Models (LLMs) with HL7 FHIR data via the Model\nContext Protocol (MCP) for dynamic extraction and reasoning over electronic\nhealth records (EHRs). Built on the established MCP-FHIR implementation, the\nframework enables declarative access to diverse FHIR resources through\nJSON-based configurations, supporting real-time summarization, interpretation,\nand personalized communication across multiple user personas, including\nclinicians, caregivers, and patients. To ensure privacy and reproducibility,\nthe framework is evaluated using synthetic EHR data from the SMART Health IT\nsandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4\nstandard. Unlike traditional approaches that rely on hardcoded retrieval and\nstatic workflows, the proposed method delivers scalable, explainable, and\ninteroperable AI-powered EHR applications. The agentic architecture further\nsupports multiple FHIR formats, laying a robust foundation for advancing\npersonalized digital health solutions.",
    "text": "Enhancing Clinical Decision Support and EHR Insights through LLMs and\n  the Model Context Protocol: An Open-Source MCP-FHIR Framework Enhancing clinical decision support (CDS), reducing documentation burdens,\nand improving patient health literacy remain persistent challenges in digital\nhealth. This paper presents an open-source, agent-based framework that\nintegrates Large Language Models (LLMs) with HL7 FHIR data via the Model\nContext Protocol (MCP) for dynamic extraction and reasoning over electronic\nhealth records (EHRs). Built on the established MCP-FHIR implementation, the\nframework enables declarative access to diverse FHIR resources through\nJSON-based configurations, supporting real-time summarization, interpretation,\nand personalized communication across multiple user personas, including\nclinicians, caregivers, and patients. To ensure privacy and reproducibility,\nthe framework is evaluated using synthetic EHR data from the SMART Health IT\nsandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4\nstandard. Unlike traditional approaches that rely on hardcoded retrieval and\nstatic workflows, the proposed method delivers scalable, explainable, and\ninteroperable AI-powered EHR applications. The agentic architecture further\nsupports multiple FHIR formats, laying a robust foundation for advancing\npersonalized digital health solutions.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892649
  },
  {
    "title": "Understanding the Issue Types in Open Source Blockchain-based Software\n  Projects with the Transformer-based BERTopic",
    "abstract": "Blockchain-based software systems are increasingly deployed across diverse\ndomains, yet a systematic understanding of their development challenges remains\nlimited. This paper presents a large-scale empirical study of 497,742 issues\nmined from 1,209 open-source blockchain projects hosted on GitHub. Employing\nBERTopic, a transformer-based topic modeling technique, we identify 49 distinct\nissue topics and organize them hierarchically into 11 major subcategories. Our\nanalysis reveals that both general software development issues and\nblockchain-specific concerns are nearly equally represented, with Wallet\nManagement and UI Enhancement emerging as the most prominent topics. We further\nexamine the temporal evolution of issue categories and resolution times,\nfinding that Wallet issues not only dominate in frequency but also exhibit the\nlongest resolution time. Conversely, Mechanisms issues are resolved\nsignificantly faster. Issue frequency surged after 2016 with the rise of\nEthereum and decentralized applications, but declined after 2022. These\nfindings enhance our understanding of blockchain software maintenance,\ninforming the development of specialized tools and practices to improve\nrobustness and maintainability.",
    "text": "Understanding the Issue Types in Open Source Blockchain-based Software\n  Projects with the Transformer-based BERTopic Blockchain-based software systems are increasingly deployed across diverse\ndomains, yet a systematic understanding of their development challenges remains\nlimited. This paper presents a large-scale empirical study of 497,742 issues\nmined from 1,209 open-source blockchain projects hosted on GitHub. Employing\nBERTopic, a transformer-based topic modeling technique, we identify 49 distinct\nissue topics and organize them hierarchically into 11 major subcategories. Our\nanalysis reveals that both general software development issues and\nblockchain-specific concerns are nearly equally represented, with Wallet\nManagement and UI Enhancement emerging as the most prominent topics. We further\nexamine the temporal evolution of issue categories and resolution times,\nfinding that Wallet issues not only dominate in frequency but also exhibit the\nlongest resolution time. Conversely, Mechanisms issues are resolved\nsignificantly faster. Issue frequency surged after 2016 with the rise of\nEthereum and decentralized applications, but declined after 2022. These\nfindings enhance our understanding of blockchain software maintenance,\ninforming the development of specialized tools and practices to improve\nrobustness and maintainability.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892758
  },
  {
    "title": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification",
    "abstract": "Recent advances in reinforcement learning (RL) with verifiable outcome\nrewards have significantly improved the reasoning capabilities of large\nlanguage models (LLMs), especially when combined with multi-turn tool\ninteractions. However, existing methods lack both meaningful verification\nsignals from realistic environments and explicit optimization for verification,\nleading to unreliable self-verification. To address these limitations, we\npropose ReVeal, a multi-turn reinforcement learning framework that interleaves\ncode generation with explicit self-verification and tool-based evaluation.\nReVeal enables LLMs to autonomously generate test cases, invoke external tools\nfor precise feedback, and improves performance via a customized RL algorithm\nwith dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a\nmodel's generation and verification capabilities through RL training, expanding\nthe reasoning boundaries of the base model, demonstrated by significant gains\nin Pass@k on LiveCodeBench. It also enables test-time scaling into deeper\ninference regimes, with code consistently evolving as the number of turns\nincreases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.\nThese findings highlight the promise of ReVeal as a scalable and effective\nparadigm for building more robust and autonomous AI agents.",
    "text": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification Recent advances in reinforcement learning (RL) with verifiable outcome\nrewards have significantly improved the reasoning capabilities of large\nlanguage models (LLMs), especially when combined with multi-turn tool\ninteractions. However, existing methods lack both meaningful verification\nsignals from realistic environments and explicit optimization for verification,\nleading to unreliable self-verification. To address these limitations, we\npropose ReVeal, a multi-turn reinforcement learning framework that interleaves\ncode generation with explicit self-verification and tool-based evaluation.\nReVeal enables LLMs to autonomously generate test cases, invoke external tools\nfor precise feedback, and improves performance via a customized RL algorithm\nwith dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a\nmodel's generation and verification capabilities through RL training, expanding\nthe reasoning boundaries of the base model, demonstrated by significant gains\nin Pass@k on LiveCodeBench. It also enables test-time scaling into deeper\ninference regimes, with code consistently evolving as the number of turns\nincreases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.\nThese findings highlight the promise of ReVeal as a scalable and effective\nparadigm for building more robust and autonomous AI agents.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892851
  },
  {
    "title": "A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing\n  Pipeline",
    "abstract": "Autonomous drones are rapidly reshaping industries ranging from aerial\ndelivery and infrastructure inspection to environmental monitoring and disaster\nresponse. Ensuring the safety, reliability, and efficiency of these systems is\nparamount as they transition from research prototypes to mission-critical\nplatforms. This paper presents a step-by-step guide to establishing a robust\nautonomous drone testing pipeline, covering each critical stage:\nSoftware-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)\nTesting, Controlled Real-World Testing, and In-Field Testing. Using practical\nexamples, including the marker-based autonomous landing system, we demonstrate\nhow to systematically verify drone system behaviors, identify integration\nissues, and optimize performance. Furthermore, we highlight emerging trends\nshaping the future of drone testing, including the integration of Neurosymbolic\nand LLMs, creating co-simulation environments, and Digital Twin-enabled\nsimulation-based testing techniques. By following this pipeline, developers and\nresearchers can achieve comprehensive validation, minimize deployment risks,\nand prepare autonomous drones for safe and reliable real-world operations.",
    "text": "A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing\n  Pipeline Autonomous drones are rapidly reshaping industries ranging from aerial\ndelivery and infrastructure inspection to environmental monitoring and disaster\nresponse. Ensuring the safety, reliability, and efficiency of these systems is\nparamount as they transition from research prototypes to mission-critical\nplatforms. This paper presents a step-by-step guide to establishing a robust\nautonomous drone testing pipeline, covering each critical stage:\nSoftware-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)\nTesting, Controlled Real-World Testing, and In-Field Testing. Using practical\nexamples, including the marker-based autonomous landing system, we demonstrate\nhow to systematically verify drone system behaviors, identify integration\nissues, and optimize performance. Furthermore, we highlight emerging trends\nshaping the future of drone testing, including the integration of Neurosymbolic\nand LLMs, creating co-simulation environments, and Digital Twin-enabled\nsimulation-based testing techniques. By following this pipeline, developers and\nresearchers can achieve comprehensive validation, minimize deployment risks,\nand prepare autonomous drones for safe and reliable real-world operations.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0892951
  },
  {
    "title": "A Tale of Two Systems: Characterizing Architectural Complexity on\n  Machine Learning-Enabled Systems",
    "abstract": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper brings, side-by-side, the\narchitecture representation of two systems that can be used as case studies for\ncreating the metrics-based architectural model: the SPIRA and the Ocean Guard\nMLES.",
    "text": "A Tale of Two Systems: Characterizing Architectural Complexity on\n  Machine Learning-Enabled Systems How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper brings, side-by-side, the\narchitecture representation of two systems that can be used as case studies for\ncreating the metrics-based architectural model: the SPIRA and the Ocean Guard\nMLES.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893052
  },
  {
    "title": "Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling\n  Evaluation",
    "abstract": "Large language models (LLMs) are routinely deployed as agentic systems, with\naccess to tools that interact with live environments to accomplish tasks. In\nenterprise deployments these systems need to interact with API collections that\ncan be extremely large and complex, often backed by databases. In order to\ncreate datasets with such characteristics, we explore how existing NL2SQL\n(Natural Language to SQL query) datasets can be used to automatically create\nNL2API datasets. Specifically, this work describes a novel data generation\npipeline that exploits the syntax of SQL queries to construct a functionally\nequivalent sequence of API calls. We apply this pipeline to one of the largest\nNL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be\nserved as invocable tools or REST-endpoints. We pair natural language queries\nfrom BIRD-SQL to ground-truth API sequences based on this API pool. We use this\ncollection to study the performance of 10 public LLMs and find that all models\nstruggle to determine the right set of tools (consisting of tasks of intent\ndetection, sequencing with nested function calls, and slot-filling). We find\nthat models have extremely low task completion rates (7-47 percent - depending\non the dataset) which marginally improves to 50 percent when models are\nemployed as ReACT agents that interact with the live API environment. The best\ntask completion rates are far below what may be required for effective\ngeneral-use tool-calling agents, suggesting substantial scope for improvement\nin current state-of-the-art tool-calling LLMs. We also conduct detailed\nablation studies, such as assessing the impact of the number of tools available\nas well as the impact of tool and slot-name obfuscation. We compare the\nperformance of models on the original SQL generation tasks and find that\ncurrent models are sometimes able to exploit SQL better than APIs.",
    "text": "Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling\n  Evaluation Large language models (LLMs) are routinely deployed as agentic systems, with\naccess to tools that interact with live environments to accomplish tasks. In\nenterprise deployments these systems need to interact with API collections that\ncan be extremely large and complex, often backed by databases. In order to\ncreate datasets with such characteristics, we explore how existing NL2SQL\n(Natural Language to SQL query) datasets can be used to automatically create\nNL2API datasets. Specifically, this work describes a novel data generation\npipeline that exploits the syntax of SQL queries to construct a functionally\nequivalent sequence of API calls. We apply this pipeline to one of the largest\nNL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be\nserved as invocable tools or REST-endpoints. We pair natural language queries\nfrom BIRD-SQL to ground-truth API sequences based on this API pool. We use this\ncollection to study the performance of 10 public LLMs and find that all models\nstruggle to determine the right set of tools (consisting of tasks of intent\ndetection, sequencing with nested function calls, and slot-filling). We find\nthat models have extremely low task completion rates (7-47 percent - depending\non the dataset) which marginally improves to 50 percent when models are\nemployed as ReACT agents that interact with the live API environment. The best\ntask completion rates are far below what may be required for effective\ngeneral-use tool-calling agents, suggesting substantial scope for improvement\nin current state-of-the-art tool-calling LLMs. We also conduct detailed\nablation studies, such as assessing the impact of the number of tools available\nas well as the impact of tool and slot-name obfuscation. We compare the\nperformance of models on the original SQL generation tasks and find that\ncurrent models are sometimes able to exploit SQL better than APIs.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893161
  },
  {
    "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and\n  Refinement for Natural Language to Bash in IT Automation",
    "abstract": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement.",
    "text": "LLM-as-a-Judge for Reference-less Automatic Code Validation and\n  Refinement for Natural Language to Bash in IT Automation In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893261
  },
  {
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "abstract": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "text": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089337
  },
  {
    "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework",
    "abstract": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.",
    "text": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089348
  },
  {
    "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.",
    "text": "Evaluating Large Language Models on Non-Code Software Engineering Tasks Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893571
  },
  {
    "title": "Solving Package Management via Hypergraph Dependency Resolution",
    "abstract": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.",
    "text": "Solving Package Management via Hypergraph Dependency Resolution Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893672
  },
  {
    "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on\n  AI-Powered Mobile Apps",
    "abstract": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.",
    "text": "What Users Value and Critique: Large-Scale Analysis of User Feedback on\n  AI-Powered Mobile Apps Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089378
  },
  {
    "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring\n  Operational Machine Learning Models",
    "abstract": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.",
    "text": "From Tea Leaves to System Maps: Context-awareness in Monitoring\n  Operational Machine Learning Models Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.0893881
  },
  {
    "title": "Formalising Software Requirements using Large Language Models",
    "abstract": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.",
    "text": "Formalising Software Requirements using Large Language Models This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089398
  },
  {
    "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context\n  Protocol in Manufacturing",
    "abstract": "Explicit modeling of capabilities and skills -- whether based on ontologies,\nAsset Administration Shells, or other technologies -- requires considerable\nmanual effort and often results in representations that are not easily\naccessible to Large Language Models (LLMs). In this work-in-progress paper, we\npresent an alternative approach based on the recently introduced Model Context\nProtocol (MCP). MCP allows systems to expose functionality through a\nstandardized interface that is directly consumable by LLM-based agents. We\nconduct a prototypical evaluation on a laboratory-scale manufacturing system,\nwhere resource functions are made available via MCP. A general-purpose LLM is\nthen tasked with planning and executing a multi-step process, including\nconstraint handling and the invocation of resource functions via MCP. The\nresults indicate that such an approach can enable flexible industrial\nautomation without relying on explicit semantic models. This work lays the\nbasis for further exploration of external tool integration in LLM-driven\nproduction systems.",
    "text": "Beyond Formal Semantics for Capabilities and Skills: Model Context\n  Protocol in Manufacturing Explicit modeling of capabilities and skills -- whether based on ontologies,\nAsset Administration Shells, or other technologies -- requires considerable\nmanual effort and often results in representations that are not easily\naccessible to Large Language Models (LLMs). In this work-in-progress paper, we\npresent an alternative approach based on the recently introduced Model Context\nProtocol (MCP). MCP allows systems to expose functionality through a\nstandardized interface that is directly consumable by LLM-based agents. We\nconduct a prototypical evaluation on a laboratory-scale manufacturing system,\nwhere resource functions are made available via MCP. A general-purpose LLM is\nthen tasked with planning and executing a multi-step process, including\nconstraint handling and the invocation of resource functions via MCP. The\nresults indicate that such an approach can enable flexible industrial\nautomation without relying on explicit semantic models. This work lays the\nbasis for further exploration of external tool integration in LLM-driven\nproduction systems.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089409
  },
  {
    "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From\n  GitHub",
    "abstract": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.",
    "text": "Not One to Rule Them All: Mining Meaningful Code Review Orders From\n  GitHub Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.08942
  },
  {
    "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC,\n  QEMU and Containerization",
    "abstract": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.",
    "text": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC,\n  QEMU and Containerization The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.",
    "url": "",
    "category": "cs.SE",
    "source": "arxiv",
    "timestamp": 1750404537.089431
  },
  {
    "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards",
    "abstract": "Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.",
    "text": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.593885
  },
  {
    "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos",
    "abstract": "Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .",
    "text": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939012
  },
  {
    "title": "Dense SAE Latents Are Features, Not Bugs",
    "abstract": "Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.",
    "text": "Dense SAE Latents Are Features, Not Bugs Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.593911
  },
  {
    "title": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear\n  Dynamical Systems",
    "abstract": "This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.",
    "text": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear\n  Dynamical Systems This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59392
  },
  {
    "title": "On the Upper Bounds for the Matrix Spectral Norm",
    "abstract": "We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.",
    "text": "On the Upper Bounds for the Matrix Spectral Norm We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59393
  },
  {
    "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy\n  Optimization",
    "abstract": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.",
    "text": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy\n  Optimization Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939388
  },
  {
    "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning",
    "abstract": "Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.",
    "text": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59395
  },
  {
    "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning",
    "abstract": "Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.",
    "text": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59396
  },
  {
    "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses",
    "abstract": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.",
    "text": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939689
  },
  {
    "title": "Revisiting Randomization in Greedy Model Search",
    "abstract": "Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.",
    "text": "Revisiting Randomization in Greedy Model Search Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939798
  },
  {
    "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction",
    "abstract": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.",
    "text": "Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939898
  },
  {
    "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification",
    "abstract": "Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.",
    "text": "GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5939999
  },
  {
    "title": "The Compositional Architecture of Regret in Large Language Models",
    "abstract": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.",
    "text": "The Compositional Architecture of Regret in Large Language Models Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59401
  },
  {
    "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
    "abstract": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.",
    "text": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5940192
  },
  {
    "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts",
    "abstract": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.",
    "text": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594029
  },
  {
    "title": "Memory-Efficient Differentially Private Training with Gradient Random\n  Projection",
    "abstract": "Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.",
    "text": "Memory-Efficient Differentially Private Training with Gradient Random\n  Projection Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594039
  },
  {
    "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh\n  Smoothing",
    "abstract": "Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.",
    "text": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh\n  Smoothing Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594049
  },
  {
    "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents",
    "abstract": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
    "text": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594058
  },
  {
    "title": "Task-Agnostic Experts Composition for Continual Learning",
    "abstract": "Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.",
    "text": "Task-Agnostic Experts Composition for Continual Learning Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5940669
  },
  {
    "title": "Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates",
    "abstract": "Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.",
    "text": "Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594076
  },
  {
    "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones",
    "abstract": "Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.",
    "text": "DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594086
  },
  {
    "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning",
    "abstract": "Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.",
    "text": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594095
  },
  {
    "title": "Learning Algorithms in the Limit",
    "abstract": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.",
    "text": "Learning Algorithms in the Limit This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594104
  },
  {
    "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework",
    "abstract": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).",
    "text": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594114
  },
  {
    "title": "A Simplified Analysis of SGD for Linear Regression with Weight Averaging",
    "abstract": "Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.",
    "text": "A Simplified Analysis of SGD for Linear Regression with Weight Averaging Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5941231
  },
  {
    "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music\n  Diffusion Models",
    "abstract": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.",
    "text": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music\n  Diffusion Models Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594135
  },
  {
    "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation",
    "abstract": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.",
    "text": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594145
  },
  {
    "title": "Over-squashing in Spatiotemporal Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.",
    "text": "Over-squashing in Spatiotemporal Graph Neural Networks Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594154
  },
  {
    "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a\n  Systematic Literature Review",
    "abstract": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.",
    "text": "Insights on Adversarial Attacks for Tabular Machine Learning via a\n  Systematic Literature Review Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594163
  },
  {
    "title": "Time-dependent density estimation using binary classifiers",
    "abstract": "We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.",
    "text": "Time-dependent density estimation using binary classifiers We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594174
  },
  {
    "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge",
    "abstract": "Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.",
    "text": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594184
  },
  {
    "title": "Pixel-level Certified Explanations via Randomized Smoothing",
    "abstract": "Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.",
    "text": "Pixel-level Certified Explanations via Randomized Smoothing Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594193
  },
  {
    "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling",
    "abstract": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.",
    "text": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594203
  },
  {
    "title": "LIT-LVM: Structured Regularization for Interaction Terms in Linear\n  Predictors using Latent Variable Models",
    "abstract": "Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.",
    "text": "LIT-LVM: Structured Regularization for Interaction Terms in Linear\n  Predictors using Latent Variable Models Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594213
  },
  {
    "title": "Creating User-steerable Projections with Interactive Semantic Mapping",
    "abstract": "Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.",
    "text": "Creating User-steerable Projections with Interactive Semantic Mapping Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594222
  },
  {
    "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI",
    "abstract": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.",
    "text": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594232
  },
  {
    "title": "Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic\n  Graphs",
    "abstract": "Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.",
    "text": "Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic\n  Graphs Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59424
  },
  {
    "title": "All is Not Lost: LLM Recovery without Checkpoints",
    "abstract": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.",
    "text": "All is Not Lost: LLM Recovery without Checkpoints Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594251
  },
  {
    "title": "Warping and Matching Subsequences Between Time Series",
    "abstract": "Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.",
    "text": "Warping and Matching Subsequences Between Time Series Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59426
  },
  {
    "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning",
    "abstract": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.",
    "text": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59427
  },
  {
    "title": "Zero-Shot Reinforcement Learning Under Partial Observability",
    "abstract": "Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.",
    "text": "Zero-Shot Reinforcement Learning Under Partial Observability Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5942788
  },
  {
    "title": "Reward Models in Deep Reinforcement Learning: A Survey",
    "abstract": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.",
    "text": "Reward Models in Deep Reinforcement Learning: A Survey In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5942888
  },
  {
    "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI",
    "abstract": "Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.",
    "text": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594302
  },
  {
    "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level\n  Relevance",
    "abstract": "Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.",
    "text": "NERO: Explainable Out-of-Distribution Detection with Neuron-level\n  Relevance Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594313
  },
  {
    "title": "Learn to Vaccinate: Combining Structure Learning and Effective\n  Vaccination for Epidemic and Outbreak Control",
    "abstract": "The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.",
    "text": "Learn to Vaccinate: Combining Structure Learning and Effective\n  Vaccination for Epidemic and Outbreak Control The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594322
  },
  {
    "title": "Multi-Timescale Gradient Sliding for Distributed Optimization",
    "abstract": "We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).",
    "text": "Multi-Timescale Gradient Sliding for Distributed Optimization We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5943308
  },
  {
    "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models",
    "abstract": "Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.",
    "text": "Provable Maximum Entropy Manifold Exploration via Diffusion Models Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594341
  },
  {
    "title": "Global Ground Metric Learning with Applications to scRNA data",
    "abstract": "Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.",
    "text": "Global Ground Metric Learning with Applications to scRNA data Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59435
  },
  {
    "title": "Sampling 3D Molecular Conformers with Diffusion Transformers",
    "abstract": "Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.",
    "text": "Sampling 3D Molecular Conformers with Diffusion Transformers Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59436
  },
  {
    "title": "Performative Validity of Recourse Explanations",
    "abstract": "When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.",
    "text": "Performative Validity of Recourse Explanations When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594369
  },
  {
    "title": "Enhancing One-run Privacy Auditing with Quantile Regression-Based\n  Membership Inference",
    "abstract": "Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.",
    "text": "Enhancing One-run Privacy Auditing with Quantile Regression-Based\n  Membership Inference Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594379
  },
  {
    "title": "Acoustic Waveform Inversion with Image-to-Image Schrdinger Bridges",
    "abstract": "Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.",
    "text": "Acoustic Waveform Inversion with Image-to-Image Schrdinger Bridges Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594389
  },
  {
    "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural\n  Network-Based Molecular Dynamics Simulations",
    "abstract": "Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.",
    "text": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural\n  Network-Based Molecular Dynamics Simulations Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594399
  },
  {
    "title": "Universal Laboratory Model: prognosis of abnormal clinical outcomes\n  based on routine tests",
    "abstract": "Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.",
    "text": "Universal Laboratory Model: prognosis of abnormal clinical outcomes\n  based on routine tests Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594409
  },
  {
    "title": "When and How Unlabeled Data Provably Improve In-Context Learning",
    "abstract": "Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.",
    "text": "When and How Unlabeled Data Provably Improve In-Context Learning Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5944178
  },
  {
    "title": "Proximal Operators of Sorted Nonconvex Penalties",
    "abstract": "This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.",
    "text": "Proximal Operators of Sorted Nonconvex Penalties This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5944269
  },
  {
    "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation",
    "abstract": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.",
    "text": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5944371
  },
  {
    "title": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language\n  Models Using Forward-Only Passes",
    "abstract": "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.",
    "text": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language\n  Models Using Forward-Only Passes Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5944471
  },
  {
    "title": "Conditional Generative Modeling for Enhanced Credit Risk Management in\n  Supply Chain Finance",
    "abstract": "The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.",
    "text": "Conditional Generative Modeling for Enhanced Credit Risk Management in\n  Supply Chain Finance The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594458
  },
  {
    "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification",
    "abstract": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.",
    "text": "ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5944672
  },
  {
    "title": "DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for\n  Optimizing Large-Scale EV Charging Infrastructure",
    "abstract": "The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.",
    "text": "DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for\n  Optimizing Large-Scale EV Charging Infrastructure The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594477
  },
  {
    "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data",
    "abstract": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.",
    "text": "Unlocking Post-hoc Dataset Inference with Synthetic Data The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594486
  },
  {
    "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning",
    "abstract": "Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.",
    "text": "Centroid Approximation for Byzantine-Tolerant Federated Learning Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594496
  },
  {
    "title": "Minimizing Structural Vibrations via Guided Flow Matching Design\n  Optimization",
    "abstract": "Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.",
    "text": "Minimizing Structural Vibrations via Guided Flow Matching Design\n  Optimization Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594507
  },
  {
    "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model",
    "abstract": "Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.",
    "text": "Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594517
  },
  {
    "title": "Context-Aware Deep Lagrangian Networks for Model Predictive Control",
    "abstract": "Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.",
    "text": "Context-Aware Deep Lagrangian Networks for Model Predictive Control Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594527
  },
  {
    "title": "Interpretability and Generalization Bounds for Learning Spatial Physics",
    "abstract": "While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.",
    "text": "Interpretability and Generalization Bounds for Learning Spatial Physics While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594537
  },
  {
    "title": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal\n  Behaviors",
    "abstract": "Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.",
    "text": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal\n  Behaviors Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594547
  },
  {
    "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning",
    "abstract": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.",
    "text": "Classification of Multi-Parametric Body MRI Series Using Deep Learning Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594557
  },
  {
    "title": "ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed\n  Machine Learning",
    "abstract": "Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.",
    "text": "ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed\n  Machine Learning Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594568
  },
  {
    "title": "In-Context Learning for Gradient-Free Receiver Adaptation: Principles,\n  Applications, and Theory",
    "abstract": "In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.",
    "text": "In-Context Learning for Gradient-Free Receiver Adaptation: Principles,\n  Applications, and Theory In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594578
  },
  {
    "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rnyi Divergence-Based Solution",
    "abstract": "Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.",
    "text": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rnyi Divergence-Based Solution Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5945878
  },
  {
    "title": "Towards Reliable Forgetting: A Survey on Machine Unlearning\n  Verification, Challenges, and Future Directions",
    "abstract": "With growing demands for privacy protection, security, and legal compliance\n(e.g., GDPR), machine unlearning has emerged as a critical technique for\nensuring the controllability and regulatory alignment of machine learning\nmodels. However, a fundamental challenge in this field lies in effectively\nverifying whether unlearning operations have been successfully and thoroughly\nexecuted. Despite a growing body of work on unlearning techniques, verification\nmethodologies remain comparatively underexplored and often fragmented. Existing\napproaches lack a unified taxonomy and a systematic framework for evaluation.\nTo bridge this gap, this paper presents the first structured survey of machine\nunlearning verification methods. We propose a taxonomy that organizes current\ntechniques into two principal categories -- behavioral verification and\nparametric verification -- based on the type of evidence used to assess\nunlearning fidelity. We examine representative methods within each category,\nanalyze their underlying assumptions, strengths, and limitations, and identify\npotential vulnerabilities in practical deployment. In closing, we articulate a\nset of open problems in current verification research, aiming to provide a\nfoundation for developing more robust, efficient, and theoretically grounded\nunlearning verification mechanisms.",
    "text": "Towards Reliable Forgetting: A Survey on Machine Unlearning\n  Verification, Challenges, and Future Directions With growing demands for privacy protection, security, and legal compliance\n(e.g., GDPR), machine unlearning has emerged as a critical technique for\nensuring the controllability and regulatory alignment of machine learning\nmodels. However, a fundamental challenge in this field lies in effectively\nverifying whether unlearning operations have been successfully and thoroughly\nexecuted. Despite a growing body of work on unlearning techniques, verification\nmethodologies remain comparatively underexplored and often fragmented. Existing\napproaches lack a unified taxonomy and a systematic framework for evaluation.\nTo bridge this gap, this paper presents the first structured survey of machine\nunlearning verification methods. We propose a taxonomy that organizes current\ntechniques into two principal categories -- behavioral verification and\nparametric verification -- based on the type of evidence used to assess\nunlearning fidelity. We examine representative methods within each category,\nanalyze their underlying assumptions, strengths, and limitations, and identify\npotential vulnerabilities in practical deployment. In closing, we articulate a\nset of open problems in current verification research, aiming to provide a\nfoundation for developing more robust, efficient, and theoretically grounded\nunlearning verification mechanisms.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5945978
  },
  {
    "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region\n  Representation Learning",
    "abstract": "Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.",
    "text": "Transit for All: Mapping Equitable Bike2Subway Connection using Region\n  Representation Learning Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5946271
  },
  {
    "title": "Neural Canonical Polyadic Factorization for Traffic Analysis",
    "abstract": "Modern intelligent transportation systems rely on accurate spatiotemporal\ntraffic analysis to optimize urban mobility and infrastructure resilience.\nHowever, pervasive missing data caused by sensor failures and heterogeneous\nsensing gaps fundamentally hinders reliable traffic modeling. This paper\nproposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes\nlow-rank tensor algebra with deep representation learning for robust traffic\ndata imputation. The model innovatively embeds CP decomposition into neural\narchitecture through learnable embedding projections, where sparse traffic\ntensors are encoded into dense latent factors across road segments, time\nintervals, and mobility metrics. A hierarchical feature fusion mechanism\nemploys Hadamard products to explicitly model multilinear interactions, while\nstacked multilayer perceptron layers nonlinearly refine these representations\nto capture complex spatiotemporal couplings. Extensive evaluations on six urban\ntraffic datasets demonstrate NCPF's superiority over six state-of-the-art\nbaselines. By unifying CP decomposition's interpretable factor analysis with\nneural network's nonlinear expressive power, NCPF provides a principled yet\nflexible approaches for high-dimensional traffic data imputation, offering\ncritical support for next-generation transportation digital twins and adaptive\ntraffic control systems.",
    "text": "Neural Canonical Polyadic Factorization for Traffic Analysis Modern intelligent transportation systems rely on accurate spatiotemporal\ntraffic analysis to optimize urban mobility and infrastructure resilience.\nHowever, pervasive missing data caused by sensor failures and heterogeneous\nsensing gaps fundamentally hinders reliable traffic modeling. This paper\nproposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes\nlow-rank tensor algebra with deep representation learning for robust traffic\ndata imputation. The model innovatively embeds CP decomposition into neural\narchitecture through learnable embedding projections, where sparse traffic\ntensors are encoded into dense latent factors across road segments, time\nintervals, and mobility metrics. A hierarchical feature fusion mechanism\nemploys Hadamard products to explicitly model multilinear interactions, while\nstacked multilayer perceptron layers nonlinearly refine these representations\nto capture complex spatiotemporal couplings. Extensive evaluations on six urban\ntraffic datasets demonstrate NCPF's superiority over six state-of-the-art\nbaselines. By unifying CP decomposition's interpretable factor analysis with\nneural network's nonlinear expressive power, NCPF provides a principled yet\nflexible approaches for high-dimensional traffic data imputation, offering\ncritical support for next-generation transportation digital twins and adaptive\ntraffic control systems.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594639
  },
  {
    "title": "Enhancing Vector Quantization with Distributional Matching: A\n  Theoretical and Empirical Study",
    "abstract": "The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.",
    "text": "Enhancing Vector Quantization with Distributional Matching: A\n  Theoretical and Empirical Study The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5946481
  },
  {
    "title": "Learning-Time Encoding Shapes Unlearning in LLMs",
    "abstract": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.",
    "text": "Learning-Time Encoding Shapes Unlearning in LLMs As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5946581
  },
  {
    "title": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID\n  Datasets",
    "abstract": "In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE.",
    "text": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID\n  Datasets In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5946681
  },
  {
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
    "abstract": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "text": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.59469
  },
  {
    "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by\n  Large Language Models",
    "abstract": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.",
    "text": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by\n  Large Language Models Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594702
  },
  {
    "title": "HiPreNets: High-Precision Neural Networks through Progressive Training",
    "abstract": "Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.",
    "text": "HiPreNets: High-Precision Neural Networks through Progressive Training Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594712
  },
  {
    "title": "Muon Optimizes Under Spectral Norm Constraints",
    "abstract": "The pursuit of faster optimization algorithms remains an active and important\nresearch direction in deep learning. Recently, the Muon optimizer [JJB+24] has\ndemonstrated promising empirical performance, but its theoretical foundation\nremains less understood. In this paper, we bridge this gap and provide a\ntheoretical analysis of Muon by placing it within the Lion-$\\mathcal{K}$ family\nof optimizers [CLLL24]. Specifically, we show that Muon corresponds to\nLion-$\\mathcal{K}$ when equipped with the nuclear norm, and we leverage the\ntheoretical results of Lion-$\\mathcal{K}$ to establish that Muon (with\ndecoupled weight decay) implicitly solves an optimization problem that enforces\na constraint on the spectral norm of weight matrices. This perspective not only\ndemystifies the implicit regularization effects of Muon but also leads to\nnatural generalizations through varying the choice of convex map $\\mathcal{K}$,\nallowing for the exploration of a broader class of implicitly regularized and\nconstrained optimization algorithms.",
    "text": "Muon Optimizes Under Spectral Norm Constraints The pursuit of faster optimization algorithms remains an active and important\nresearch direction in deep learning. Recently, the Muon optimizer [JJB+24] has\ndemonstrated promising empirical performance, but its theoretical foundation\nremains less understood. In this paper, we bridge this gap and provide a\ntheoretical analysis of Muon by placing it within the Lion-$\\mathcal{K}$ family\nof optimizers [CLLL24]. Specifically, we show that Muon corresponds to\nLion-$\\mathcal{K}$ when equipped with the nuclear norm, and we leverage the\ntheoretical results of Lion-$\\mathcal{K}$ to establish that Muon (with\ndecoupled weight decay) implicitly solves an optimization problem that enforces\na constraint on the spectral norm of weight matrices. This perspective not only\ndemystifies the implicit regularization effects of Muon but also leads to\nnatural generalizations through varying the choice of convex map $\\mathcal{K}$,\nallowing for the exploration of a broader class of implicitly regularized and\nconstrained optimization algorithms.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594721
  },
  {
    "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization",
    "abstract": "Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.",
    "text": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594738
  },
  {
    "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled\n  Medical Devices",
    "abstract": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.",
    "text": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled\n  Medical Devices The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594749
  },
  {
    "title": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size",
    "abstract": "Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.",
    "text": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594765
  },
  {
    "title": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large\n  Language Models",
    "abstract": "Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.",
    "text": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large\n  Language Models Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5947762
  },
  {
    "title": "Data analysis using discrete cubical homology",
    "abstract": "We present a new tool for data analysis: persistence discrete homology, which\nis well-suited to analyze filtrations of graphs. In particular, we provide a\nnovel way of representing high-dimensional data as a filtration of graphs using\npairwise correlations. We discuss several applications of these tools, e.g., in\nweather and financial data, comparing them to the standard methods used in the\nrespective fields.",
    "text": "Data analysis using discrete cubical homology We present a new tool for data analysis: persistence discrete homology, which\nis well-suited to analyze filtrations of graphs. In particular, we provide a\nnovel way of representing high-dimensional data as a filtration of graphs using\npairwise correlations. We discuss several applications of these tools, e.g., in\nweather and financial data, comparing them to the standard methods used in the\nrespective fields.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5947862
  },
  {
    "title": "Stable CDE Autoencoders with Acuity Regularization for Offline\n  Reinforcement Learning in Sepsis Treatment",
    "abstract": "Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.",
    "text": "Stable CDE Autoencoders with Acuity Regularization for Offline\n  Reinforcement Learning in Sepsis Treatment Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5947971
  },
  {
    "title": "Private Continual Counting of Unbounded Streams",
    "abstract": "We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$.",
    "text": "Private Continual Counting of Unbounded Streams We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948071
  },
  {
    "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees\n  in Industrial URLLC",
    "abstract": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.",
    "text": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees\n  in Industrial URLLC Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594823
  },
  {
    "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings",
    "abstract": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.",
    "text": "Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948339
  },
  {
    "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement\n  in Real-World Noisy Environments",
    "abstract": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions.",
    "text": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement\n  in Real-World Noisy Environments Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948439
  },
  {
    "title": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple\n  Choice Settings",
    "abstract": "As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.",
    "text": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple\n  Choice Settings As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948539
  },
  {
    "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits",
    "abstract": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.",
    "text": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948632
  },
  {
    "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in\n  Industrial URLLC",
    "abstract": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.",
    "text": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in\n  Industrial URLLC Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5948732
  },
  {
    "title": "Early Prediction of Multiple Sclerosis Disability Progression via\n  Multimodal Foundation Model Benchmarks",
    "abstract": "Early multiple sclerosis (MS) disability progression prediction is\nchallenging due to disease heterogeneity. This work predicts 48- and 72-week\ndisability using sparse baseline clinical data and 12 weeks of daily digital\nFloodlight data from the CONSONANCE clinical trial. We employed\nstate-of-the-art tabular and time-series foundation models (FMs), a custom\nmultimodal attention-based transformer, and machine learning methods. Despite\nthe difficulty of early prediction (AUROC 0.63), integrating digital data via\nadvanced models improved performance over clinical data alone. A transformer\nmodel using unimodal embeddings from the Moment FM yielded the best result, but\nour multimodal transformer consistently outperformed its unimodal counterpart,\nconfirming the advantages of combining clinical with digital data. Our findings\ndemonstrate the promise of FMs and multimodal approaches to extract predictive\nsignals from complex and diverse clinical and digital life sciences data (e.g.,\nimaging, omics), enabling more accurate prognostics for MS and potentially\nother complex diseases.",
    "text": "Early Prediction of Multiple Sclerosis Disability Progression via\n  Multimodal Foundation Model Benchmarks Early multiple sclerosis (MS) disability progression prediction is\nchallenging due to disease heterogeneity. This work predicts 48- and 72-week\ndisability using sparse baseline clinical data and 12 weeks of daily digital\nFloodlight data from the CONSONANCE clinical trial. We employed\nstate-of-the-art tabular and time-series foundation models (FMs), a custom\nmultimodal attention-based transformer, and machine learning methods. Despite\nthe difficulty of early prediction (AUROC 0.63), integrating digital data via\nadvanced models improved performance over clinical data alone. A transformer\nmodel using unimodal embeddings from the Moment FM yielded the best result, but\nour multimodal transformer consistently outperformed its unimodal counterpart,\nconfirming the advantages of combining clinical with digital data. Our findings\ndemonstrate the promise of FMs and multimodal approaches to extract predictive\nsignals from complex and diverse clinical and digital life sciences data (e.g.,\nimaging, omics), enabling more accurate prognostics for MS and potentially\nother complex diseases.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594892
  },
  {
    "title": "Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays",
    "abstract": "Synaptic delays play a crucial role in biological neuronal networks, where\ntheir modulation has been observed in mammalian learning processes. In the\nrealm of neuromorphic computing, although spiking neural networks (SNNs) aim to\nemulate biology more closely than traditional artificial neural networks do,\nsynaptic delays are rarely incorporated into their simulation. We introduce a\nnovel learning rule for simultaneously learning synaptic connection strengths\nand delays, by extending spike-timing dependent plasticity (STDP), a Hebbian\nmethod commonly used for learning synaptic weights. We validate our approach by\nextending a widely-used SNN model for classification trained with unsupervised\nlearning. Then we demonstrate the effectiveness of our new method by comparing\nit against another existing methods for co-learning synaptic weights and delays\nas well as against STDP without synaptic delays. Results demonstrate that our\nproposed method consistently achieves superior performance across a variety of\ntest scenarios. Furthermore, our experimental results yield insight into the\ninterplay between synaptic efficacy and delay.",
    "text": "Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays Synaptic delays play a crucial role in biological neuronal networks, where\ntheir modulation has been observed in mammalian learning processes. In the\nrealm of neuromorphic computing, although spiking neural networks (SNNs) aim to\nemulate biology more closely than traditional artificial neural networks do,\nsynaptic delays are rarely incorporated into their simulation. We introduce a\nnovel learning rule for simultaneously learning synaptic connection strengths\nand delays, by extending spike-timing dependent plasticity (STDP), a Hebbian\nmethod commonly used for learning synaptic weights. We validate our approach by\nextending a widely-used SNN model for classification trained with unsupervised\nlearning. Then we demonstrate the effectiveness of our new method by comparing\nit against another existing methods for co-learning synaptic weights and delays\nas well as against STDP without synaptic delays. Results demonstrate that our\nproposed method consistently achieves superior performance across a variety of\ntest scenarios. Furthermore, our experimental results yield insight into the\ninterplay between synaptic efficacy and delay.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594902
  },
  {
    "title": "ODD: Overlap-aware Estimation of Model Performance under Distribution\n  Shift",
    "abstract": "Reliable and accurate estimation of the error of an ML model in unseen test\ndomains is an important problem for safe intelligent systems. Prior work uses\ndisagreement discrepancy (DIS^2) to derive practical error bounds under\ndistribution shifts. It optimizes for a maximally disagreeing classifier on the\ntarget domain to bound the error of a given source classifier. Although this\napproach offers a reliable and competitively accurate estimate of the target\nerror, we identify a problem in this approach which causes the disagreement\ndiscrepancy objective to compete in the overlapping region between source and\ntarget domains. With an intuitive assumption that the target disagreement\nshould be no more than the source disagreement in the overlapping region due to\nhigh enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).\nMaximizing ODD only requires disagreement in the non-overlapping target domain,\nremoving the competition. Our ODD-based bound uses domain-classifiers to\nestimate domain-overlap and better predicts target performance than DIS^2. We\nconduct experiments on a wide array of benchmarks to show that our method\nimproves the overall performance-estimation error while remaining valid and\nreliable. Our code and results are available on GitHub.",
    "text": "ODD: Overlap-aware Estimation of Model Performance under Distribution\n  Shift Reliable and accurate estimation of the error of an ML model in unseen test\ndomains is an important problem for safe intelligent systems. Prior work uses\ndisagreement discrepancy (DIS^2) to derive practical error bounds under\ndistribution shifts. It optimizes for a maximally disagreeing classifier on the\ntarget domain to bound the error of a given source classifier. Although this\napproach offers a reliable and competitively accurate estimate of the target\nerror, we identify a problem in this approach which causes the disagreement\ndiscrepancy objective to compete in the overlapping region between source and\ntarget domains. With an intuitive assumption that the target disagreement\nshould be no more than the source disagreement in the overlapping region due to\nhigh enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).\nMaximizing ODD only requires disagreement in the non-overlapping target domain,\nremoving the competition. Our ODD-based bound uses domain-classifiers to\nestimate domain-overlap and better predicts target performance than DIS^2. We\nconduct experiments on a wide array of benchmarks to show that our method\nimproves the overall performance-estimation error while remaining valid and\nreliable. Our code and results are available on GitHub.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.5949109
  },
  {
    "title": "NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for\n  Multi-Modal Neurological Disorder Classification",
    "abstract": "The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.",
    "text": "NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for\n  Multi-Modal Neurological Disorder Classification The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594929
  },
  {
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "abstract": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "text": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "url": "",
    "category": "cs.LG",
    "source": "arxiv",
    "timestamp": 1750404538.594939
  },
  {
    "title": "Dense SAE Latents Are Features, Not Bugs",
    "abstract": "Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.",
    "text": "Dense SAE Latents Are Features, Not Bugs Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097366
  },
  {
    "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
    "abstract": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "text": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097388
  },
  {
    "title": "Sekai: A Video Dataset towards World Exploration",
    "abstract": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "text": "Sekai: A Video Dataset towards World Exploration Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0974
  },
  {
    "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
    "abstract": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.",
    "text": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097413
  },
  {
    "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
    "abstract": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "text": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0974238
  },
  {
    "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning",
    "abstract": "Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.",
    "text": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097437
  },
  {
    "title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning\n  Models for Self-Guided Efficiency Enhancement",
    "abstract": "Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.",
    "text": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning\n  Models for Self-Guided Efficiency Enhancement Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097451
  },
  {
    "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
    "abstract": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
    "text": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0974631
  },
  {
    "title": "The AI Policy Module: Developing Computer Science Student Competency in\n  AI Ethics and Policy",
    "abstract": "As artificial intelligence (AI) further embeds itself into many settings\nacross personal and professional contexts, increasing attention must be paid\nnot only to AI ethics, but also to the governance and regulation of AI\ntechnologies through AI policy. However, the prevailing post-secondary\ncomputing curriculum is currently ill-equipped to prepare future AI\npractitioners to confront increasing demands to implement abstract ethical\nprinciples and normative policy preferences into the design and development of\nAI systems. We believe that familiarity with the 'AI policy landscape' and the\nability to translate ethical principles to practices will in the future\nconstitute an important responsibility for even the most technically-focused AI\nengineers.\n  Toward preparing current computer science (CS) students for these new\nexpectations, we developed an AI Policy Module to introduce discussions of AI\npolicy into the CS curriculum. Building on a successful pilot in fall 2024, in\nthis innovative practice full paper we present an updated and expanded version\nof the module, including a technical assignment on \"AI regulation\". We present\nthe findings from our pilot of the AI Policy Module 2.0, evaluating student\nattitudes towards AI ethics and policy through pre- and post-module surveys.\nFollowing the module, students reported increased concern about the ethical\nimpacts of AI technologies while also expressing greater confidence in their\nabilities to engage in discussions about AI regulation. Finally, we highlight\nthe AI Regulation Assignment as an effective and engaging tool for exploring\nthe limits of AI alignment and emphasizing the role of 'policy' in addressing\nethical challenges.",
    "text": "The AI Policy Module: Developing Computer Science Student Competency in\n  AI Ethics and Policy As artificial intelligence (AI) further embeds itself into many settings\nacross personal and professional contexts, increasing attention must be paid\nnot only to AI ethics, but also to the governance and regulation of AI\ntechnologies through AI policy. However, the prevailing post-secondary\ncomputing curriculum is currently ill-equipped to prepare future AI\npractitioners to confront increasing demands to implement abstract ethical\nprinciples and normative policy preferences into the design and development of\nAI systems. We believe that familiarity with the 'AI policy landscape' and the\nability to translate ethical principles to practices will in the future\nconstitute an important responsibility for even the most technically-focused AI\nengineers.\n  Toward preparing current computer science (CS) students for these new\nexpectations, we developed an AI Policy Module to introduce discussions of AI\npolicy into the CS curriculum. Building on a successful pilot in fall 2024, in\nthis innovative practice full paper we present an updated and expanded version\nof the module, including a technical assignment on \"AI regulation\". We present\nthe findings from our pilot of the AI Policy Module 2.0, evaluating student\nattitudes towards AI ethics and policy through pre- and post-module surveys.\nFollowing the module, students reported increased concern about the ethical\nimpacts of AI technologies while also expressing greater confidence in their\nabilities to engage in discussions about AI regulation. Finally, we highlight\nthe AI Regulation Assignment as an effective and engaging tool for exploring\nthe limits of AI alignment and emphasizing the role of 'policy' in addressing\nethical challenges.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097476
  },
  {
    "title": "Revisiting Compositional Generalization Capability of Large Language\n  Models Considering Instruction Following Ability",
    "abstract": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.",
    "text": "Revisiting Compositional Generalization Capability of Large Language\n  Models Considering Instruction Following Ability In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097488
  },
  {
    "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction",
    "abstract": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.",
    "text": "Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097503
  },
  {
    "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic\n  Routing Games",
    "abstract": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.",
    "text": "The Effect of State Representation on LLM Agent Behavior in Dynamic\n  Routing Games Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097516
  },
  {
    "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification",
    "abstract": "Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.",
    "text": "GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097527
  },
  {
    "title": "The Compositional Architecture of Regret in Large Language Models",
    "abstract": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.",
    "text": "The Compositional Architecture of Regret in Large Language Models Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0975392
  },
  {
    "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
    "abstract": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.",
    "text": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0975502
  },
  {
    "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with\n  Narrative and Difficulty Concerns",
    "abstract": "While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.",
    "text": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with\n  Narrative and Difficulty Concerns While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0975611
  },
  {
    "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts",
    "abstract": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.",
    "text": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097573
  },
  {
    "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution",
    "abstract": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.",
    "text": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097584
  },
  {
    "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents",
    "abstract": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
    "text": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097595
  },
  {
    "title": "Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates",
    "abstract": "Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.",
    "text": "Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097606
  },
  {
    "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones",
    "abstract": "Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.",
    "text": "DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097619
  },
  {
    "title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse\n  Myocardial Scar Synthesis and Segmentation",
    "abstract": "Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.",
    "text": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse\n  Myocardial Scar Synthesis and Segmentation Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097631
  },
  {
    "title": "Learning Algorithms in the Limit",
    "abstract": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.",
    "text": "Learning Algorithms in the Limit This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097642
  },
  {
    "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and\n  Network Sparsity",
    "abstract": "We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.",
    "text": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and\n  Network Sparsity We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097655
  },
  {
    "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework",
    "abstract": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).",
    "text": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097666
  },
  {
    "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation",
    "abstract": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.",
    "text": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0976782
  },
  {
    "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in\n  LangChain A CoT-Enhanced Prompt Engineering Approach",
    "abstract": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.",
    "text": "Optimizing Web-Based AI Query Retrieval with GPT Integration in\n  LangChain A CoT-Enhanced Prompt Engineering Approach Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09769
  },
  {
    "title": "Over-squashing in Spatiotemporal Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.",
    "text": "Over-squashing in Spatiotemporal Graph Neural Networks Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0977018
  },
  {
    "title": "Pixel-level Certified Explanations via Randomized Smoothing",
    "abstract": "Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.",
    "text": "Pixel-level Certified Explanations via Randomized Smoothing Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097713
  },
  {
    "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling",
    "abstract": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.",
    "text": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097725
  },
  {
    "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis\n  for Unseen Objects",
    "abstract": "While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.",
    "text": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis\n  for Unseen Objects While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097738
  },
  {
    "title": "Context-Informed Grounding Supervision",
    "abstract": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.",
    "text": "Context-Informed Grounding Supervision Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0977511
  },
  {
    "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI",
    "abstract": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.",
    "text": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097764
  },
  {
    "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
    "abstract": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
    "text": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097777
  },
  {
    "title": "Uncovering Intention through LLM-Driven Code Snippet Description\n  Generation",
    "abstract": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.",
    "text": "Uncovering Intention through LLM-Driven Code Snippet Description\n  Generation Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0977888
  },
  {
    "title": "Warping and Matching Subsequences Between Time Series",
    "abstract": "Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.",
    "text": "Warping and Matching Subsequences Between Time Series Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0977988
  },
  {
    "title": "Zero-Shot Reinforcement Learning Under Partial Observability",
    "abstract": "Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.",
    "text": "Zero-Shot Reinforcement Learning Under Partial Observability Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09781
  },
  {
    "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material",
    "abstract": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.",
    "text": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material 3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0978198
  },
  {
    "title": "Reward Models in Deep Reinforcement Learning: A Survey",
    "abstract": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.",
    "text": "Reward Models in Deep Reinforcement Learning: A Survey In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0978298
  },
  {
    "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI",
    "abstract": "Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.",
    "text": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0978408
  },
  {
    "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System",
    "abstract": "Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.",
    "text": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097853
  },
  {
    "title": "A Real-time Endoscopic Image Denoising System",
    "abstract": "Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.",
    "text": "A Real-time Endoscopic Image Denoising System Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097864
  },
  {
    "title": "Evaluation Pipeline for systematically searching for Anomaly Detection\n  Systems",
    "abstract": "Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation.",
    "text": "Evaluation Pipeline for systematically searching for Anomaly Detection\n  Systems Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097874
  },
  {
    "title": "Efficient and Generalizable Environmental Understanding for Visual\n  Navigation",
    "abstract": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.",
    "text": "Efficient and Generalizable Environmental Understanding for Visual\n  Navigation Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0978858
  },
  {
    "title": "Open-World Object Counting in Videos",
    "abstract": "We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.",
    "text": "Open-World Object Counting in Videos We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097895
  },
  {
    "title": "When and How Unlabeled Data Provably Improve In-Context Learning",
    "abstract": "Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.",
    "text": "When and How Unlabeled Data Provably Improve In-Context Learning Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0979059
  },
  {
    "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image\n  Sensor",
    "abstract": "This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.",
    "text": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image\n  Sensor This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097916
  },
  {
    "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual\n  Learning",
    "abstract": "In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.",
    "text": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual\n  Learning In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0979269
  },
  {
    "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation",
    "abstract": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.",
    "text": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097939
  },
  {
    "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification",
    "abstract": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.",
    "text": "ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09795
  },
  {
    "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment",
    "abstract": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.",
    "text": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.097992
  },
  {
    "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with\n  Garment-aware Diffusion Models",
    "abstract": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.",
    "text": "Human Motion Capture from Loose and Sparse Inertial Sensors with\n  Garment-aware Diffusion Models Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098007
  },
  {
    "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data",
    "abstract": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.",
    "text": "Unlocking Post-hoc Dataset Inference with Synthetic Data The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098018
  },
  {
    "title": "Domain Adaptation for Image Classification of Defects in Semiconductor\n  Manufacturing",
    "abstract": "In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.",
    "text": "Domain Adaptation for Image Classification of Defects in Semiconductor\n  Manufacturing In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0980291
  },
  {
    "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM\n  Agents in Real-World Environments",
    "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.",
    "text": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM\n  Agents in Real-World Environments The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09804
  },
  {
    "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model",
    "abstract": "Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.",
    "text": "Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098051
  },
  {
    "title": "Joint Computation Offloading and Resource Allocation for Uncertain\n  Maritime MEC via Cooperation of UAVs and Vessels",
    "abstract": "The computation demands from the maritime Internet of Things (MIoT) increase\nrapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels\nbased multi-access edge computing (MEC) can fulfill these MIoT requirements.\nHowever, the uncertain maritime tasks present significant challenges of\ninefficient computation offloading and resource allocation. In this paper, we\nfocus on the maritime computation offloading and resource allocation through\nthe cooperation of UAVs and vessels, with consideration of uncertain tasks.\nSpecifically, we propose a cooperative MEC framework for computation offloading\nand resource allocation, including MIoT devices, UAVs and vessels. Then, we\nformulate the optimization problem to minimize the total execution time. As for\nthe uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the\nunpredictable task arrivals and varying computational resource availability. By\nconverting the long-term constraints into short-term constraints, we obtain a\nset of small-scale optimization problems. Further, considering the\nheterogeneity of actions and resources of UAVs and vessels, we reformulate the\nsmall-scale optimization problem into a Markov game (MG). Moreover, a\nheterogeneous-agent soft actor-critic is proposed to sequentially update\nvarious neural networks and effectively solve the MG problem. Finally,\nsimulations are conducted to verify the effectiveness in addressing\ncomputational offloading and resource allocation.",
    "text": "Joint Computation Offloading and Resource Allocation for Uncertain\n  Maritime MEC via Cooperation of UAVs and Vessels The computation demands from the maritime Internet of Things (MIoT) increase\nrapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels\nbased multi-access edge computing (MEC) can fulfill these MIoT requirements.\nHowever, the uncertain maritime tasks present significant challenges of\ninefficient computation offloading and resource allocation. In this paper, we\nfocus on the maritime computation offloading and resource allocation through\nthe cooperation of UAVs and vessels, with consideration of uncertain tasks.\nSpecifically, we propose a cooperative MEC framework for computation offloading\nand resource allocation, including MIoT devices, UAVs and vessels. Then, we\nformulate the optimization problem to minimize the total execution time. As for\nthe uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the\nunpredictable task arrivals and varying computational resource availability. By\nconverting the long-term constraints into short-term constraints, we obtain a\nset of small-scale optimization problems. Further, considering the\nheterogeneity of actions and resources of UAVs and vessels, we reformulate the\nsmall-scale optimization problem into a Markov game (MG). Moreover, a\nheterogeneous-agent soft actor-critic is proposed to sequentially update\nvarious neural networks and effectively solve the MG problem. Finally,\nsimulations are conducted to verify the effectiveness in addressing\ncomputational offloading and resource allocation.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098061
  },
  {
    "title": "A Comparative Study of Task Adaptation Techniques of Large Language\n  Models for Identifying Sustainable Development Goals",
    "abstract": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).",
    "text": "A Comparative Study of Task Adaptation Techniques of Large Language\n  Models for Identifying Sustainable Development Goals In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09807
  },
  {
    "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth\n  Observation: A Realistic Case Study",
    "abstract": "The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised\nEarth Observation (EO) missions, addressing challenges in climate monitoring,\ndisaster management, and more. However, autonomous coordination in\nmulti-satellite systems remains a fundamental challenge. Traditional\noptimisation approaches struggle to handle the real-time decision-making\ndemands of dynamic EO missions, necessitating the use of Reinforcement Learning\n(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we\ninvestigate RL-based autonomous EO mission planning by modelling\nsingle-satellite operations and extending to multi-satellite constellations\nusing MARL frameworks. We address key challenges, including energy and data\nstorage limitations, uncertainties in satellite observations, and the\ncomplexities of decentralised coordination under partial observability. By\nleveraging a near-realistic satellite simulation environment, we evaluate the\ntraining stability and performance of state-of-the-art MARL algorithms,\nincluding PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can\neffectively balance imaging and resource management while addressing\nnon-stationarity and reward interdependency in multi-satellite coordination.\nThe insights gained from this study provide a foundation for autonomous\nsatellite operations, offering practical guidelines for improving policy\nlearning in decentralised EO missions.",
    "text": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth\n  Observation: A Realistic Case Study The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised\nEarth Observation (EO) missions, addressing challenges in climate monitoring,\ndisaster management, and more. However, autonomous coordination in\nmulti-satellite systems remains a fundamental challenge. Traditional\noptimisation approaches struggle to handle the real-time decision-making\ndemands of dynamic EO missions, necessitating the use of Reinforcement Learning\n(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we\ninvestigate RL-based autonomous EO mission planning by modelling\nsingle-satellite operations and extending to multi-satellite constellations\nusing MARL frameworks. We address key challenges, including energy and data\nstorage limitations, uncertainties in satellite observations, and the\ncomplexities of decentralised coordination under partial observability. By\nleveraging a near-realistic satellite simulation environment, we evaluate the\ntraining stability and performance of state-of-the-art MARL algorithms,\nincluding PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can\neffectively balance imaging and resource management while addressing\nnon-stationarity and reward interdependency in multi-satellite coordination.\nThe insights gained from this study provide a foundation for autonomous\nsatellite operations, offering practical guidelines for improving policy\nlearning in decentralised EO missions.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098081
  },
  {
    "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
    "abstract": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
    "text": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0980911
  },
  {
    "title": "Accessible Gesture-Driven Augmented Reality Interaction System",
    "abstract": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction",
    "text": "Accessible Gesture-Driven Augmented Reality Interaction System Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098101
  },
  {
    "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning",
    "abstract": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.",
    "text": "Classification of Multi-Parametric Body MRI Series Using Deep Learning Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098112
  },
  {
    "title": "LLM Agent for Hyper-Parameter Optimization",
    "abstract": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.",
    "text": "LLM Agent for Hyper-Parameter Optimization Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098122
  },
  {
    "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
    "abstract": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "text": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098132
  },
  {
    "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for\n  Generative Language Models",
    "abstract": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.",
    "text": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for\n  Generative Language Models This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098142
  },
  {
    "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs",
    "abstract": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.",
    "text": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098151
  },
  {
    "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rnyi Divergence-Based Solution",
    "abstract": "Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.",
    "text": "Advancing Loss Functions in Recommender Systems: A Comparative Study\n  with a Rnyi Divergence-Based Solution Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098176
  },
  {
    "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region\n  Representation Learning",
    "abstract": "Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.",
    "text": "Transit for All: Mapping Equitable Bike2Subway Connection using Region\n  Representation Learning Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098188
  },
  {
    "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance\n  Clarification",
    "abstract": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.",
    "text": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance\n  Clarification Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0981991
  },
  {
    "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization",
    "abstract": "Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.",
    "text": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098218
  },
  {
    "title": "Truncated Proximal Policy Optimization",
    "abstract": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "text": "Truncated Proximal Policy Optimization Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098229
  },
  {
    "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in\n  Mental Health Support for Alzheimer's and Dementia Caregivers",
    "abstract": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.",
    "text": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in\n  Mental Health Support for Alzheimer's and Dementia Caregivers Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098239
  },
  {
    "title": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size",
    "abstract": "Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.",
    "text": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0982559
  },
  {
    "title": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large\n  Language Models",
    "abstract": "Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.",
    "text": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large\n  Language Models Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0982678
  },
  {
    "title": "Stable CDE Autoencoders with Acuity Regularization for Offline\n  Reinforcement Learning in Sepsis Treatment",
    "abstract": "Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.",
    "text": "Stable CDE Autoencoders with Acuity Regularization for Offline\n  Reinforcement Learning in Sepsis Treatment Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098279
  },
  {
    "title": "Insights Informed Generative AI for Design: Incorporating Real-world\n  Data for Text-to-Image Output",
    "abstract": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.",
    "text": "Insights Informed Generative AI for Design: Incorporating Real-world\n  Data for Text-to-Image Output Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0982988
  },
  {
    "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models",
    "abstract": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.",
    "text": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09831
  },
  {
    "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings",
    "abstract": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.",
    "text": "Memory Tokens: Large Language Models Can Generate Reversible Sentence\n  Embeddings In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098322
  },
  {
    "title": "Improved Image Reconstruction and Diffusion Parameter Estimation Using a\n  Temporal Convolutional Network Model of Gradient Trajectory Errors",
    "abstract": "Summary: Errors in gradient trajectories introduce significant artifacts and\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\nPurpose: Our objective is to develop a general, nonlinear gradient system model\nthat can accurately predict gradient distortions using convolutional networks.\nMethods: A set of training gradient waveforms were measured on a small animal\nimaging system, and used to train a temporal convolutional network to predict\nthe gradient waveforms produced by the imaging system. Results: The trained\nnetwork was able to accurately predict nonlinear distortions produced by the\ngradient system. Network prediction of gradient waveforms was incorporated into\nthe image reconstruction pipeline and provided improvements in image quality\nand diffusion parameter mapping compared to both the nominal gradient waveform\nand the gradient impulse response function. Conclusion: Temporal convolutional\nnetworks can more accurately model gradient system behavior than existing\nlinear methods and may be used to retrospectively correct gradient errors.",
    "text": "Improved Image Reconstruction and Diffusion Parameter Estimation Using a\n  Temporal Convolutional Network Model of Gradient Trajectory Errors Summary: Errors in gradient trajectories introduce significant artifacts and\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\nPurpose: Our objective is to develop a general, nonlinear gradient system model\nthat can accurately predict gradient distortions using convolutional networks.\nMethods: A set of training gradient waveforms were measured on a small animal\nimaging system, and used to train a temporal convolutional network to predict\nthe gradient waveforms produced by the imaging system. Results: The trained\nnetwork was able to accurately predict nonlinear distortions produced by the\ngradient system. Network prediction of gradient waveforms was incorporated into\nthe image reconstruction pipeline and provided improvements in image quality\nand diffusion parameter mapping compared to both the nominal gradient waveform\nand the gradient impulse response function. Conclusion: Temporal convolutional\nnetworks can more accurately model gradient system behavior than existing\nlinear methods and may be used to retrospectively correct gradient errors.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098333
  },
  {
    "title": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning",
    "abstract": "Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms, with environment availability strongly\nimpacting research. One particularly underexplored intersection is continual\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\nbenchmarks run environments on the CPU, leading to computational bottlenecks\nand limiting the length of task sequences. MEAL leverages JAX for GPU\nacceleration, enabling continual learning across sequences of 100 tasks on a\nstandard desktop PC in a few hours. We show that naively combining popular CL\nand MARL methods yields strong performance on simple environments, but fails to\nscale to more complex settings requiring sustained coordination and adaptation.\nOur ablation study identifies architectural and algorithmic features critical\nfor CMARL on MEAL.",
    "text": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms, with environment availability strongly\nimpacting research. One particularly underexplored intersection is continual\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\nbenchmarks run environments on the CPU, leading to computational bottlenecks\nand limiting the length of task sequences. MEAL leverages JAX for GPU\nacceleration, enabling continual learning across sequences of 100 tasks on a\nstandard desktop PC in a few hours. We show that naively combining popular CL\nand MARL methods yields strong performance on simple environments, but fails to\nscale to more complex settings requiring sustained coordination and adaptation.\nOur ablation study identifies architectural and algorithmic features critical\nfor CMARL on MEAL.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0983481
  },
  {
    "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits",
    "abstract": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.",
    "text": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098358
  },
  {
    "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker\n  Directional Speech Recognition",
    "abstract": "Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks.",
    "text": "Thinking in Directivity: Speech Large Language Model for Multi-Talker\n  Directional Speech Recognition Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0983691
  },
  {
    "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild\n  Personalization",
    "abstract": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast",
    "text": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild\n  Personalization Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09838
  },
  {
    "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
    "abstract": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "text": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098391
  },
  {
    "title": "Flat Channels to Infinity in Neural Loss Landscapes",
    "abstract": "The loss landscapes of neural networks contain minima and saddle points that\nmay be connected in flat regions or appear in isolation. We identify and\ncharacterize a special structure in the loss landscape: channels along which\nthe loss decreases extremely slowly, while the output weights of at least two\nneurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight\nvectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At\nconvergence, the two neurons implement a gated linear unit:\n$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot\n\\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot\n\\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\ncritical points. Gradient flow solvers, and related optimization methods like\nSGD or ADAM, reach the channels with high probability in diverse regression\nsettings, but without careful inspection they look like flat local minima with\nfinite parameter values. Our characterization provides a comprehensive picture\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\nfunctional interpretation. The emergence of gated linear units at the end of\nthe channels highlights a surprising aspect of the computational capabilities\nof fully connected layers.",
    "text": "Flat Channels to Infinity in Neural Loss Landscapes The loss landscapes of neural networks contain minima and saddle points that\nmay be connected in flat regions or appear in isolation. We identify and\ncharacterize a special structure in the loss landscape: channels along which\nthe loss decreases extremely slowly, while the output weights of at least two\nneurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight\nvectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At\nconvergence, the two neurons implement a gated linear unit:\n$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot\n\\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot\n\\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\ncritical points. Gradient flow solvers, and related optimization methods like\nSGD or ADAM, reach the channels with high probability in diverse regression\nsettings, but without careful inspection they look like flat local minima with\nfinite parameter values. Our characterization provides a comprehensive picture\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\nfunctional interpretation. The emergence of gated linear units at the end of\nthe channels highlights a surprising aspect of the computational capabilities\nof fully connected layers.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098407
  },
  {
    "title": "Determinao Automtica de Limiar de Deteco de Ataques\n  em Redes de Computadores Utilizando Autoencoders",
    "abstract": "Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.",
    "text": "Determinao Automtica de Limiar de Deteco de Ataques\n  em Redes de Computadores Utilizando Autoencoders Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.09842
  },
  {
    "title": "CALM: Contextual Analog Logic with Multimodality",
    "abstract": "In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\nCALM unites symbolic reasoning with neural generation, enabling systems to make\ncontext-sensitive decisions grounded in real-world multi-modal data.\n  Background: Classic bivalent logic systems cannot capture the nuance of human\ndecision-making. They also require human grounding in multi-modal environments,\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\nrich contextual information from multi-modal data, but lack interpretable\nstructures for reasoning.\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\ncreating an analog logic that can reason over multi-modal inputs. Without this\nintegration, AI systems remain either brittle or unstructured, unable to\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\nto analog truth values computed by neural networks and constrained search.\n  Methods: CALM represents each predicate using a domain tree, which\niteratively refines its analog truth value when the contextual groundings of\nits entities are determined. The iterative refinement is predicted by neural\nnetworks capable of capturing multi-modal information and is filtered through a\nsymbolic reasoning module to ensure constraint satisfaction.\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\nalso demonstrated spatial heatmap generation aligned with logical constraints\nand delicate human preferences, as shown by a human study.\n  Conclusions: CALM demonstrates the potential to reason with logic structure\nwhile aligning with preferences in multi-modal environments. It lays the\nfoundation for next-gen AI systems that require the precision and\ninterpretation of logic and the multimodal information processing of neural\nnetworks.",
    "text": "CALM: Contextual Analog Logic with Multimodality In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\nCALM unites symbolic reasoning with neural generation, enabling systems to make\ncontext-sensitive decisions grounded in real-world multi-modal data.\n  Background: Classic bivalent logic systems cannot capture the nuance of human\ndecision-making. They also require human grounding in multi-modal environments,\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\nrich contextual information from multi-modal data, but lack interpretable\nstructures for reasoning.\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\ncreating an analog logic that can reason over multi-modal inputs. Without this\nintegration, AI systems remain either brittle or unstructured, unable to\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\nto analog truth values computed by neural networks and constrained search.\n  Methods: CALM represents each predicate using a domain tree, which\niteratively refines its analog truth value when the contextual groundings of\nits entities are determined. The iterative refinement is predicted by neural\nnetworks capable of capturing multi-modal information and is filtered through a\nsymbolic reasoning module to ensure constraint satisfaction.\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\nalso demonstrated spatial heatmap generation aligned with logical constraints\nand delicate human preferences, as shown by a human study.\n  Conclusions: CALM demonstrates the potential to reason with logic structure\nwhile aligning with preferences in multi-modal environments. It lays the\nfoundation for next-gen AI systems that require the precision and\ninterpretation of logic and the multimodal information processing of neural\nnetworks.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0984309
  },
  {
    "title": "Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based\n  Crypto Anomaly Detection",
    "abstract": "The decentralized finance (DeFi) community has grown rapidly in recent years,\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\npotential of new markets. The surge in popularity of cryptocurrency has ushered\nin a new era of financial crime. Unfortunately, the novelty of the technology\nmakes the task of catching and prosecuting offenders particularly challenging.\nThus, it is necessary to implement automated detection tools related to\npolicies to address the growing criminality in the cryptocurrency realm.",
    "text": "Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based\n  Crypto Anomaly Detection The decentralized finance (DeFi) community has grown rapidly in recent years,\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\npotential of new markets. The surge in popularity of cryptocurrency has ushered\nin a new era of financial crime. Unfortunately, the novelty of the technology\nmakes the task of catching and prosecuting offenders particularly challenging.\nThus, it is necessary to implement automated detection tools related to\npolicies to address the growing criminality in the cryptocurrency realm.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098442
  },
  {
    "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with\n  Knowledge Guidance",
    "abstract": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.",
    "text": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with\n  Knowledge Guidance Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0985851
  },
  {
    "title": "Forecasting the spatiotemporal evolution of fluid-induced\n  microearthquakes with deep learning",
    "abstract": "Microearthquakes (MEQs) generated by subsurface fluid injection record the\nevolving stress state and permeability of reservoirs. Forecasting their full\nspatiotemporal evolution is therefore critical for applications such as\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\ngeo-engineering applications. We present a transformer-based deep learning\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\n15-second forecast horizon across all targets, and supplies uncertainty\nestimates through a learned standard deviation term. These accurate,\nuncertainty-quantified forecasts enable real-time inference of fracture\npropagation and permeability evolution, demonstrating the strong potential of\ndeep-learning approaches to improve seismic-risk assessment and guide\nmitigation strategies in future fluid-injection operations.",
    "text": "Forecasting the spatiotemporal evolution of fluid-induced\n  microearthquakes with deep learning Microearthquakes (MEQs) generated by subsurface fluid injection record the\nevolving stress state and permeability of reservoirs. Forecasting their full\nspatiotemporal evolution is therefore critical for applications such as\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\ngeo-engineering applications. We present a transformer-based deep learning\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\n15-second forecast horizon across all targets, and supplies uncertainty\nestimates through a learned standard deviation term. These accurate,\nuncertainty-quantified forecasts enable real-time inference of fracture\npropagation and permeability evolution, demonstrating the strong potential of\ndeep-learning approaches to improve seismic-risk assessment and guide\nmitigation strategies in future fluid-injection operations.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0986161
  },
  {
    "title": "Foundation Artificial Intelligence Models for Health Recognition Using\n  Face Photographs (FAHR-Face)",
    "abstract": "Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.",
    "text": "Foundation Artificial Intelligence Models for Health Recognition Using\n  Face Photographs (FAHR-Face) Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0986292
  },
  {
    "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved\n  Vision-Language Reasoning",
    "abstract": "Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.",
    "text": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved\n  Vision-Language Reasoning Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0986419
  },
  {
    "title": "A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models",
    "abstract": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.",
    "text": "A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098652
  },
  {
    "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
    "abstract": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.",
    "text": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.0986629
  },
  {
    "title": "Optimizing Length Compression in Large Reasoning Models",
    "abstract": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
    "text": "Optimizing Length Compression in Large Reasoning Models Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098675
  },
  {
    "title": "Exploring Speaker Diarization with Mixture of Experts",
    "abstract": "In this paper, we propose a novel neural speaker diarization system using\nmemory-aware multi-speaker embedding with sequence-to-sequence architecture\n(NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with\na sequence-to-sequence architecture. The system leverages a memory module to\nenhance speaker embeddings and employs a Seq2Seq framework to efficiently map\nacoustic features to speaker labels. Additionally, we explore the application\nof mixture of experts in speaker diarization, and introduce a Shared and Soft\nMixture of Experts (SS-MoE) module to further mitigate model bias and enhance\nperformance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE.\nExperiments on multiple complex acoustic datasets, including CHiME-6, DiPCo,\nMixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in\nrobustness and generalization. The proposed methods achieve state-of-the-art\nresults, showcasing their effectiveness in challenging real-world scenarios.",
    "text": "Exploring Speaker Diarization with Mixture of Experts In this paper, we propose a novel neural speaker diarization system using\nmemory-aware multi-speaker embedding with sequence-to-sequence architecture\n(NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with\na sequence-to-sequence architecture. The system leverages a memory module to\nenhance speaker embeddings and employs a Seq2Seq framework to efficiently map\nacoustic features to speaker labels. Additionally, we explore the application\nof mixture of experts in speaker diarization, and introduce a Shared and Soft\nMixture of Experts (SS-MoE) module to further mitigate model bias and enhance\nperformance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE.\nExperiments on multiple complex acoustic datasets, including CHiME-6, DiPCo,\nMixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in\nrobustness and generalization. The proposed methods achieve state-of-the-art\nresults, showcasing their effectiveness in challenging real-world scenarios.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098685
  },
  {
    "title": "Preparing for the Intelligence Explosion",
    "abstract": "AI that can accelerate research could drive a century of technological\nprogress over just a few years. During such a period, new technological or\npolitical developments will raise consequential and hard-to-reverse decisions,\nin rapid succession. We call these developments grand challenges. These\nchallenges include new weapons of mass destruction, AI-enabled autocracies,\nraces to grab offworld resources, and digital beings worthy of moral\nconsideration, as well as opportunities to dramatically improve quality of life\nand collective decision-making. We argue that these challenges cannot always be\ndelegated to future AI systems, and suggest things we can do today to\nmeaningfully improve our prospects. AGI preparedness is therefore not just\nabout ensuring that advanced AI systems are aligned: we should be preparing,\nnow, for the disorienting range of developments an intelligence explosion would\nbring.",
    "text": "Preparing for the Intelligence Explosion AI that can accelerate research could drive a century of technological\nprogress over just a few years. During such a period, new technological or\npolitical developments will raise consequential and hard-to-reverse decisions,\nin rapid succession. We call these developments grand challenges. These\nchallenges include new weapons of mass destruction, AI-enabled autocracies,\nraces to grab offworld resources, and digital beings worthy of moral\nconsideration, as well as opportunities to dramatically improve quality of life\nand collective decision-making. We argue that these challenges cannot always be\ndelegated to future AI systems, and suggest things we can do today to\nmeaningfully improve our prospects. AGI preparedness is therefore not just\nabout ensuring that advanced AI systems are aligned: we should be preparing,\nnow, for the disorienting range of developments an intelligence explosion would\nbring.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098696
  },
  {
    "title": "Identifiability by common backdoor in summary causal graphs of time\n  series",
    "abstract": "The identifiability problem for interventions aims at assessing whether the\ntotal effect of some given interventions can be written with a do-free formula,\nand thus be computed from observational data only. We study this problem,\nconsidering multiple interventions and multiple effects, in the context of time\nseries when only abstractions of the true causal graph in the form of summary\ncausal graphs are available. We focus in this study on identifiability by a\ncommon backdoor set, and establish, for time series with and without\nconsistency throughout time, conditions under which such a set exists. We also\nprovide algorithms of limited complexity to decide whether the problem is\nidentifiable or not.",
    "text": "Identifiability by common backdoor in summary causal graphs of time\n  series The identifiability problem for interventions aims at assessing whether the\ntotal effect of some given interventions can be written with a do-free formula,\nand thus be computed from observational data only. We study this problem,\nconsidering multiple interventions and multiple effects, in the context of time\nseries when only abstractions of the true causal graph in the form of summary\ncausal graphs are available. We focus in this study on identifiability by a\ncommon backdoor set, and establish, for time series with and without\nconsistency throughout time, conditions under which such a set exists. We also\nprovide algorithms of limited complexity to decide whether the problem is\nidentifiable or not.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098707
  },
  {
    "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
    "abstract": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
    "text": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098718
  },
  {
    "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP\n  Boxes",
    "abstract": "While knowledge distillation has become a mature field for compressing large\nlanguage models (LLMs) into smaller ones by aligning their outputs or internal\nrepresentations, the distillation of LLM-based agents, which involve planning,\nmemory, and tool use, remains relatively underexplored. Existing agent\ndistillation methods typically replay full teacher trajectories or imitate\nstep-by-step teacher tool usage, but they often struggle to train student\nagents to dynamically plan and act in novel environments. We propose\nAgentDistill, a novel, training-free agent distillation framework that enables\nefficient and scalable knowledge transfer via direct reuse of\nModel-Context-Protocols (MCPs), which are structured and reusable task-solving\nmodules autonomously generated by teacher agents. The reuse of these distilled\nMCPs enables student agents to generalize their capabilities across domains and\nsolve new problems with minimal supervision or human intervention. Experiments\non biomedical and mathematical benchmarks demonstrate that our distilled\nstudent agents, built on small language models, can achieve performance\ncomparable to advanced systems using large LLMs such as OctoTools (GPT-4o),\nhighlighting the effectiveness of our framework in building scalable and\ncost-efficient intelligent agents.",
    "text": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP\n  Boxes While knowledge distillation has become a mature field for compressing large\nlanguage models (LLMs) into smaller ones by aligning their outputs or internal\nrepresentations, the distillation of LLM-based agents, which involve planning,\nmemory, and tool use, remains relatively underexplored. Existing agent\ndistillation methods typically replay full teacher trajectories or imitate\nstep-by-step teacher tool usage, but they often struggle to train student\nagents to dynamically plan and act in novel environments. We propose\nAgentDistill, a novel, training-free agent distillation framework that enables\nefficient and scalable knowledge transfer via direct reuse of\nModel-Context-Protocols (MCPs), which are structured and reusable task-solving\nmodules autonomously generated by teacher agents. The reuse of these distilled\nMCPs enables student agents to generalize their capabilities across domains and\nsolve new problems with minimal supervision or human intervention. Experiments\non biomedical and mathematical benchmarks demonstrate that our distilled\nstudent agents, built on small language models, can achieve performance\ncomparable to advanced systems using large LLMs such as OctoTools (GPT-4o),\nhighlighting the effectiveness of our framework in building scalable and\ncost-efficient intelligent agents.",
    "url": "",
    "category": "cs.AI",
    "source": "arxiv",
    "timestamp": 1750404540.098731
  },
  {
    "title": "HARMONY: A Scalable Distributed Vector Database for High-Throughput\n  Approximate Nearest Neighbor Search",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) is essential for various\ndata-intensive applications, including recommendation systems, image retrieval,\nand machine learning. Scaling ANNS to handle billions of high-dimensional\nvectors on a single machine presents significant challenges in memory capacity\nand processing efficiency. To address these challenges, distributed vector\ndatabases leverage multiple nodes for the parallel storage and processing of\nvectors. However, existing solutions often suffer from load imbalance and high\ncommunication overhead, primarily due to traditional partition strategies that\nfail to effectively distribute the workload. In this paper, we introduce\nHarmony, a distributed ANNS system that employs a novel multi-granularity\npartition strategy, combining dimension-based and vector-based partition. This\nstrategy ensures a balanced distribution of computational load across all nodes\nwhile effectively minimizing communication costs. Furthermore, Harmony\nincorporates an early-stop pruning mechanism that leverages the monotonicity of\ndistance computations in dimension-based partition, resulting in significant\nreductions in both computational and communication overhead. We conducted\nextensive experiments on diverse real-world datasets, demonstrating that\nHarmony outperforms leading distributed vector databases, achieving 4.63 times\nthroughput on average in four nodes and 58% performance improvement over\ntraditional distribution for skewed workloads.",
    "text": "HARMONY: A Scalable Distributed Vector Database for High-Throughput\n  Approximate Nearest Neighbor Search Approximate Nearest Neighbor Search (ANNS) is essential for various\ndata-intensive applications, including recommendation systems, image retrieval,\nand machine learning. Scaling ANNS to handle billions of high-dimensional\nvectors on a single machine presents significant challenges in memory capacity\nand processing efficiency. To address these challenges, distributed vector\ndatabases leverage multiple nodes for the parallel storage and processing of\nvectors. However, existing solutions often suffer from load imbalance and high\ncommunication overhead, primarily due to traditional partition strategies that\nfail to effectively distribute the workload. In this paper, we introduce\nHarmony, a distributed ANNS system that employs a novel multi-granularity\npartition strategy, combining dimension-based and vector-based partition. This\nstrategy ensures a balanced distribution of computational load across all nodes\nwhile effectively minimizing communication costs. Furthermore, Harmony\nincorporates an early-stop pruning mechanism that leverages the monotonicity of\ndistance computations in dimension-based partition, resulting in significant\nreductions in both computational and communication overhead. We conducted\nextensive experiments on diverse real-world datasets, demonstrating that\nHarmony outperforms leading distributed vector databases, achieving 4.63 times\nthroughput on average in four nodes and 58% performance improvement over\ntraditional distribution for skewed workloads.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1525278
  },
  {
    "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
    "abstract": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
    "text": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version) We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152543
  },
  {
    "title": "Sketched Sum-Product Networks for Joins",
    "abstract": "Sketches have shown high accuracy in multi-way join cardinality estimation, a\ncritical problem in cost-based query optimization. Accurately estimating the\ncardinality of a join operation -- analogous to its computational cost --\nallows the optimization of query execution costs in relational database\nsystems. However, although sketches have shown high efficacy in query\noptimization, they are typically constructed specifically for predefined\nselections in queries that are assumed to be given a priori, hindering their\napplicability to new queries. As a more general solution, we propose for\nSum-Product Networks to dynamically approximate sketches on-the-fly.\nSum-Product Networks can decompose and model multivariate distributions, such\nas relations, as linear combinations of multiple univariate distributions. By\nrepresenting these univariate distributions as sketches, Sum-Product Networks\ncan combine them element-wise to efficiently approximate the sketch of any\nquery selection. These approximate sketches can then be applied to join\ncardinality estimation. In particular, we implement the Fast-AGMS and Bound\nSketch methods, which have successfully been used in prior work, despite their\ncostly construction. By accurately approximating them instead, our work\nprovides a practical alternative to apply these sketches to query optimization.",
    "text": "Sketched Sum-Product Networks for Joins Sketches have shown high accuracy in multi-way join cardinality estimation, a\ncritical problem in cost-based query optimization. Accurately estimating the\ncardinality of a join operation -- analogous to its computational cost --\nallows the optimization of query execution costs in relational database\nsystems. However, although sketches have shown high efficacy in query\noptimization, they are typically constructed specifically for predefined\nselections in queries that are assumed to be given a priori, hindering their\napplicability to new queries. As a more general solution, we propose for\nSum-Product Networks to dynamically approximate sketches on-the-fly.\nSum-Product Networks can decompose and model multivariate distributions, such\nas relations, as linear combinations of multiple univariate distributions. By\nrepresenting these univariate distributions as sketches, Sum-Product Networks\ncan combine them element-wise to efficiently approximate the sketch of any\nquery selection. These approximate sketches can then be applied to join\ncardinality estimation. In particular, we implement the Fast-AGMS and Bound\nSketch methods, which have successfully been used in prior work, despite their\ncostly construction. By accurately approximating them instead, our work\nprovides a practical alternative to apply these sketches to query optimization.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152548
  },
  {
    "title": "Parachute: Single-Pass Bi-Directional Information Passing",
    "abstract": "Sideways information passing is a well-known technique for mitigating the\nimpact of large build sides in a database query plan. As currently implemented\nin production systems, sideways information passing enables only a\nuni-directional information flow, as opposed to instance-optimal algorithms,\nsuch as Yannakakis'. On the other hand, the latter require an additional pass\nover the input, which hinders adoption in production systems.\n  In this paper, we make a step towards enabling single-pass bi-directional\ninformation passing during query execution. We achieve this by statically\nanalyzing between which tables the information flow is blocked and by\nleveraging precomputed join-induced fingerprint columns on FK-tables. On the\nJOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time\nwithout and with semi-join filtering by 1.54x and 1.24x, respectively, when\nallowed to use 15% extra space.",
    "text": "Parachute: Single-Pass Bi-Directional Information Passing Sideways information passing is a well-known technique for mitigating the\nimpact of large build sides in a database query plan. As currently implemented\nin production systems, sideways information passing enables only a\nuni-directional information flow, as opposed to instance-optimal algorithms,\nsuch as Yannakakis'. On the other hand, the latter require an additional pass\nover the input, which hinders adoption in production systems.\n  In this paper, we make a step towards enabling single-pass bi-directional\ninformation passing during query execution. We achieve this by statically\nanalyzing between which tables the information flow is blocked and by\nleveraging precomputed join-induced fingerprint columns on FK-tables. On the\nJOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time\nwithout and with semi-join filtering by 1.54x and 1.24x, respectively, when\nallowed to use 15% extra space.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152554
  },
  {
    "title": "EnhanceGraph: A Continuously Enhanced Graph-based Index for\n  High-dimensional Approximate Nearest Neighbor Search",
    "abstract": "Recently, Approximate Nearest Neighbor Search in high-dimensional vector\nspaces has garnered considerable attention due to the rapid advancement of deep\nlearning techniques. We observed that a substantial amount of search and\nconstruction logs are generated throughout the lifespan of a graph-based index.\nHowever, these two types of valuable logs are not fully exploited due to the\nstatic nature of existing indexes. We present the EnhanceGraph framework, which\nintegrates two types of logs into a novel structure called a conjugate graph.\nThe conjugate graph is then used to improve search quality. Through theoretical\nanalyses and observations of the limitations of graph-based indexes, we propose\nseveral optimization methods. For the search logs, the conjugate graph stores\nthe edges from local optima to global optima to enhance routing to the nearest\nneighbor. For the construction logs, the conjugate graph stores the pruned\nedges from the proximity graph to enhance retrieving of k nearest neighbors.\nOur experimental results on several public and real-world industrial datasets\nshow that EnhanceGraph significantly improves search accuracy with the greatest\nimprovement on recall from 41.74% to 93.42%, but does not sacrifices search\nefficiency. In addition, our EnhanceGraph algorithm has been integrated into\nAnt Group's open-source vector library, VSAG.",
    "text": "EnhanceGraph: A Continuously Enhanced Graph-based Index for\n  High-dimensional Approximate Nearest Neighbor Search Recently, Approximate Nearest Neighbor Search in high-dimensional vector\nspaces has garnered considerable attention due to the rapid advancement of deep\nlearning techniques. We observed that a substantial amount of search and\nconstruction logs are generated throughout the lifespan of a graph-based index.\nHowever, these two types of valuable logs are not fully exploited due to the\nstatic nature of existing indexes. We present the EnhanceGraph framework, which\nintegrates two types of logs into a novel structure called a conjugate graph.\nThe conjugate graph is then used to improve search quality. Through theoretical\nanalyses and observations of the limitations of graph-based indexes, we propose\nseveral optimization methods. For the search logs, the conjugate graph stores\nthe edges from local optima to global optima to enhance routing to the nearest\nneighbor. For the construction logs, the conjugate graph stores the pruned\nedges from the proximity graph to enhance retrieving of k nearest neighbors.\nOur experimental results on several public and real-world industrial datasets\nshow that EnhanceGraph significantly improves search accuracy with the greatest\nimprovement on recall from 41.74% to 93.42%, but does not sacrifices search\nefficiency. In addition, our EnhanceGraph algorithm has been integrated into\nAnt Group's open-source vector library, VSAG.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152558
  },
  {
    "title": "Humans, Machine Learning, and Language Models in Union: A Cognitive\n  Study on Table Unionability",
    "abstract": "Data discovery and table unionability in particular became key tasks in\nmodern Data Science. However, the human perspective for these tasks is still\nunder-explored. Thus, this research investigates the human behavior in\ndetermining table unionability within data discovery. We have designed an\nexperimental survey and conducted a comprehensive analysis, in which we assess\nhuman decision-making for table unionability. We use the observations from the\nanalysis to develop a machine learning framework to boost the (raw) performance\nof humans. Furthermore, we perform a preliminary study on how LLM performance\nis compared to humans indicating that it is typically better to consider a\ncombination of both. We believe that this work lays the foundations for\ndeveloping future Human-in-the-Loop systems for efficient data discovery.",
    "text": "Humans, Machine Learning, and Language Models in Union: A Cognitive\n  Study on Table Unionability Data discovery and table unionability in particular became key tasks in\nmodern Data Science. However, the human perspective for these tasks is still\nunder-explored. Thus, this research investigates the human behavior in\ndetermining table unionability within data discovery. We have designed an\nexperimental survey and conducted a comprehensive analysis, in which we assess\nhuman decision-making for table unionability. We use the observations from the\nanalysis to develop a machine learning framework to boost the (raw) performance\nof humans. Furthermore, we perform a preliminary study on how LLM performance\nis compared to humans indicating that it is typically better to consider a\ncombination of both. We believe that this work lays the foundations for\ndeveloping future Human-in-the-Loop systems for efficient data discovery.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152562
  },
  {
    "title": "Towards Visualizing Electronic Medical Records via Natural Language\n  Queries",
    "abstract": "Electronic medical records (EMRs) contain essential data for patient care and\nclinical research. With the diversity of structured and unstructured data in\nEHR, data visualization is an invaluable tool for managing and explaining these\ncomplexities. However, the scarcity of relevant medical visualization data and\nthe high cost of manual annotation required to develop such datasets pose\nsignificant challenges to advancing medical visualization techniques. To\naddress this issue, we propose an innovative approach using large language\nmodels (LLMs) for generating visualization data without labor-intensive manual\nannotation. We introduce a new pipeline for building text-to-visualization\nbenchmarks suitable for EMRs, enabling users to visualize EMR statistics\nthrough natural language queries (NLQs). The dataset presented in this paper\nprimarily consists of paired text medical records, NLQs, and corresponding\nvisualizations, forming the first large-scale text-to-visual dataset for\nelectronic medical record information called MedicalVis with 35,374 examples.\nAdditionally, we introduce an LLM-based approach called MedCodeT5, showcasing\nits viability in generating EMR visualizations from NLQs, outperforming various\nstrong text-to-visualization baselines. Our work facilitates standardized\nevaluation of EMR visualization methods while providing researchers with tools\nto advance this influential field of application. In a nutshell, this study and\ndataset have the potential to promote advancements in eliciting medical\ninsights through visualization.",
    "text": "Towards Visualizing Electronic Medical Records via Natural Language\n  Queries Electronic medical records (EMRs) contain essential data for patient care and\nclinical research. With the diversity of structured and unstructured data in\nEHR, data visualization is an invaluable tool for managing and explaining these\ncomplexities. However, the scarcity of relevant medical visualization data and\nthe high cost of manual annotation required to develop such datasets pose\nsignificant challenges to advancing medical visualization techniques. To\naddress this issue, we propose an innovative approach using large language\nmodels (LLMs) for generating visualization data without labor-intensive manual\nannotation. We introduce a new pipeline for building text-to-visualization\nbenchmarks suitable for EMRs, enabling users to visualize EMR statistics\nthrough natural language queries (NLQs). The dataset presented in this paper\nprimarily consists of paired text medical records, NLQs, and corresponding\nvisualizations, forming the first large-scale text-to-visual dataset for\nelectronic medical record information called MedicalVis with 35,374 examples.\nAdditionally, we introduce an LLM-based approach called MedCodeT5, showcasing\nits viability in generating EMR visualizations from NLQs, outperforming various\nstrong text-to-visualization baselines. Our work facilitates standardized\nevaluation of EMR visualization methods while providing researchers with tools\nto advance this influential field of application. In a nutshell, this study and\ndataset have the potential to promote advancements in eliciting medical\ninsights through visualization.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1525671
  },
  {
    "title": "Redbench: A Benchmark Reflecting Real Workloads",
    "abstract": "Instance-optimized components have made their way into production systems. To\nsome extent, this adoption is due to the characteristics of customer workloads,\nwhich can be individually leveraged during the model training phase. However,\nthere is a gap between research and industry that impedes the development of\nrealistic learned components: the lack of suitable workloads. Existing ones,\nsuch as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail\nto exhibit real workload patterns, particularly distribution shifts.\n  In this paper, we introduce Redbench, a collection of 30 workloads that\nreflect query patterns observed in the real world. The workloads were obtained\nby sampling queries from support benchmarks and aligning them with workload\ncharacteristics observed in Redset.",
    "text": "Redbench: A Benchmark Reflecting Real Workloads Instance-optimized components have made their way into production systems. To\nsome extent, this adoption is due to the characteristics of customer workloads,\nwhich can be individually leveraged during the model training phase. However,\nthere is a gap between research and industry that impedes the development of\nrealistic learned components: the lack of suitable workloads. Existing ones,\nsuch as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail\nto exhibit real workload patterns, particularly distribution shifts.\n  In this paper, we introduce Redbench, a collection of 30 workloads that\nreflect query patterns observed in the real world. The workloads were obtained\nby sampling queries from support benchmarks and aligning them with workload\ncharacteristics observed in Redset.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152571
  },
  {
    "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and\n  Ethics",
    "abstract": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), such as enhancing their reasoning skills, adaptability\nto various tasks, increased computational efficiency, and ability to make\nethical decisions. The techniques that have been most effective in bridging the\ngap between human and machine communications include the Chain-of-Thought\nprompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.\nThe improvements in multimodal learning and few-shot or zero-shot techniques\nhave further empowered LLMs to handle complex jobs with minor input. They also\nmanage to do more with less by applying scaling and optimization tricks for\ncomputing power conservation. This survey also offers a broader perspective on\nrecent advancements in LLMs going beyond isolated aspects such as model\narchitecture or ethical concerns. It categorizes emerging methods that enhance\nLLM reasoning, efficiency, and ethical alignment. It also identifies\nunderexplored areas such as interpretability, cross-modal integration and\nsustainability. With recent progress, challenges like huge computational costs,\nbiases, and ethical risks remain constant. Addressing these requires bias\nmitigation, transparent decision-making, and clear ethical guidelines. Future\nresearch will focus on enhancing models ability to handle multiple input,\nthereby making them more intelligent, safe, and reliable.",
    "text": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and\n  Ethics This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), such as enhancing their reasoning skills, adaptability\nto various tasks, increased computational efficiency, and ability to make\nethical decisions. The techniques that have been most effective in bridging the\ngap between human and machine communications include the Chain-of-Thought\nprompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.\nThe improvements in multimodal learning and few-shot or zero-shot techniques\nhave further empowered LLMs to handle complex jobs with minor input. They also\nmanage to do more with less by applying scaling and optimization tricks for\ncomputing power conservation. This survey also offers a broader perspective on\nrecent advancements in LLMs going beyond isolated aspects such as model\narchitecture or ethical concerns. It categorizes emerging methods that enhance\nLLM reasoning, efficiency, and ethical alignment. It also identifies\nunderexplored areas such as interpretability, cross-modal integration and\nsustainability. With recent progress, challenges like huge computational costs,\nbiases, and ethical risks remain constant. Addressing these requires bias\nmitigation, transparent decision-making, and clear ethical guidelines. Future\nresearch will focus on enhancing models ability to handle multiple input,\nthereby making them more intelligent, safe, and reliable.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152575
  },
  {
    "title": "Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task",
    "abstract": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.",
    "text": "Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152579
  },
  {
    "title": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection",
    "abstract": "Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations.",
    "text": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152586
  },
  {
    "title": "OCPQ: Object-Centric Process Querying & Constraints",
    "abstract": "Process querying is used to extract information and insights from process\nexecution data. Similarly, process constraints can be checked against input\ndata, yielding information on which process instances violate them.\nTraditionally, such process mining techniques use case-centric event data as\ninput. However, with the uptake of Object-Centric Process Mining (OCPM),\nexisting querying and constraint checking techniques are no longer applicable.\nObject-Centric Event Data (OCED) removes the requirement to pick a single case\nnotion (i.e., requiring that events belong to exactly one case) and can thus\nrepresent many real-life processes much more accurately. In this paper, we\npresent a novel highly-expressive approach for object-centric process querying,\ncalled OCPQ. It supports a wide variety of applications, including OCED-based\nconstraint checking and filtering. The visual representation of nested queries\nin OCPQ allows users to intuitively read and create queries and constraints. We\nimplemented our approach using (1) a high-performance execution engine backend\nand (2) an easy-to-use editor frontend. Additionally, we evaluated our approach\non a real-life dataset, showing the lack in expressiveness of prior work and\nruntime performance significantly better than the general querying solutions\nSQLite and Neo4j, as well as comparable to the performance-focused DuckDB.",
    "text": "OCPQ: Object-Centric Process Querying & Constraints Process querying is used to extract information and insights from process\nexecution data. Similarly, process constraints can be checked against input\ndata, yielding information on which process instances violate them.\nTraditionally, such process mining techniques use case-centric event data as\ninput. However, with the uptake of Object-Centric Process Mining (OCPM),\nexisting querying and constraint checking techniques are no longer applicable.\nObject-Centric Event Data (OCED) removes the requirement to pick a single case\nnotion (i.e., requiring that events belong to exactly one case) and can thus\nrepresent many real-life processes much more accurately. In this paper, we\npresent a novel highly-expressive approach for object-centric process querying,\ncalled OCPQ. It supports a wide variety of applications, including OCED-based\nconstraint checking and filtering. The visual representation of nested queries\nin OCPQ allows users to intuitively read and create queries and constraints. We\nimplemented our approach using (1) a high-performance execution engine backend\nand (2) an easy-to-use editor frontend. Additionally, we evaluated our approach\non a real-life dataset, showing the lack in expressiveness of prior work and\nruntime performance significantly better than the general querying solutions\nSQLite and Neo4j, as well as comparable to the performance-focused DuckDB.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15259
  },
  {
    "title": "Jelly: a fast and convenient RDF serialization format",
    "abstract": "Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are\nwidely used for communication and storage in knowledge graph and Semantic Web\napplications. However, they suffer from limitations in performance, compression\nratio, and lack of native support for RDF streams. To address these\nshortcomings, we introduce Jelly, a fast and convenient binary serialization\nformat for RDF data that supports both batch and streaming use cases. Jelly is\ndesigned to maximize serialization throughput, reduce file size with\nlightweight streaming compression, and minimize compute resource usage. Built\non Protocol Buffers, Jelly is easy to integrate with modern programming\nlanguages and RDF libraries. To maximize reusability, Jelly has an open\nprotocol specification, open-source implementations in Java and Python\nintegrated with popular RDF libraries, and a versatile command-line tool. To\nillustrate its usefulness, we outline concrete use cases where Jelly can\nprovide tangible benefits. By combining practical usability with\nstate-of-the-art efficiency, Jelly is an important contribution to the Semantic\nWeb tool stack.",
    "text": "Jelly: a fast and convenient RDF serialization format Existing RDF serialization formats such as Turtle, N-Triples, and JSON-LD are\nwidely used for communication and storage in knowledge graph and Semantic Web\napplications. However, they suffer from limitations in performance, compression\nratio, and lack of native support for RDF streams. To address these\nshortcomings, we introduce Jelly, a fast and convenient binary serialization\nformat for RDF data that supports both batch and streaming use cases. Jelly is\ndesigned to maximize serialization throughput, reduce file size with\nlightweight streaming compression, and minimize compute resource usage. Built\non Protocol Buffers, Jelly is easy to integrate with modern programming\nlanguages and RDF libraries. To maximize reusability, Jelly has an open\nprotocol specification, open-source implementations in Java and Python\nintegrated with popular RDF libraries, and a versatile command-line tool. To\nillustrate its usefulness, we outline concrete use cases where Jelly can\nprovide tangible benefits. By combining practical usability with\nstate-of-the-art efficiency, Jelly is an important contribution to the Semantic\nWeb tool stack.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1525939
  },
  {
    "title": "S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable\n  with DBOS",
    "abstract": "To meet the needs of a large pharmaceutical organization, we set out to\ncreate S3Mirror - an application for transferring large genomic sequencing\ndatasets between S3 buckets quickly, reliably, and observably. We used the DBOS\nTransact durable execution framework to achieve these goals and benchmarked the\nperformance and cost of the application. S3Mirror is an open source DBOS Python\napplication that can run in a variety of environments, including DBOS Cloud\nPro, where it runs as much as 40x faster than AWS DataSync at a fraction of the\ncost. Moreover, S3Mirror is resilient to failures and allows for real-time\nfilewise observability of ongoing and past transfers.",
    "text": "S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable\n  with DBOS To meet the needs of a large pharmaceutical organization, we set out to\ncreate S3Mirror - an application for transferring large genomic sequencing\ndatasets between S3 buckets quickly, reliably, and observably. We used the DBOS\nTransact durable execution framework to achieve these goals and benchmarked the\nperformance and cost of the application. S3Mirror is an open source DBOS Python\napplication that can run in a variety of environments, including DBOS Cloud\nPro, where it runs as much as 40x faster than AWS DataSync at a fraction of the\ncost. Moreover, S3Mirror is resilient to failures and allows for real-time\nfilewise observability of ongoing and past transfers.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152598
  },
  {
    "title": "A Hybrid Heuristic Framework for Resource-Efficient Querying of\n  Scientific Experiments Data",
    "abstract": "Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.",
    "text": "A Hybrid Heuristic Framework for Resource-Efficient Querying of\n  Scientific Experiments Data Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152608
  },
  {
    "title": "A Unifying Algorithm for Hierarchical Queries",
    "abstract": "The class of hierarchical queries is known to define the boundary of the\ndichotomy between tractability and intractability for the following two\nextensively studied problems about self-join free Boolean conjunctive queries\n(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic\ndatabase; (ii) computing the Shapley value of a fact in a database on which a\nSJF-BCQ evaluates to true. Here, we establish that hierarchical queries define\nalso the boundary of the dichotomy between tractability and intractability for\na different natural algorithmic problem, which we call the \"bag-set\nmaximization\" problem. The bag-set maximization problem associated with a\nSJF-BCQ $Q$ asks: given a database $\\cal D$, find the biggest value that $Q$\ntakes under bag semantics on a database $\\cal D'$ obtained from $\\cal D$ by\nadding at most $\\theta$ facts from another given database $\\cal D^r$.\n  For non-hierarchical queries, we show that the bag-set maximization problem\nis an NP-complete optimization problem. More significantly, for hierarchical\nqueries, we show that all three aforementioned problems (probabilistic query\nevaluation, Shapley value computation, and bag-set maximization) admit a single\nunifying polynomial-time algorithm that operates on an abstract algebraic\nstructure, called a \"2-monoid\". Each of the three problems requires a different\ninstantiation of the 2-monoid tailored for the problem at hand.",
    "text": "A Unifying Algorithm for Hierarchical Queries The class of hierarchical queries is known to define the boundary of the\ndichotomy between tractability and intractability for the following two\nextensively studied problems about self-join free Boolean conjunctive queries\n(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic\ndatabase; (ii) computing the Shapley value of a fact in a database on which a\nSJF-BCQ evaluates to true. Here, we establish that hierarchical queries define\nalso the boundary of the dichotomy between tractability and intractability for\na different natural algorithmic problem, which we call the \"bag-set\nmaximization\" problem. The bag-set maximization problem associated with a\nSJF-BCQ $Q$ asks: given a database $\\cal D$, find the biggest value that $Q$\ntakes under bag semantics on a database $\\cal D'$ obtained from $\\cal D$ by\nadding at most $\\theta$ facts from another given database $\\cal D^r$.\n  For non-hierarchical queries, we show that the bag-set maximization problem\nis an NP-complete optimization problem. More significantly, for hierarchical\nqueries, we show that all three aforementioned problems (probabilistic query\nevaluation, Shapley value computation, and bag-set maximization) admit a single\nunifying polynomial-time algorithm that operates on an abstract algebraic\nstructure, called a \"2-monoid\". Each of the three problems requires a different\ninstantiation of the 2-monoid tailored for the problem at hand.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1526122
  },
  {
    "title": "GPU Acceleration of SQL Analytics on Compressed Data",
    "abstract": "GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to\ntheir massive compute parallelism and High Bandwidth Memory (HBM) -- when\ndatasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU\nHBMs remain typically small when compared with lower-bandwidth CPU main memory.\nBesides brute-force scaling across many GPUs, current solutions to accelerate\nqueries on large datasets include leveraging data partitioning and loading\nsmaller data batches in GPU HBM, and hybrid execution with a connected device\n(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of\nlower main memory and host-to-device interconnect bandwidths, introduce\nadditional I/O overheads, or incur higher costs. This is a substantial problem\nwhen trying to scale adoption of GPUs on larger datasets. Data compression can\nalleviate this bottleneck, but to avoid paying for costly\ndecompression/decoding, an ideal solution must include computation primitives\nto operate directly on data in compressed form.\n  This is the focus of our paper: a set of new methods for running queries\ndirectly on light-weight compressed data using schemes such as Run-Length\nEncoding (RLE), index encoding, bit-width reductions, and dictionary encoding.\nOur novelty includes operating on multiple RLE columns without decompression,\nhandling heterogeneous column encodings, and leveraging PyTorch tensor\noperations for portability across devices. Experimental evaluations show\nspeedups of an order of magnitude compared to state-of-the-art commercial\nCPU-only analytics systems, for real-world queries on a production dataset that\nwould not fit into GPU memory uncompressed. This work paves the road for GPU\nadoption in a much broader set of use cases, and it is complementary to most\nother scale-out or fallback mechanisms.",
    "text": "GPU Acceleration of SQL Analytics on Compressed Data GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to\ntheir massive compute parallelism and High Bandwidth Memory (HBM) -- when\ndatasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU\nHBMs remain typically small when compared with lower-bandwidth CPU main memory.\nBesides brute-force scaling across many GPUs, current solutions to accelerate\nqueries on large datasets include leveraging data partitioning and loading\nsmaller data batches in GPU HBM, and hybrid execution with a connected device\n(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of\nlower main memory and host-to-device interconnect bandwidths, introduce\nadditional I/O overheads, or incur higher costs. This is a substantial problem\nwhen trying to scale adoption of GPUs on larger datasets. Data compression can\nalleviate this bottleneck, but to avoid paying for costly\ndecompression/decoding, an ideal solution must include computation primitives\nto operate directly on data in compressed form.\n  This is the focus of our paper: a set of new methods for running queries\ndirectly on light-weight compressed data using schemes such as Run-Length\nEncoding (RLE), index encoding, bit-width reductions, and dictionary encoding.\nOur novelty includes operating on multiple RLE columns without decompression,\nhandling heterogeneous column encodings, and leveraging PyTorch tensor\noperations for portability across devices. Experimental evaluations show\nspeedups of an order of magnitude compared to state-of-the-art commercial\nCPU-only analytics systems, for real-world queries on a production dataset that\nwould not fit into GPU memory uncompressed. This work paves the road for GPU\nadoption in a much broader set of use cases, and it is complementary to most\nother scale-out or fallback mechanisms.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152617
  },
  {
    "title": "Microservices and Real-Time Processing in Retail IT: A Review of\n  Open-Source Toolchains and Deployment Strategies",
    "abstract": "With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.",
    "text": "Microservices and Real-Time Processing in Retail IT: A Review of\n  Open-Source Toolchains and Deployment Strategies With the rapid pace of digital transformation, the retail industry is\nincreasingly depending on real-time, scalable, and resilient systems to manage\nfinancial transactions, analyze customer behavior, and streamline order\nprocessing. This literature review explores how modern event-driven and\nmicroservices-based architectures, particularly those leveraging Apache Kafka,\nSpring Boot, MongoDB, and Kubernetes are transforming retail and financial\nsystems. By systematically reviewing academic publications, technical white\npapers, and industry reports from recent years, this study synthesizes key\nthemes and implementation strategies. The analysis reveals that technologies\nlike Kafka and Spring Boot are instrumental in building low-latency,\nevent-driven applications that support real-time analytics and fraud detection,\nwhile MongoDB, when deployed on Kubernetes, ensures fault tolerance and high\navailability in inventory and transaction systems. Kubernetes itself plays a\ncrucial role in automating deployment and scaling of microservices. These\nfindings provide valuable insights for industry practitioners aiming to design\nscalable infrastructures, identify research opportunities in hybrid deployment\nmodels, and offer educators a foundation to integrate modern system\narchitectures into professional and technical communication training.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1526241
  },
  {
    "title": "Linking Data Citation to Repository Visibility: An Empirical Study",
    "abstract": "In today's data-driven research landscape, dataset visibility and\naccessibility play a crucial role in advancing scientific knowledge. At the\nsame time, data citation is essential for maintaining academic integrity,\nacknowledging contributions, validating research outcomes, and fostering\nscientific reproducibility. As a critical link, it connects scholarly\npublications with the datasets that drive scientific progress. This study\ninvestigates whether repository visibility influences data citation rates. We\nhypothesize that repositories with higher visibility, as measured by search\nengine metrics, are associated with increased dataset citations. Using OpenAlex\ndata and repository impact indicators (including the visibility index from\nSistrix, the h-index of repositories, and citation metrics such as mean and\nmedian citations), we analyze datasets in Social Sciences and Economics to\nexplore their relationship. Our findings suggest that datasets hosted on more\nvisible web domains tend to receive more citations, with a positive correlation\nobserved between web domain visibility and dataset citation counts,\nparticularly for datasets with at least one citation. However, when analyzing\ndomain-level citation metrics, such as the h-index, mean, and median citations,\nthe correlations are inconsistent and weaker. While higher visibility domains\ntend to host datasets with greater citation impact, the distribution of\ncitations across datasets varies significantly. These results suggest that\nwhile visibility plays a role in increasing citation counts, it is not the sole\nfactor influencing dataset citation impact. Other elements, such as dataset\nquality, research trends, and disciplinary norms, also contribute significantly\nto citation patterns.",
    "text": "Linking Data Citation to Repository Visibility: An Empirical Study In today's data-driven research landscape, dataset visibility and\naccessibility play a crucial role in advancing scientific knowledge. At the\nsame time, data citation is essential for maintaining academic integrity,\nacknowledging contributions, validating research outcomes, and fostering\nscientific reproducibility. As a critical link, it connects scholarly\npublications with the datasets that drive scientific progress. This study\ninvestigates whether repository visibility influences data citation rates. We\nhypothesize that repositories with higher visibility, as measured by search\nengine metrics, are associated with increased dataset citations. Using OpenAlex\ndata and repository impact indicators (including the visibility index from\nSistrix, the h-index of repositories, and citation metrics such as mean and\nmedian citations), we analyze datasets in Social Sciences and Economics to\nexplore their relationship. Our findings suggest that datasets hosted on more\nvisible web domains tend to receive more citations, with a positive correlation\nobserved between web domain visibility and dataset citation counts,\nparticularly for datasets with at least one citation. However, when analyzing\ndomain-level citation metrics, such as the h-index, mean, and median citations,\nthe correlations are inconsistent and weaker. While higher visibility domains\ntend to host datasets with greater citation impact, the distribution of\ncitations across datasets varies significantly. These results suggest that\nwhile visibility plays a role in increasing citation counts, it is not the sole\nfactor influencing dataset citation impact. Other elements, such as dataset\nquality, research trends, and disciplinary norms, also contribute significantly\nto citation patterns.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152628
  },
  {
    "title": "ArcNeural: A Multi-Modal Database for the Gen-AI Era",
    "abstract": "ArcNeural introduces a novel multimodal database tailored for the demands of\nGenerative AI and Large Language Models, enabling efficient management of\ndiverse data types such as graphs, vectors, and documents. Its storage-compute\nseparated architecture integrates graph technology, advanced vector indexing,\nand transaction processing to support real-time analytics and AI-driven\napplications. Key features include a unified storage layer, adaptive edge\ncollection in MemEngine, and seamless integration of transaction and analytical\nprocessing. Experimental evaluations demonstrate ArcNeural's superior\nperformance and scalability compared to state-of-the-art systems. This system\nbridges structured and unstructured data management, offering a versatile\nsolution for enterprise-grade AI applications.\n  ArcNeural's design addresses the challenges of multimodal data processing,\nproviding a robust framework for intelligent, data-driven solutions in the Gen\nAI era.",
    "text": "ArcNeural: A Multi-Modal Database for the Gen-AI Era ArcNeural introduces a novel multimodal database tailored for the demands of\nGenerative AI and Large Language Models, enabling efficient management of\ndiverse data types such as graphs, vectors, and documents. Its storage-compute\nseparated architecture integrates graph technology, advanced vector indexing,\nand transaction processing to support real-time analytics and AI-driven\napplications. Key features include a unified storage layer, adaptive edge\ncollection in MemEngine, and seamless integration of transaction and analytical\nprocessing. Experimental evaluations demonstrate ArcNeural's superior\nperformance and scalability compared to state-of-the-art systems. This system\nbridges structured and unstructured data management, offering a versatile\nsolution for enterprise-grade AI applications.\n  ArcNeural's design addresses the challenges of multimodal data processing,\nproviding a robust framework for intelligent, data-driven solutions in the Gen\nAI era.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152648
  },
  {
    "title": "LLM-Driven Data Generation and a Novel Soft Metric for Evaluating\n  Text-to-SQL in Aviation MRO",
    "abstract": "The application of Large Language Models (LLMs) to text-to-SQL tasks promises\nto democratize data access, particularly in critical industries like aviation\nMaintenance, Repair, and Operation (MRO). However, progress is hindered by two\nkey challenges: the rigidity of conventional evaluation metrics such as\nexecution accuracy, which offer coarse, binary feedback, and the scarcity of\ndomain-specific evaluation datasets. This paper addresses these gaps. To enable\nmore nuanced assessment, we introduce a novel F1-score-based 'soft' metric that\nquantifies the informational overlap between generated and ground-truth SQL\nresults. To address data scarcity, we propose an LLM-driven pipeline that\nsynthesizes realistic question-SQL pairs from database schemas. We demonstrate\nour contributions through an empirical evaluation on an authentic MRO database.\nOur experiments show that the proposed soft metric provides more insightful\nperformance analysis than strict accuracy, and our data generation technique is\neffective in creating a domain-specific benchmark. Together, these\ncontributions offer a robust framework for evaluating and advancing text-to-SQL\nsystems in specialized environments.",
    "text": "LLM-Driven Data Generation and a Novel Soft Metric for Evaluating\n  Text-to-SQL in Aviation MRO The application of Large Language Models (LLMs) to text-to-SQL tasks promises\nto democratize data access, particularly in critical industries like aviation\nMaintenance, Repair, and Operation (MRO). However, progress is hindered by two\nkey challenges: the rigidity of conventional evaluation metrics such as\nexecution accuracy, which offer coarse, binary feedback, and the scarcity of\ndomain-specific evaluation datasets. This paper addresses these gaps. To enable\nmore nuanced assessment, we introduce a novel F1-score-based 'soft' metric that\nquantifies the informational overlap between generated and ground-truth SQL\nresults. To address data scarcity, we propose an LLM-driven pipeline that\nsynthesizes realistic question-SQL pairs from database schemas. We demonstrate\nour contributions through an empirical evaluation on an authentic MRO database.\nOur experiments show that the proposed soft metric provides more insightful\nperformance analysis than strict accuracy, and our data generation technique is\neffective in creating a domain-specific benchmark. Together, these\ncontributions offer a robust framework for evaluating and advancing text-to-SQL\nsystems in specialized environments.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152651
  },
  {
    "title": "Terabyte-Scale Analytics in the Blink of an Eye",
    "abstract": "For the past two decades, the DB community has devoted substantial research\nto take advantage of cheap clusters of machines for distributed data analytics\n-- we believe that we are at the beginning of a paradigm shift. The scaling\nlaws and popularity of AI models lead to the deployment of incredibly powerful\nGPU clusters in commercial data centers. Compared to CPU-only solutions, these\nclusters deliver impressive improvements in per-node compute, memory bandwidth,\nand inter-node interconnect performance. In this paper, we study the problem of\nscaling analytical SQL queries on distributed clusters of GPUs, with the stated\ngoal of establishing an upper bound on the likely performance gains. To do so,\nwe build a prototype designed to maximize performance by leveraging ML/HPC best\npractices, such as group communication primitives for cross-device data\nmovements. This allows us to conduct thorough performance experimentation to\npoint our community towards a massive performance opportunity of at least\n60$\\times$. To make these gains more relatable, before you can blink twice, our\nsystem can run all 22 queries of TPC-H at a 1TB scale factor!",
    "text": "Terabyte-Scale Analytics in the Blink of an Eye For the past two decades, the DB community has devoted substantial research\nto take advantage of cheap clusters of machines for distributed data analytics\n-- we believe that we are at the beginning of a paradigm shift. The scaling\nlaws and popularity of AI models lead to the deployment of incredibly powerful\nGPU clusters in commercial data centers. Compared to CPU-only solutions, these\nclusters deliver impressive improvements in per-node compute, memory bandwidth,\nand inter-node interconnect performance. In this paper, we study the problem of\nscaling analytical SQL queries on distributed clusters of GPUs, with the stated\ngoal of establishing an upper bound on the likely performance gains. To do so,\nwe build a prototype designed to maximize performance by leveraging ML/HPC best\npractices, such as group communication primitives for cross-device data\nmovements. This allows us to conduct thorough performance experimentation to\npoint our community towards a massive performance opportunity of at least\n60$\\times$. To make these gains more relatable, before you can blink twice, our\nsystem can run all 22 queries of TPC-H at a 1TB scale factor!",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152659
  },
  {
    "title": "Not all those who drift are lost: Drift correction and calibration\n  scheduling for the IoT",
    "abstract": "Sensors provide a vital source of data that link digital systems with the\nphysical world. However, as sensors age, the relationship between what they\nmeasure and what they output changes. This is known as sensor drift and poses a\nsignificant challenge that, combined with limited opportunity for\nre-calibration, can severely limit data quality over time. Previous approaches\nto drift correction typically require large volumes of ground truth data and do\nnot consider measurement or prediction uncertainty. In this paper, we propose a\nprobabilistic sensor drift correction method that takes a fundamental approach\nto modelling the sensor response using Gaussian Process Regression. Tested\nusing dissolved oxygen sensors, our method delivers mean squared error (MSE)\nreductions of up to 90% and more than 20% on average. We also propose a novel\nuncertainty-driven calibration schedule optimisation approach that builds on\ntop of drift correction and further reduces MSE by up to 15.7%.",
    "text": "Not all those who drift are lost: Drift correction and calibration\n  scheduling for the IoT Sensors provide a vital source of data that link digital systems with the\nphysical world. However, as sensors age, the relationship between what they\nmeasure and what they output changes. This is known as sensor drift and poses a\nsignificant challenge that, combined with limited opportunity for\nre-calibration, can severely limit data quality over time. Previous approaches\nto drift correction typically require large volumes of ground truth data and do\nnot consider measurement or prediction uncertainty. In this paper, we propose a\nprobabilistic sensor drift correction method that takes a fundamental approach\nto modelling the sensor response using Gaussian Process Regression. Tested\nusing dissolved oxygen sensors, our method delivers mean squared error (MSE)\nreductions of up to 90% and more than 20% on average. We also propose a novel\nuncertainty-driven calibration schedule optimisation approach that builds on\ntop of drift correction and further reduces MSE by up to 15.7%.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152663
  },
  {
    "title": "Qymera: Simulating Quantum Circuits using RDBMS",
    "abstract": "Quantum circuit simulation is crucial for quantum computing such as\nvalidating quantum algorithms. We present Qymera, a system that repurposes\nrelational database management systems (RDBMSs) for simulation by translating\ncircuits into SQL queries, allowing quantum operations to run natively within\nan RDBMS. Qymera supports a wide range of quantum circuits, offering a\ngraphical circuit builder and code-based interfaces to input circuits. With a\nbenchmarking framework, Qymera facilitates comparison of RDBMS-based simulation\nagainst state-of-the-art simulation methods. Our demonstration showcases\nQymera's end-to-end SQL-based execution, seamless integration with classical\nworkflows, and its utility for development, benchmarking, and education in\nquantum computing and data management.",
    "text": "Qymera: Simulating Quantum Circuits using RDBMS Quantum circuit simulation is crucial for quantum computing such as\nvalidating quantum algorithms. We present Qymera, a system that repurposes\nrelational database management systems (RDBMSs) for simulation by translating\ncircuits into SQL queries, allowing quantum operations to run natively within\nan RDBMS. Qymera supports a wide range of quantum circuits, offering a\ngraphical circuit builder and code-based interfaces to input circuits. With a\nbenchmarking framework, Qymera facilitates comparison of RDBMS-based simulation\nagainst state-of-the-art simulation methods. Our demonstration showcases\nQymera's end-to-end SQL-based execution, seamless integration with classical\nworkflows, and its utility for development, benchmarking, and education in\nquantum computing and data management.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15267
  },
  {
    "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for\n  Semantically-Rich Recommender Systems",
    "abstract": "Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",
    "text": "Bridging RDF Knowledge Graphs with Graph Neural Networks for\n  Semantically-Rich Recommender Systems Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152673
  },
  {
    "title": "Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and\n  Design Choices",
    "abstract": "LSM-tree-based data stores are widely used in industry due to their\nexceptional performance. However, as data volumes grow, efficiently querying\nlarge-scale databases becomes increasingly challenging. To address this, recent\nstudies attempted to integrate learned indexes into LSM-trees to enhance lookup\nperformance, which has demonstrated promising improvements. Despite this, only\na limited range of learned index types has been considered, and the strengths\nand weaknesses of different learned indexes remain unclear, making them\ndifficult for practical use. To fill this gap, we provide a comprehensive and\nsystematic benchmark to pursue an in-depth understanding of learned indexes in\nLSM-tree systems. In this work, we summarize the workflow of 8 existing learned\nindexes and analyze the associated theoretical cost. We also identify several\nkey factors that significantly influence the performance of learned indexes and\nconclude them with a novel configuration space, including various index types,\nboundary positions, and granularity. Moreover, we implement different learned\nindex designs on a unified platform to evaluate across various configurations.\nSurprisingly, our experiments reveal several unexpected insights, such as the\nmarginal lookup enhancement when allocating a large memory budget to learned\nindexes and modest retraining overhead of learned indexes. Besides, we also\noffer practical guidelines to help developers intelligently select and tune\nlearned indexes for custom use cases.",
    "text": "Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and\n  Design Choices LSM-tree-based data stores are widely used in industry due to their\nexceptional performance. However, as data volumes grow, efficiently querying\nlarge-scale databases becomes increasingly challenging. To address this, recent\nstudies attempted to integrate learned indexes into LSM-trees to enhance lookup\nperformance, which has demonstrated promising improvements. Despite this, only\na limited range of learned index types has been considered, and the strengths\nand weaknesses of different learned indexes remain unclear, making them\ndifficult for practical use. To fill this gap, we provide a comprehensive and\nsystematic benchmark to pursue an in-depth understanding of learned indexes in\nLSM-tree systems. In this work, we summarize the workflow of 8 existing learned\nindexes and analyze the associated theoretical cost. We also identify several\nkey factors that significantly influence the performance of learned indexes and\nconclude them with a novel configuration space, including various index types,\nboundary positions, and granularity. Moreover, we implement different learned\nindex designs on a unified platform to evaluate across various configurations.\nSurprisingly, our experiments reveal several unexpected insights, such as the\nmarginal lookup enhancement when allocating a large memory budget to learned\nindexes and modest retraining overhead of learned indexes. Besides, we also\noffer practical guidelines to help developers intelligently select and tune\nlearned indexes for custom use cases.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152679
  },
  {
    "title": "LEANN: A Low-Storage Vector Index",
    "abstract": "Embedding-based search is widely used in applications such as recommendation\nand retrieval-augmented generation (RAG). Recently, there is a growing demand\nto support these capabilities over personal data stored locally on devices.\nHowever, maintaining the necessary data structure associated with the\nembedding-based search is often infeasible due to its high storage overhead.\nFor example, indexing 100 GB of raw data requires 150 to 700 GB of storage,\nmaking local deployment impractical. Reducing this overhead while maintaining\nsearch quality and latency becomes a critical challenge. In this paper, we\npresent LEANN, a storage-efficient approximate nearest neighbor (ANN) search\nindex optimized for resource-constrained personal devices. LEANN combines a\ncompact graph-based structure with an efficient on-the-fly recomputation\nstrategy to enable fast and accurate retrieval with minimal storage overhead.\nOur evaluation shows that LEANN reduces index size to under 5% of the original\nraw data, achieving up to 50 times smaller storage than standard indexes, while\nmaintaining 90% top-3 recall in under 2 seconds on real-world question\nanswering benchmarks.",
    "text": "LEANN: A Low-Storage Vector Index Embedding-based search is widely used in applications such as recommendation\nand retrieval-augmented generation (RAG). Recently, there is a growing demand\nto support these capabilities over personal data stored locally on devices.\nHowever, maintaining the necessary data structure associated with the\nembedding-based search is often infeasible due to its high storage overhead.\nFor example, indexing 100 GB of raw data requires 150 to 700 GB of storage,\nmaking local deployment impractical. Reducing this overhead while maintaining\nsearch quality and latency becomes a critical challenge. In this paper, we\npresent LEANN, a storage-efficient approximate nearest neighbor (ANN) search\nindex optimized for resource-constrained personal devices. LEANN combines a\ncompact graph-based structure with an efficient on-the-fly recomputation\nstrategy to enable fast and accurate retrieval with minimal storage overhead.\nOur evaluation shows that LEANN reduces index size to under 5% of the original\nraw data, achieving up to 50 times smaller storage than standard indexes, while\nmaintaining 90% top-3 recall in under 2 seconds on real-world question\nanswering benchmarks.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152683
  },
  {
    "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data",
    "abstract": "Language models (LMs) are increasingly being deployed to perform autonomous\ndata analyses. However, their data awareness -- the ability to recognize,\nreason over, and appropriately handle data artifacts such as missing values,\noutliers, and logical inconsistencies -- remains underexplored. These artifacts\nare especially common in real-world tabular data and, if mishandled, can\nsignificantly compromise the validity of analytical conclusions. To address\nthis gap, we present RADAR, a benchmark for systematically evaluating\ndata-aware reasoning on tabular data. We develop a framework to simulate data\nartifacts via programmatic perturbations to enable targeted evaluation of model\nbehavior. RADAR comprises 2980 table query pairs, grounded in real-world data\nspanning 9 domains and 5 data artifact types. In addition to evaluating\nartifact handling, RADAR systematically varies table size to study how\nreasoning performance holds when increasing table size. Our evaluation reveals\nthat, despite decent performance on tables without data artifacts, frontier\nmodels degrade significantly when data artifacts are introduced, exposing\ncritical gaps in their capacity for robust, data-aware analysis. Designed to be\nflexible and extensible, RADAR supports diverse perturbation types and\ncontrollable table sizes, offering a valuable resource for advancing tabular\nreasoning.",
    "text": "RADAR: Benchmarking Language Models on Imperfect Tabular Data Language models (LMs) are increasingly being deployed to perform autonomous\ndata analyses. However, their data awareness -- the ability to recognize,\nreason over, and appropriately handle data artifacts such as missing values,\noutliers, and logical inconsistencies -- remains underexplored. These artifacts\nare especially common in real-world tabular data and, if mishandled, can\nsignificantly compromise the validity of analytical conclusions. To address\nthis gap, we present RADAR, a benchmark for systematically evaluating\ndata-aware reasoning on tabular data. We develop a framework to simulate data\nartifacts via programmatic perturbations to enable targeted evaluation of model\nbehavior. RADAR comprises 2980 table query pairs, grounded in real-world data\nspanning 9 domains and 5 data artifact types. In addition to evaluating\nartifact handling, RADAR systematically varies table size to study how\nreasoning performance holds when increasing table size. Our evaluation reveals\nthat, despite decent performance on tables without data artifacts, frontier\nmodels degrade significantly when data artifacts are introduced, exposing\ncritical gaps in their capacity for robust, data-aware analysis. Designed to be\nflexible and extensible, RADAR supports diverse perturbation types and\ncontrollable table sizes, offering a valuable resource for advancing tabular\nreasoning.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152687
  },
  {
    "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents",
    "abstract": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.",
    "text": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152693
  },
  {
    "title": "Quantum Information-Theoretical Size Bounds for Conjunctive Queries with\n  Functional Dependencies",
    "abstract": "Deriving formulations for computing and estimating tight worst-case size\nincreases for conjunctive queries with various constraints has been at the core\nof theoretical database research. If the problem has no constraints or only one\nconstraint, such as functional dependencies or degree constraints, tight\nworst-case size bounds have been proven, and they are even practically\ncomputable. If the problem has more than one constraint, computing tight bounds\ncan be difficult in practice and may even require an infinite number of linear\ninequalities in its optimization formulation. While these challenges have been\naddressed with varying methods, no prior research has employed quantum\ninformation theory to address this problem. In this work, we establish a\nconnection between earlier work on estimating size bounds for conjunctive\nqueries with classical information theory and the field of quantum information\ntheory. We propose replacing the classical Shannon entropy formulation with the\nquantum R\\'enyi entropy. Whereas classical Shannon entropy requires infinitely\nmany inequalities to characterize the optimization space, R\\'enyi entropy\nrequires only one type of inequality, which is non-negativity. Although this is\na promising modification, optimization with respect to the quantum states\ninstead of classical distributions creates a new set of challenges that prevent\nus from finding a practically computable, tight worst-case size bound. In this\nline, we propose a quantum version to derive worst-case size bounds. The\nprevious tight classical worst-case size bound can be viewed as a special limit\nof this quantum bound. We also provide a comprehensive background on prior\nresearch and discuss the future possibilities of quantum information theory in\ntheoretical database research.",
    "text": "Quantum Information-Theoretical Size Bounds for Conjunctive Queries with\n  Functional Dependencies Deriving formulations for computing and estimating tight worst-case size\nincreases for conjunctive queries with various constraints has been at the core\nof theoretical database research. If the problem has no constraints or only one\nconstraint, such as functional dependencies or degree constraints, tight\nworst-case size bounds have been proven, and they are even practically\ncomputable. If the problem has more than one constraint, computing tight bounds\ncan be difficult in practice and may even require an infinite number of linear\ninequalities in its optimization formulation. While these challenges have been\naddressed with varying methods, no prior research has employed quantum\ninformation theory to address this problem. In this work, we establish a\nconnection between earlier work on estimating size bounds for conjunctive\nqueries with classical information theory and the field of quantum information\ntheory. We propose replacing the classical Shannon entropy formulation with the\nquantum R\\'enyi entropy. Whereas classical Shannon entropy requires infinitely\nmany inequalities to characterize the optimization space, R\\'enyi entropy\nrequires only one type of inequality, which is non-negativity. Although this is\na promising modification, optimization with respect to the quantum states\ninstead of classical distributions creates a new set of challenges that prevent\nus from finding a practically computable, tight worst-case size bound. In this\nline, we propose a quantum version to derive worst-case size bounds. The\nprevious tight classical worst-case size bound can be viewed as a special limit\nof this quantum bound. We also provide a comprehensive background on prior\nresearch and discuss the future possibilities of quantum information theory in\ntheoretical database research.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152697
  },
  {
    "title": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over\n  Data Lakes",
    "abstract": "Constructing real-world data-to-insight pipelines often involves data\nextraction from data lakes, data integration across heterogeneous data sources,\nand diverse operations from data cleaning to analysis. The design and\nimplementation of data science pipelines require domain knowledge, technical\nexpertise, and even project-specific insights. AI systems have shown remarkable\nreasoning, coding, and understanding capabilities. However, it remains unclear\nto what extent these capabilities translate into successful design and\nexecution of such complex pipelines. We introduce KRAMABENCH: a benchmark\ncomposed of 104 manually-curated real-world data science pipelines spanning\n1700 data files from 24 data sources in 6 different domains. We show that these\npipelines test the end-to-end capabilities of AI systems on data processing,\nrequiring data discovery, wrangling and cleaning, efficient processing,\nstatistical reasoning, and orchestrating data processing steps given a\nhigh-level task. Our evaluation tests 5 general models and 3 code generation\nmodels using our reference framework, DS-GURU, which instructs the AI model to\ndecompose a question into a sequence of subtasks, reason through each step, and\nsynthesize Python code that implements the proposed design. Our results on\nKRAMABENCH show that, although the models are sufficiently capable of solving\nwell-specified data science code generation tasks, when extensive data\nprocessing and domain knowledge are required to construct real-world data\nscience pipelines, existing out-of-box models fall short. Progress on\nKramaBench represents crucial steps towards developing autonomous data science\nagents for real-world applications. Our code, reference framework, and data are\navailable at https://github.com/mitdbg/KramaBench.",
    "text": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over\n  Data Lakes Constructing real-world data-to-insight pipelines often involves data\nextraction from data lakes, data integration across heterogeneous data sources,\nand diverse operations from data cleaning to analysis. The design and\nimplementation of data science pipelines require domain knowledge, technical\nexpertise, and even project-specific insights. AI systems have shown remarkable\nreasoning, coding, and understanding capabilities. However, it remains unclear\nto what extent these capabilities translate into successful design and\nexecution of such complex pipelines. We introduce KRAMABENCH: a benchmark\ncomposed of 104 manually-curated real-world data science pipelines spanning\n1700 data files from 24 data sources in 6 different domains. We show that these\npipelines test the end-to-end capabilities of AI systems on data processing,\nrequiring data discovery, wrangling and cleaning, efficient processing,\nstatistical reasoning, and orchestrating data processing steps given a\nhigh-level task. Our evaluation tests 5 general models and 3 code generation\nmodels using our reference framework, DS-GURU, which instructs the AI model to\ndecompose a question into a sequence of subtasks, reason through each step, and\nsynthesize Python code that implements the proposed design. Our results on\nKRAMABENCH show that, although the models are sufficiently capable of solving\nwell-specified data science code generation tasks, when extensive data\nprocessing and domain knowledge are required to construct real-world data\nscience pipelines, existing out-of-box models fall short. Progress on\nKramaBench represents crucial steps towards developing autonomous data science\nagents for real-world applications. Our code, reference framework, and data are\navailable at https://github.com/mitdbg/KramaBench.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152701
  },
  {
    "title": "Stream DaQ: Stream-First Data Quality Monitoring",
    "abstract": "Data quality is fundamental to modern data science workflows, where data\ncontinuously flows as unbounded streams feeding critical downstream tasks, from\nelementary analytics to advanced artificial intelligence models. Existing data\nquality approaches either focus exclusively on static data or treat streaming\nas an extension of batch processing, lacking the temporal granularity and\ncontextual awareness required for true streaming applications. In this paper,\nwe present a novel data quality monitoring model specifically designed for\nunbounded data streams. Our model introduces stream-first concepts, such as\nconfigurable windowing mechanisms, dynamic constraint adaptation, and\ncontinuous assessment that produces quality meta-streams for real-time pipeline\nawareness. To demonstrate practical applicability, we developed Stream DaQ, an\nopen-source Python framework that implements our theoretical model. Stream DaQ\nunifies and adapts over 30 quality checks fragmented across existing static\ntools into a comprehensive streaming suite, enabling practitioners to define\nsophisticated, context-aware quality constraints through compositional\nexpressiveness. Our evaluation demonstrates that the model's implementation\nsignificantly outperforms a production-grade alternative in both execution time\nand throughput while offering richer functionality via native streaming\ncapabilities compared to other choices. Through its Python-native design,\nStream DaQ seamlessly integrates with modern data science workflows, making\ncontinuous quality monitoring accessible to the broader data science community.",
    "text": "Stream DaQ: Stream-First Data Quality Monitoring Data quality is fundamental to modern data science workflows, where data\ncontinuously flows as unbounded streams feeding critical downstream tasks, from\nelementary analytics to advanced artificial intelligence models. Existing data\nquality approaches either focus exclusively on static data or treat streaming\nas an extension of batch processing, lacking the temporal granularity and\ncontextual awareness required for true streaming applications. In this paper,\nwe present a novel data quality monitoring model specifically designed for\nunbounded data streams. Our model introduces stream-first concepts, such as\nconfigurable windowing mechanisms, dynamic constraint adaptation, and\ncontinuous assessment that produces quality meta-streams for real-time pipeline\nawareness. To demonstrate practical applicability, we developed Stream DaQ, an\nopen-source Python framework that implements our theoretical model. Stream DaQ\nunifies and adapts over 30 quality checks fragmented across existing static\ntools into a comprehensive streaming suite, enabling practitioners to define\nsophisticated, context-aware quality constraints through compositional\nexpressiveness. Our evaluation demonstrates that the model's implementation\nsignificantly outperforms a production-grade alternative in both execution time\nand throughput while offering richer functionality via native streaming\ncapabilities compared to other choices. Through its Python-native design,\nStream DaQ seamlessly integrates with modern data science workflows, making\ncontinuous quality monitoring accessible to the broader data science community.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152705
  },
  {
    "title": "Differentially Private Explanations for Clusters",
    "abstract": "The dire need to protect sensitive data has led to various flavors of privacy\ndefinitions. Among these, Differential privacy (DP) is considered one of the\nmost rigorous and secure notions of privacy, enabling data analysis while\npreserving the privacy of data contributors. One of the fundamental tasks of\ndata analysis is clustering , which is meant to unravel hidden patterns within\ncomplex datasets. However, interpreting clustering results poses significant\nchallenges, and often necessitates an extensive analytical process.\nInterpreting clustering results under DP is even more challenging, as analysts\nare provided with noisy responses to queries, and longer, manual exploration\nsessions require additional noise to meet privacy constraints. While increasing\nattention has been given to clustering explanation frameworks that aim at\nassisting analysts by automatically uncovering the characteristics of each\ncluster, such frameworks may also disclose sensitive information within the\ndataset, leading to a breach in privacy. To address these challenges, we\npresent DPClustX, a framework that provides explanations for black-box\nclustering results while satisfying DP. DPClustX takes as input the sensitive\ndataset alongside privately computed clustering labels, and outputs a global\nexplanation, emphasizing prominent characteristics of each cluster while\nguaranteeing DP. We perform an extensive experimental analysis of DPClustX on\nreal data, showing that it provides insightful and accurate explanations even\nunder tight privacy constraints.",
    "text": "Differentially Private Explanations for Clusters The dire need to protect sensitive data has led to various flavors of privacy\ndefinitions. Among these, Differential privacy (DP) is considered one of the\nmost rigorous and secure notions of privacy, enabling data analysis while\npreserving the privacy of data contributors. One of the fundamental tasks of\ndata analysis is clustering , which is meant to unravel hidden patterns within\ncomplex datasets. However, interpreting clustering results poses significant\nchallenges, and often necessitates an extensive analytical process.\nInterpreting clustering results under DP is even more challenging, as analysts\nare provided with noisy responses to queries, and longer, manual exploration\nsessions require additional noise to meet privacy constraints. While increasing\nattention has been given to clustering explanation frameworks that aim at\nassisting analysts by automatically uncovering the characteristics of each\ncluster, such frameworks may also disclose sensitive information within the\ndataset, leading to a breach in privacy. To address these challenges, we\npresent DPClustX, a framework that provides explanations for black-box\nclustering results while satisfying DP. DPClustX takes as input the sensitive\ndataset alongside privately computed clustering labels, and outputs a global\nexplanation, emphasizing prominent characteristics of each cluster while\nguaranteeing DP. We perform an extensive experimental analysis of DPClustX on\nreal data, showing that it provides insightful and accurate explanations even\nunder tight privacy constraints.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152709
  },
  {
    "title": "Training-Free Query Optimization via LLM-Based Plan Similarity",
    "abstract": "Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems.",
    "text": "Training-Free Query Optimization via LLM-Based Plan Similarity Large language model (LLM) embeddings offer a promising new avenue for\ndatabase query optimization. In this paper, we explore how pre-trained\nexecution plan embeddings can guide SQL query execution without the need for\nadditional model training. We introduce LLM-PM (LLM-based Plan Mapping), a\nframework that embeds the default execution plan of a query, finds its k\nnearest neighbors among previously executed plans, and recommends database\nhintsets based on neighborhood voting. A lightweight consistency check\nvalidates the selected hint, while a fallback mechanism searches the full hint\nspace when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM\nachieves an average speed-up of 21% query latency reduction. This work\nhighlights the potential of LLM-powered embeddings to deliver practical\nimprovements in query performance and opens new directions for training-free,\nembedding-based optimizer guidance systems.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152713
  },
  {
    "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark",
    "abstract": "Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU.",
    "text": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152717
  },
  {
    "title": "Natural Language Interaction with Databases on Edge Devices in the\n  Internet of Battlefield Things",
    "abstract": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making.",
    "text": "Natural Language Interaction with Databases on Edge Devices in the\n  Internet of Battlefield Things The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152724
  },
  {
    "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM",
    "abstract": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
    "text": "Memory Hierarchy Design for Caching Middleware in the Age of NVM Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152728
  },
  {
    "title": "BVLSM: Write-Efficient LSM-Tree Storage via WAL-Time Key-Value\n  Separation",
    "abstract": "Modern data-intensive applications increasingly store and process big-value\nitems, such as multimedia objects and machine learning embeddings, which\nexacerbate storage inefficiencies in Log-Structured Merge-Tree (LSM)-based\nkey-value stores. This paper presents BVLSM, a Write-Ahead Log (WAL)-time\nkey-value separation mechanism designed to address three key challenges in\nLSM-Tree storage systems: write amplification, poor memory utilization, and I/O\njitter under big-value workloads. Unlike state-of-the-art approaches that delay\nkey-value separation until the flush stage, leading to redundant data in\nMemTables and repeated writes. BVLSM proactively decouples keys and values\nduring the WAL phase. The MemTable stores only lightweight metadata, allowing\nmulti-queue parallel store for big value. The benchmark results show that BVLSM\nsignificantly outperforms both RocksDB and BlobDB under 64KB random write\nworkloads. In asynchronous WAL mode, it achieves throughput improvements of\n7.6x over RocksDB and 1.9x over BlobDB.",
    "text": "BVLSM: Write-Efficient LSM-Tree Storage via WAL-Time Key-Value\n  Separation Modern data-intensive applications increasingly store and process big-value\nitems, such as multimedia objects and machine learning embeddings, which\nexacerbate storage inefficiencies in Log-Structured Merge-Tree (LSM)-based\nkey-value stores. This paper presents BVLSM, a Write-Ahead Log (WAL)-time\nkey-value separation mechanism designed to address three key challenges in\nLSM-Tree storage systems: write amplification, poor memory utilization, and I/O\njitter under big-value workloads. Unlike state-of-the-art approaches that delay\nkey-value separation until the flush stage, leading to redundant data in\nMemTables and repeated writes. BVLSM proactively decouples keys and values\nduring the WAL phase. The MemTable stores only lightweight metadata, allowing\nmulti-queue parallel store for big value. The benchmark results show that BVLSM\nsignificantly outperforms both RocksDB and BlobDB under 64KB random write\nworkloads. In asynchronous WAL mode, it achieves throughput improvements of\n7.6x over RocksDB and 1.9x over BlobDB.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152735
  },
  {
    "title": "TransClean: Finding False Positives in Multi-Source Entity Matching\n  under Real-World Conditions via Transitive Consistency",
    "abstract": "We present TransClean, a method for detecting false positive predictions of\nentity matching algorithms under real-world conditions characterized by\nlarge-scale, noisy, and unlabeled multi-source datasets that undergo\ndistributional shifts. TransClean is explicitly designed to operate with\nmultiple data sources in an efficient, robust and fast manner while accounting\nfor edge cases and requiring limited manual labeling. TransClean leverages the\nTransitive Consistency of a matching, a measure of the consistency of a\npairwise matching model f_theta on the matching it produces G_f_theta, based\nboth on its predictions on directly evaluated record pairs and its predictions\non implied record pairs. TransClean iteratively modifies a matching through\ngradually removing false positive matches while removing as few true positive\nmatches as possible. In each of these steps, the estimation of the Transitive\nConsistency is exclusively done through model evaluations and produces\nquantities that can be used as proxies of the amounts of true and false\npositives in the matching while not requiring any manual labeling, producing an\nestimate of the quality of the matching and indicating which record groups are\nlikely to contain false positives. In our experiments, we compare combining\nTransClean with a naively trained pairwise matching model (DistilBERT) and with\na state-of-the-art end-to-end matching method (CLER) and illustrate the\nflexibility of TransClean in being able to detect most of the false positives\nof either setup across a variety of datasets. Our experiments show that\nTransClean induces an average +24.42 F1 score improvement for entity matching\nin a multi-source setting when compared to traditional pair-wise matching\nalgorithms.",
    "text": "TransClean: Finding False Positives in Multi-Source Entity Matching\n  under Real-World Conditions via Transitive Consistency We present TransClean, a method for detecting false positive predictions of\nentity matching algorithms under real-world conditions characterized by\nlarge-scale, noisy, and unlabeled multi-source datasets that undergo\ndistributional shifts. TransClean is explicitly designed to operate with\nmultiple data sources in an efficient, robust and fast manner while accounting\nfor edge cases and requiring limited manual labeling. TransClean leverages the\nTransitive Consistency of a matching, a measure of the consistency of a\npairwise matching model f_theta on the matching it produces G_f_theta, based\nboth on its predictions on directly evaluated record pairs and its predictions\non implied record pairs. TransClean iteratively modifies a matching through\ngradually removing false positive matches while removing as few true positive\nmatches as possible. In each of these steps, the estimation of the Transitive\nConsistency is exclusively done through model evaluations and produces\nquantities that can be used as proxies of the amounts of true and false\npositives in the matching while not requiring any manual labeling, producing an\nestimate of the quality of the matching and indicating which record groups are\nlikely to contain false positives. In our experiments, we compare combining\nTransClean with a naively trained pairwise matching model (DistilBERT) and with\na state-of-the-art end-to-end matching method (CLER) and illustrate the\nflexibility of TransClean in being able to detect most of the false positives\nof either setup across a variety of datasets. Our experiments show that\nTransClean induces an average +24.42 F1 score improvement for entity matching\nin a multi-source setting when compared to traditional pair-wise matching\nalgorithms.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15274
  },
  {
    "title": "An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the\n  Filter-and-Verification Tree and MapReduce",
    "abstract": "Given two different collections of sets, the exact set similarity R-S Join\nfinds all set pairs with similarity no less than a given threshold, which has\nwidespread applications. While existing algorithms accelerate large-scale R-S\nJoins using a two-stage filter-and-verification framework along with the\nparallel and distributed MapReduce framework, they suffer from excessive\ncandidate set pairs, leading to significant I/O, data transfer, and\nverification overhead, and ultimately degrading the performance. This paper\nproposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate\nfiltering and verification into a single stage through filter-and-verification\ntrees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT\n(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that\ncompresses elements and associated sets in memory, enabling single-stage\nprocessing that eliminates the candidate set generation, fast lookups, and\nreduced database scans. Correctness proofs are provided. Second, CF-RS-Join\nwith LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,\nwhich compresses non-branching paths into single nodes and stores them in\nlinear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and\nMR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce\nfor parallel processing. Empirical studies on 7 real-world datasets have been\nconducted to evaluate the performance of the proposed algorithms against\nselected existing algorithms in terms of execution time, scalability, memory\nusage, and disk usage. Experimental results demonstrate that our algorithm\nusing MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.",
    "text": "An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the\n  Filter-and-Verification Tree and MapReduce Given two different collections of sets, the exact set similarity R-S Join\nfinds all set pairs with similarity no less than a given threshold, which has\nwidespread applications. While existing algorithms accelerate large-scale R-S\nJoins using a two-stage filter-and-verification framework along with the\nparallel and distributed MapReduce framework, they suffer from excessive\ncandidate set pairs, leading to significant I/O, data transfer, and\nverification overhead, and ultimately degrading the performance. This paper\nproposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate\nfiltering and verification into a single stage through filter-and-verification\ntrees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT\n(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that\ncompresses elements and associated sets in memory, enabling single-stage\nprocessing that eliminates the candidate set generation, fast lookups, and\nreduced database scans. Correctness proofs are provided. Second, CF-RS-Join\nwith LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,\nwhich compresses non-branching paths into single nodes and stores them in\nlinear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and\nMR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce\nfor parallel processing. Empirical studies on 7 real-world datasets have been\nconducted to evaluate the performance of the proposed algorithms against\nselected existing algorithms in terms of execution time, scalability, memory\nusage, and disk usage. Experimental results demonstrate that our algorithm\nusing MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152747
  },
  {
    "title": "Signals as a First-Class Citizen When Querying Knowledge Graphs",
    "abstract": "Cyber-Physical Systems (CPSs) tightly integrate computation with physical\nentities, often generating vast amounts of time series data from thousands of\nsensors. Although knowledge graphs offer a powerful means to contextualize\nthese data, existing approaches to integrating knowledge graphs with time\nseries data lack a concept to model the continuous temporal values inherent in\nCPSs. This gap can make expressing computations on the sensor data cumbersome.\nIn this work, we propose the integration of knowledge graphs and signals, a\nproven concept for modeling temporal values. By treating signals as first-class\ncitizens in query languages, we can enable seamless querying over knowledge\ngraphs and signals. While the knowledge graph captures information on the CPS,\nsignals represent its run-time data from sensors. We discuss the implications\nof such an approach and propose SigSPARQL, an extension to the SPARQL query\nlanguage, to demonstrate these concepts. Furthermore, we evaluate the\nfeasibility of implementing SigSPARQL with a prototype and demonstrate the\napplicability of the query language for a monitoring use case within a CPS.",
    "text": "Signals as a First-Class Citizen When Querying Knowledge Graphs Cyber-Physical Systems (CPSs) tightly integrate computation with physical\nentities, often generating vast amounts of time series data from thousands of\nsensors. Although knowledge graphs offer a powerful means to contextualize\nthese data, existing approaches to integrating knowledge graphs with time\nseries data lack a concept to model the continuous temporal values inherent in\nCPSs. This gap can make expressing computations on the sensor data cumbersome.\nIn this work, we propose the integration of knowledge graphs and signals, a\nproven concept for modeling temporal values. By treating signals as first-class\ncitizens in query languages, we can enable seamless querying over knowledge\ngraphs and signals. While the knowledge graph captures information on the CPS,\nsignals represent its run-time data from sensors. We discuss the implications\nof such an approach and propose SigSPARQL, an extension to the SPARQL query\nlanguage, to demonstrate these concepts. Furthermore, we evaluate the\nfeasibility of implementing SigSPARQL with a prototype and demonstrate the\napplicability of the query language for a monitoring use case within a CPS.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1527522
  },
  {
    "title": "OxO2 -- A SSSOM mapping browser for logically sound crosswalks",
    "abstract": "EMBL-EBI created OxO to enable users to map between datasets that are\nannotated with different ontologies. Mappings identified by the first version\nof OxO were not necessarily logically sound, lacked important provenance\ninformation such as author and reviewer, and could timeout or crash for certain\nrequests. In this paper we introduce OxO2 to address these concerns. Provenance\nis addressed by implementing SSSOM, a mapping standard that defines provenance\nfor mappings. SSSOM defines the conditions under which logical sound mappings\ncan be derived and is implemented in OxO2 using Nemo, a Datalog rule engine. To\nensure reasoning is performant and memory efficient, Nemo implements a number\nof strategies that ensures OxO2 will be stable for all requests. Due to these\nchanges, OxO2 users will be able to integrate between disparate datasets with\ngreater confidence.",
    "text": "OxO2 -- A SSSOM mapping browser for logically sound crosswalks EMBL-EBI created OxO to enable users to map between datasets that are\nannotated with different ontologies. Mappings identified by the first version\nof OxO were not necessarily logically sound, lacked important provenance\ninformation such as author and reviewer, and could timeout or crash for certain\nrequests. In this paper we introduce OxO2 to address these concerns. Provenance\nis addressed by implementing SSSOM, a mapping standard that defines provenance\nfor mappings. SSSOM defines the conditions under which logical sound mappings\ncan be derived and is implemented in OxO2 using Nemo, a Datalog rule engine. To\nensure reasoning is performant and memory efficient, Nemo implements a number\nof strategies that ensures OxO2 will be stable for all requests. Due to these\nchanges, OxO2 users will be able to integrate between disparate datasets with\ngreater confidence.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152757
  },
  {
    "title": "Universal Reusability in Recommender Systems: The Case for Dataset- and\n  Task-Independent Frameworks",
    "abstract": "Recommender systems are pivotal in delivering personalized experiences across\nindustries, yet their adoption and scalability remain hindered by the need for\nextensive dataset- and task-specific configurations. Existing systems often\nrequire significant manual intervention, domain expertise, and engineering\neffort to adapt to new datasets or tasks, creating barriers to entry and\nlimiting reusability. In contrast, recent advancements in large language models\n(LLMs) have demonstrated the transformative potential of reusable systems,\nwhere a single model can handle diverse tasks without significant\nreconfiguration. Inspired by this paradigm, we propose the Dataset- and\nTask-Independent Recommender System (DTIRS), a framework aimed at maximizing\nthe reusability of recommender systems while minimizing barriers to entry.\nUnlike LLMs, which achieve task generalization directly, DTIRS focuses on\neliminating the need to rebuild or reconfigure recommendation pipelines for\nevery new dataset or task, even though models may still need retraining on new\ndata. By leveraging the novel Dataset Description Language (DsDL), DTIRS\nenables standardized dataset descriptions and explicit task definitions,\nallowing autonomous feature engineering, model selection, and optimization.\nThis paper introduces the concept of DTIRS and establishes a roadmap for\ntransitioning from Level-1 automation (dataset-agnostic but task-specific\nsystems) to Level-2 automation (fully dataset- and task-independent systems).\nAchieving this paradigm would maximize code reusability and lower barriers to\nadoption. We discuss key challenges, including the trade-offs between\ngeneralization and specialization, computational overhead, and scalability,\nwhile presenting DsDL as a foundational tool for this vision.",
    "text": "Universal Reusability in Recommender Systems: The Case for Dataset- and\n  Task-Independent Frameworks Recommender systems are pivotal in delivering personalized experiences across\nindustries, yet their adoption and scalability remain hindered by the need for\nextensive dataset- and task-specific configurations. Existing systems often\nrequire significant manual intervention, domain expertise, and engineering\neffort to adapt to new datasets or tasks, creating barriers to entry and\nlimiting reusability. In contrast, recent advancements in large language models\n(LLMs) have demonstrated the transformative potential of reusable systems,\nwhere a single model can handle diverse tasks without significant\nreconfiguration. Inspired by this paradigm, we propose the Dataset- and\nTask-Independent Recommender System (DTIRS), a framework aimed at maximizing\nthe reusability of recommender systems while minimizing barriers to entry.\nUnlike LLMs, which achieve task generalization directly, DTIRS focuses on\neliminating the need to rebuild or reconfigure recommendation pipelines for\nevery new dataset or task, even though models may still need retraining on new\ndata. By leveraging the novel Dataset Description Language (DsDL), DTIRS\nenables standardized dataset descriptions and explicit task definitions,\nallowing autonomous feature engineering, model selection, and optimization.\nThis paper introduces the concept of DTIRS and establishes a roadmap for\ntransitioning from Level-1 automation (dataset-agnostic but task-specific\nsystems) to Level-2 automation (fully dataset- and task-independent systems).\nAchieving this paradigm would maximize code reusability and lower barriers to\nadoption. We discuss key challenges, including the trade-offs between\ngeneralization and specialization, computational overhead, and scalability,\nwhile presenting DsDL as a foundational tool for this vision.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1527622
  },
  {
    "title": "Hermes: High-Performance Homomorphically Encrypted Vector Databases",
    "abstract": "Fully Homomorphic Encryption (FHE) has long promised the ability to compute\nover encrypted data without revealing sensitive contents -- a foundational goal\nfor secure cloud analytics. Yet despite decades of cryptographic advances,\npractical integration of FHE into real-world relational databases remains\nelusive. This paper presents \\textbf{Hermes}, the first system to enable\nFHE-native vector query processing inside a standard SQL engine. By leveraging\nthe multi-slot capabilities of modern schemes, Hermes introduces a novel data\nmodel that packs multiple records per ciphertext and embeds encrypted auxiliary\nstatistics (e.g., local sums) to support in-place updates and aggregation. To\nreconcile ciphertext immutability with record-level mutability, we develop new\nhomomorphic algorithms based on slot masking, shifting, and rewriting. Hermes\nis implemented as native C++ loadable functions in MySQL using OpenFHE v1.2.4,\ncomprising over 3,500 lines of code. Experiments on real-world datasets show up\nto 1{,}600$\\times$ throughput gain in encryption and over 30$\\times$ speedup in\ninsertion compared to per-tuple baselines. Hermes brings FHE from cryptographic\npromise to practical reality -- realizing a long-standing vision at the\nintersection of databases and secure computation.",
    "text": "Hermes: High-Performance Homomorphically Encrypted Vector Databases Fully Homomorphic Encryption (FHE) has long promised the ability to compute\nover encrypted data without revealing sensitive contents -- a foundational goal\nfor secure cloud analytics. Yet despite decades of cryptographic advances,\npractical integration of FHE into real-world relational databases remains\nelusive. This paper presents \\textbf{Hermes}, the first system to enable\nFHE-native vector query processing inside a standard SQL engine. By leveraging\nthe multi-slot capabilities of modern schemes, Hermes introduces a novel data\nmodel that packs multiple records per ciphertext and embeds encrypted auxiliary\nstatistics (e.g., local sums) to support in-place updates and aggregation. To\nreconcile ciphertext immutability with record-level mutability, we develop new\nhomomorphic algorithms based on slot masking, shifting, and rewriting. Hermes\nis implemented as native C++ loadable functions in MySQL using OpenFHE v1.2.4,\ncomprising over 3,500 lines of code. Experiments on real-world datasets show up\nto 1{,}600$\\times$ throughput gain in encryption and over 30$\\times$ speedup in\ninsertion compared to per-tuple baselines. Hermes brings FHE from cryptographic\npromise to practical reality -- realizing a long-standing vision at the\nintersection of databases and secure computation.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152766
  },
  {
    "title": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering",
    "abstract": "Money laundering enables organized crime by allowing illicit funds to enter\nthe legitimate economy. Although trillions of dollars are laundered each year,\nonly a small fraction is ever uncovered. This stems from a range of factors,\nincluding deliberate evasion by launderers, the rarity of confirmed cases, and\nthe limited visibility each financial institution has into the global\ntransaction network. While several synthetic datasets are available, they fail\nto model the structural and behavioral complexity of real-world money\nlaundering. In particular, they often overlook partial observability, sparse\nand uncertain labels, strategic behavior, temporal dynamics, class imbalance,\nand network-level dependencies. To address these limitations, we present\nAMLGentex, an open-source suite for generating realistic, configurable\ntransaction data and benchmarking detection methods. It enables systematic\nevaluation of anti-money laundering (AML) systems in a controlled environment\nthat captures key real-world challenges. We demonstrate how the framework can\nbe used to rigorously evaluate methods under conditions that reflect the\ncomplexity of practical AML scenarios.",
    "text": "AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering Money laundering enables organized crime by allowing illicit funds to enter\nthe legitimate economy. Although trillions of dollars are laundered each year,\nonly a small fraction is ever uncovered. This stems from a range of factors,\nincluding deliberate evasion by launderers, the rarity of confirmed cases, and\nthe limited visibility each financial institution has into the global\ntransaction network. While several synthetic datasets are available, they fail\nto model the structural and behavioral complexity of real-world money\nlaundering. In particular, they often overlook partial observability, sparse\nand uncertain labels, strategic behavior, temporal dynamics, class imbalance,\nand network-level dependencies. To address these limitations, we present\nAMLGentex, an open-source suite for generating realistic, configurable\ntransaction data and benchmarking detection methods. It enables systematic\nevaluation of anti-money laundering (AML) systems in a controlled environment\nthat captures key real-world challenges. We demonstrate how the framework can\nbe used to rigorously evaluate methods under conditions that reflect the\ncomplexity of practical AML scenarios.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152771
  },
  {
    "title": "Process Mining on Distributed Data Sources",
    "abstract": "Major domains such as logistics, healthcare, and smart cities increasingly\nrely on sensor technologies and distributed infrastructures to monitor complex\nprocesses in real time. These developments are transforming the data landscape\nfrom discrete, structured records stored in centralized systems to continuous,\nfine-grained, and heterogeneous event streams collected across distributed\nenvironments. As a result, traditional process mining techniques, which assume\ncentralized event logs from enterprise systems, are no longer sufficient. In\nthis paper, we discuss the conceptual and methodological foundations for this\nemerging field. We identify three key shifts: from offline to online analysis,\nfrom centralized to distributed computing, and from event logs to sensor data.\nThese shifts challenge traditional assumptions about process data and call for\nnew approaches that integrate infrastructure, data, and user perspectives. To\nthis end, we define a research agenda that addresses six interconnected fields,\neach spanning multiple system dimensions. We advocate a principled methodology\ngrounded in algorithm engineering, combining formal modeling with empirical\nevaluation. This approach enables the development of scalable, privacy-aware,\nand user-centric process mining techniques suitable for distributed\nenvironments. Our synthesis provides a roadmap for advancing process mining\nbeyond its classical setting, toward a more responsive and decentralized\nparadigm of process intelligence.",
    "text": "Process Mining on Distributed Data Sources Major domains such as logistics, healthcare, and smart cities increasingly\nrely on sensor technologies and distributed infrastructures to monitor complex\nprocesses in real time. These developments are transforming the data landscape\nfrom discrete, structured records stored in centralized systems to continuous,\nfine-grained, and heterogeneous event streams collected across distributed\nenvironments. As a result, traditional process mining techniques, which assume\ncentralized event logs from enterprise systems, are no longer sufficient. In\nthis paper, we discuss the conceptual and methodological foundations for this\nemerging field. We identify three key shifts: from offline to online analysis,\nfrom centralized to distributed computing, and from event logs to sensor data.\nThese shifts challenge traditional assumptions about process data and call for\nnew approaches that integrate infrastructure, data, and user perspectives. To\nthis end, we define a research agenda that addresses six interconnected fields,\neach spanning multiple system dimensions. We advocate a principled methodology\ngrounded in algorithm engineering, combining formal modeling with empirical\nevaluation. This approach enables the development of scalable, privacy-aware,\nand user-centric process mining techniques suitable for distributed\nenvironments. Our synthesis provides a roadmap for advancing process mining\nbeyond its classical setting, toward a more responsive and decentralized\nparadigm of process intelligence.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152776
  },
  {
    "title": "A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads",
    "abstract": "Lakehouse systems enable the same data to be queried with multiple execution\nengines. However, selecting the engine best suited to run a SQL query still\nrequires a priori knowledge of the query computational requirements and an\nengine capability, a complex and manual task that only becomes more difficult\nwith the emergence of new engines and workloads. In this paper, we address this\nlimitation by proposing a cross-engine optimizer that can automate engine\nselection for diverse SQL queries through a learned cost model. Optimized with\nhints, a query plan is used for query cost prediction and routing. Cost\nprediction is formulated as a multi-task learning problem, and multiple\npredictor heads, corresponding to different engines and provisionings, are used\nin the model architecture. This eliminates the need to train engine-specific\nmodels and allows the flexible addition of new engines at a minimal fine-tuning\ncost. Results on various databases and engines show that using a query\noptimized logical plan for cost estimation decreases the average Q-error by\neven 12.6% over using unoptimized plans as input. Moreover, the proposed\ncross-engine optimizer reduces the total workload runtime by up to 25.2% in a\nzero-shot setting and 30.4% in a few-shot setting when compared to random\nrouting.",
    "text": "A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads Lakehouse systems enable the same data to be queried with multiple execution\nengines. However, selecting the engine best suited to run a SQL query still\nrequires a priori knowledge of the query computational requirements and an\nengine capability, a complex and manual task that only becomes more difficult\nwith the emergence of new engines and workloads. In this paper, we address this\nlimitation by proposing a cross-engine optimizer that can automate engine\nselection for diverse SQL queries through a learned cost model. Optimized with\nhints, a query plan is used for query cost prediction and routing. Cost\nprediction is formulated as a multi-task learning problem, and multiple\npredictor heads, corresponding to different engines and provisionings, are used\nin the model architecture. This eliminates the need to train engine-specific\nmodels and allows the flexible addition of new engines at a minimal fine-tuning\ncost. Results on various databases and engines show that using a query\noptimized logical plan for cost estimation decreases the average Q-error by\neven 12.6% over using unoptimized plans as input. Moreover, the proposed\ncross-engine optimizer reduces the total workload runtime by up to 25.2% in a\nzero-shot setting and 30.4% in a few-shot setting when compared to random\nrouting.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15278
  },
  {
    "title": "In-context Clustering-based Entity Resolution with Large Language\n  Models: A Design Space Exploration",
    "abstract": "Entity Resolution (ER) is a fundamental data quality improvement task that\nidentifies and links records referring to the same real-world entity.\nTraditional ER approaches often rely on pairwise comparisons, which can be\ncostly in terms of time and monetary resources, especially with large datasets.\nRecently, Large Language Models (LLMs) have shown promising results in ER\ntasks. However, existing methods typically focus on pairwise matching, missing\nthe potential of LLMs to perform clustering directly in a more cost-effective\nand scalable manner. In this paper, we propose a novel in-context clustering\napproach for ER, where LLMs are used to cluster records directly, reducing both\ntime complexity and monetary costs. We systematically investigate the design\nspace for in-context clustering, analyzing the impact of factors such as set\nsize, diversity, variation, and ordering of records on clustering performance.\nBased on these insights, we develop LLM-CER (LLM-powered Clustering-based ER),\nwhich achieves high-quality ER results while minimizing LLM API calls. Our\napproach addresses key challenges, including efficient cluster merging and LLM\nhallucination, providing a scalable and effective solution for ER. Extensive\nexperiments on nine real-world datasets demonstrate that our method\nsignificantly improves result quality, achieving up to 150% higher accuracy,\n10% increase in the F-measure, and reducing API calls by up to 5 times, while\nmaintaining comparable monetary cost to the most cost-effective baseline.",
    "text": "In-context Clustering-based Entity Resolution with Large Language\n  Models: A Design Space Exploration Entity Resolution (ER) is a fundamental data quality improvement task that\nidentifies and links records referring to the same real-world entity.\nTraditional ER approaches often rely on pairwise comparisons, which can be\ncostly in terms of time and monetary resources, especially with large datasets.\nRecently, Large Language Models (LLMs) have shown promising results in ER\ntasks. However, existing methods typically focus on pairwise matching, missing\nthe potential of LLMs to perform clustering directly in a more cost-effective\nand scalable manner. In this paper, we propose a novel in-context clustering\napproach for ER, where LLMs are used to cluster records directly, reducing both\ntime complexity and monetary costs. We systematically investigate the design\nspace for in-context clustering, analyzing the impact of factors such as set\nsize, diversity, variation, and ordering of records on clustering performance.\nBased on these insights, we develop LLM-CER (LLM-powered Clustering-based ER),\nwhich achieves high-quality ER results while minimizing LLM API calls. Our\napproach addresses key challenges, including efficient cluster merging and LLM\nhallucination, providing a scalable and effective solution for ER. Extensive\nexperiments on nine real-world datasets demonstrate that our method\nsignificantly improves result quality, achieving up to 150% higher accuracy,\n10% increase in the F-measure, and reducing API calls by up to 5 times, while\nmaintaining comparable monetary cost to the most cost-effective baseline.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152786
  },
  {
    "title": "PandasBench: A Benchmark for the Pandas API",
    "abstract": "The Pandas API has been central to the success of pandas and its\nalternatives. Despite its importance, there is no benchmark for it, and we\nargue that we cannot repurpose existing benchmarks (from other domains) for the\nPandas API.\n  In this paper, we introduce requirements that are necessary for a Pandas API\nenchmark, and present the first benchmark that fulfills them: PandasBench. We\nargue that it should evaluate the real-world coverage of a technique. Yet,\nreal-world coverage is not sufficient for a useful benchmark, and so we also:\ncleaned it from irrelevant code, adapted it for benchmark usage, and introduced\ninput scaling. We claim that uniform scaling used in other benchmarks (e.g.,\nTPC-H) is too coarse-grained for PandasBench, and use a non-uniform scaling\nscheme. PandasBench is the largest Pandas API benchmark to date, with 102\nnotebooks and 3,721 cells.\n  We used PandasBench to evaluate Modin, Dask, Koalas, and Dias. This is the\nlargest-scale evaluation of all these techniques to date. Prior works report\nsignificant speedups using constrained benchmarks, but we show that on a larger\nbenchmark with real-world code, the most notebooks that got a speedup were\n8/102 (~8%) for Modin, and 0 for both Koalas and Dask. Dias showed speedups in\nup to 55 notebooks (~54%), but it rewrites code incorrectly in certain cases,\nwhich had not been observed in prior work. Second, we identified many failures:\nModin runs only 72/102 (~70%) notebooks, Dask 4 (~4%), Koalas 10 (~10%), and\nDias 97 (95%).",
    "text": "PandasBench: A Benchmark for the Pandas API The Pandas API has been central to the success of pandas and its\nalternatives. Despite its importance, there is no benchmark for it, and we\nargue that we cannot repurpose existing benchmarks (from other domains) for the\nPandas API.\n  In this paper, we introduce requirements that are necessary for a Pandas API\nenchmark, and present the first benchmark that fulfills them: PandasBench. We\nargue that it should evaluate the real-world coverage of a technique. Yet,\nreal-world coverage is not sufficient for a useful benchmark, and so we also:\ncleaned it from irrelevant code, adapted it for benchmark usage, and introduced\ninput scaling. We claim that uniform scaling used in other benchmarks (e.g.,\nTPC-H) is too coarse-grained for PandasBench, and use a non-uniform scaling\nscheme. PandasBench is the largest Pandas API benchmark to date, with 102\nnotebooks and 3,721 cells.\n  We used PandasBench to evaluate Modin, Dask, Koalas, and Dias. This is the\nlargest-scale evaluation of all these techniques to date. Prior works report\nsignificant speedups using constrained benchmarks, but we show that on a larger\nbenchmark with real-world code, the most notebooks that got a speedup were\n8/102 (~8%) for Modin, and 0 for both Koalas and Dask. Dias showed speedups in\nup to 55 notebooks (~54%), but it rewrites code incorrectly in certain cases,\nwhich had not been observed in prior work. Second, we identified many failures:\nModin runs only 72/102 (~70%) notebooks, Dask 4 (~4%), Koalas 10 (~10%), and\nDias 97 (95%).",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152791
  },
  {
    "title": "scDataset: Scalable Data Loading for Deep Learning on Large-Scale\n  Single-Cell Omics",
    "abstract": "Modern single-cell datasets now comprise hundreds of millions of cells,\npresenting significant challenges for training deep learning models that\nrequire shuffled, memory-efficient data loading. While the AnnData format is\nthe community standard for storing single-cell datasets, existing data loading\nsolutions for AnnData are often inadequate: some require loading all data into\nmemory, others convert to dense formats that increase storage demands, and many\nare hampered by slow random disk access. We present scDataset, a PyTorch\nIterableDataset that operates directly on one or more AnnData files without the\nneed for format conversion. The core innovation is a combination of block\nsampling and batched fetching, which together balance randomness and I/O\nefficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\\times$\nspeed-up over AnnLoader, a 27$\\times$ speed-up over HuggingFace Datasets, and\nan 18$\\times$ speed-up over BioNeMo in single-core settings. These advances\ndemocratize large-scale single-cell model training for the broader research\ncommunity.",
    "text": "scDataset: Scalable Data Loading for Deep Learning on Large-Scale\n  Single-Cell Omics Modern single-cell datasets now comprise hundreds of millions of cells,\npresenting significant challenges for training deep learning models that\nrequire shuffled, memory-efficient data loading. While the AnnData format is\nthe community standard for storing single-cell datasets, existing data loading\nsolutions for AnnData are often inadequate: some require loading all data into\nmemory, others convert to dense formats that increase storage demands, and many\nare hampered by slow random disk access. We present scDataset, a PyTorch\nIterableDataset that operates directly on one or more AnnData files without the\nneed for format conversion. The core innovation is a combination of block\nsampling and batched fetching, which together balance randomness and I/O\nefficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\\times$\nspeed-up over AnnLoader, a 27$\\times$ speed-up over HuggingFace Datasets, and\nan 18$\\times$ speed-up over BioNeMo in single-core settings. These advances\ndemocratize large-scale single-cell model training for the broader research\ncommunity.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1527948
  },
  {
    "title": "All You Need Is Binary Search! A Practical View on Lightweight Database\n  Indexing on GPUs",
    "abstract": "Performing binary search on a sorted dense array is a widely used baseline\nwhen benchmarking sophisticated index structures: It is simple, fast to build,\nand indexes the dataset with minimal memory footprint. However, the popular\nopinion is that it cannot compete with sophisticated indexes in terms of lookup\nperformance, and hence, should not actually be considered in practice.\n  Interestingly, in our recent works on (even more sophisticated) GPU-resident\nindex structures, we observed the surprisingly good performance of binary\nsearch in a variety of situations. As a consequence, in this work, we analyze\nthe reasons for this and perform three types of optimizations to the standard\nimplementation to push binary search to its limits on GPUs. We show that our\nhighly-optimized version of binary search outperforms the naive variant by up\nto a factor of 2x which makes it a practical alternative to full-fledged\nindexes, such as the state-of-the-art GPU B+-Tree, while consuming considerably\nless space and having a shorter build time. Apart from the optimizations, we\ndiscuss a generalization of binary search in form of K-ary search, which is\nable to consistently outperform the B+-Tree by a factor of 1.5x to 2.7x while\nhaving a negligible space overhead over binary search.",
    "text": "All You Need Is Binary Search! A Practical View on Lightweight Database\n  Indexing on GPUs Performing binary search on a sorted dense array is a widely used baseline\nwhen benchmarking sophisticated index structures: It is simple, fast to build,\nand indexes the dataset with minimal memory footprint. However, the popular\nopinion is that it cannot compete with sophisticated indexes in terms of lookup\nperformance, and hence, should not actually be considered in practice.\n  Interestingly, in our recent works on (even more sophisticated) GPU-resident\nindex structures, we observed the surprisingly good performance of binary\nsearch in a variety of situations. As a consequence, in this work, we analyze\nthe reasons for this and perform three types of optimizations to the standard\nimplementation to push binary search to its limits on GPUs. We show that our\nhighly-optimized version of binary search outperforms the naive variant by up\nto a factor of 2x which makes it a practical alternative to full-fledged\nindexes, such as the state-of-the-art GPU B+-Tree, while consuming considerably\nless space and having a shorter build time. Apart from the optimizations, we\ndiscuss a generalization of binary search in form of K-ary search, which is\nable to consistently outperform the B+-Tree by a factor of 1.5x to 2.7x while\nhaving a negligible space overhead over binary search.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528
  },
  {
    "title": "Retrieval-Augmented Generation of Ontologies from Relational Databases",
    "abstract": "Transforming relational databases into knowledge graphs with enriched\nontologies enhances semantic interoperability and unlocks advanced graph-based\nlearning and reasoning over data. However, previous approaches either demand\nsignificant manual effort to derive an ontology from a database schema or\nproduce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative\nGeneration of RDB Ontologies, an LLM-driven approach that turns relational\nschemas into rich OWL ontologies with minimal human effort. RIGOR combines\nthree sources via RAG, the database schema and its documentation, a repository\nof domain ontologies, and a growing core ontology, to prompt a generative LLM\nfor producing successive, provenance-tagged delta ontology fragments. Each\nfragment is refined by a judge-LLM before being merged into the core ontology,\nand the process iterates table-by-table following foreign key constraints until\ncoverage is complete. Applied to real-world databases, our approach outputs\nontologies that score highly on standard quality dimensions such as accuracy,\ncompleteness, conciseness, adaptability, clarity, and consistency, while\nsubstantially reducing manual effort.",
    "text": "Retrieval-Augmented Generation of Ontologies from Relational Databases Transforming relational databases into knowledge graphs with enriched\nontologies enhances semantic interoperability and unlocks advanced graph-based\nlearning and reasoning over data. However, previous approaches either demand\nsignificant manual effort to derive an ontology from a database schema or\nproduce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative\nGeneration of RDB Ontologies, an LLM-driven approach that turns relational\nschemas into rich OWL ontologies with minimal human effort. RIGOR combines\nthree sources via RAG, the database schema and its documentation, a repository\nof domain ontologies, and a growing core ontology, to prompt a generative LLM\nfor producing successive, provenance-tagged delta ontology fragments. Each\nfragment is refined by a judge-LLM before being merged into the core ontology,\nand the process iterates table-by-table following foreign key constraints until\ncoverage is complete. Applied to real-world databases, our approach outputs\nontologies that score highly on standard quality dimensions such as accuracy,\ncompleteness, conciseness, adaptability, clarity, and consistency, while\nsubstantially reducing manual effort.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528032
  },
  {
    "title": "SIFBench: An Extensive Benchmark for Fatigue Analysis",
    "abstract": "Fatigue-induced crack growth is a leading cause of structural failure across\ncritical industries such as aerospace, civil engineering, automotive, and\nenergy. Accurate prediction of stress intensity factors (SIFs) -- the key\nparameters governing crack propagation in linear elastic fracture mechanics --\nis essential for assessing fatigue life and ensuring structural integrity.\nWhile machine learning (ML) has shown great promise in SIF prediction, its\nadvancement has been severely limited by the lack of rich, transparent,\nwell-organized, and high-quality datasets.\n  To address this gap, we introduce SIFBench, an open-source, large-scale\nbenchmark database designed to support ML-based SIF prediction. SIFBench\ncontains over 5 million different crack and component geometries derived from\nhigh-fidelity finite element simulations across 37 distinct scenarios, and\nprovides a unified Python interface for seamless data access and customization.\nWe report baseline results using a range of popular ML models -- including\nrandom forests, support vector machines, feedforward neural networks, and\nFourier neural operators -- alongside comprehensive evaluation metrics and\ntemplate code for model training, validation, and assessment. By offering a\nstandardized and scalable resource, SIFBench substantially lowers the entry\nbarrier and fosters the development and application of ML methods in damage\ntolerance design and predictive maintenance.",
    "text": "SIFBench: An Extensive Benchmark for Fatigue Analysis Fatigue-induced crack growth is a leading cause of structural failure across\ncritical industries such as aerospace, civil engineering, automotive, and\nenergy. Accurate prediction of stress intensity factors (SIFs) -- the key\nparameters governing crack propagation in linear elastic fracture mechanics --\nis essential for assessing fatigue life and ensuring structural integrity.\nWhile machine learning (ML) has shown great promise in SIF prediction, its\nadvancement has been severely limited by the lack of rich, transparent,\nwell-organized, and high-quality datasets.\n  To address this gap, we introduce SIFBench, an open-source, large-scale\nbenchmark database designed to support ML-based SIF prediction. SIFBench\ncontains over 5 million different crack and component geometries derived from\nhigh-fidelity finite element simulations across 37 distinct scenarios, and\nprovides a unified Python interface for seamless data access and customization.\nWe report baseline results using a range of popular ML models -- including\nrandom forests, support vector machines, feedforward neural networks, and\nFourier neural operators -- alongside comprehensive evaluation metrics and\ntemplate code for model training, validation, and assessment. By offering a\nstandardized and scalable resource, SIFBench substantially lowers the entry\nbarrier and fosters the development and application of ML methods in damage\ntolerance design and predictive maintenance.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152807
  },
  {
    "title": "VecFlow: A High-Performance Vector Data Management System for\n  Filtered-Search on GPUs",
    "abstract": "Vector search and database systems have become a keystone component in many\nAI applications. While many prior research has investigated how to accelerate\nthe performance of generic vector search, emerging AI applications require\nrunning more sophisticated vector queries efficiently, such as vector search\nwith attribute filters. Unfortunately, recent filtered-ANNS solutions are\nprimarily designed for CPUs, with few exploration and limited performance of\nfiltered-ANNS that take advantage of the massive parallelism offered by GPUs.\nIn this paper, we present VecFlow, a novel high-performance vector filtered\nsearch system that achieves unprecedented high throughput and recall while\nobtaining low latency for filtered-ANNS on GPUs. We propose a novel\nlabel-centric indexing and search algorithm that significantly improves the\nselectivity of ANNS with filters. In addition to algorithmic level\noptimization, we provide architectural-aware optimization for VecFlow's\nfunctional modules, effectively supporting both small batch and large batch\nqueries, and single-label and multi-label query processing. Experimental\nresults on NVIDIA A100 GPU over several public available datasets validate that\nVecFlow achieves 5 million QPS for recall 90%, outperforming state-of-the-art\nCPU-based solutions such as Filtered-DiskANN by up to 135 times. Alternatively,\nVecFlow can easily extend its support to high recall 99% regime, whereas strong\nGPU-based baselines plateau at around 80% recall. The source code is available\nat https://github.com/Supercomputing-System-AI-Lab/VecFlow.",
    "text": "VecFlow: A High-Performance Vector Data Management System for\n  Filtered-Search on GPUs Vector search and database systems have become a keystone component in many\nAI applications. While many prior research has investigated how to accelerate\nthe performance of generic vector search, emerging AI applications require\nrunning more sophisticated vector queries efficiently, such as vector search\nwith attribute filters. Unfortunately, recent filtered-ANNS solutions are\nprimarily designed for CPUs, with few exploration and limited performance of\nfiltered-ANNS that take advantage of the massive parallelism offered by GPUs.\nIn this paper, we present VecFlow, a novel high-performance vector filtered\nsearch system that achieves unprecedented high throughput and recall while\nobtaining low latency for filtered-ANNS on GPUs. We propose a novel\nlabel-centric indexing and search algorithm that significantly improves the\nselectivity of ANNS with filters. In addition to algorithmic level\noptimization, we provide architectural-aware optimization for VecFlow's\nfunctional modules, effectively supporting both small batch and large batch\nqueries, and single-label and multi-label query processing. Experimental\nresults on NVIDIA A100 GPU over several public available datasets validate that\nVecFlow achieves 5 million QPS for recall 90%, outperforming state-of-the-art\nCPU-based solutions such as Filtered-DiskANN by up to 135 times. Alternatively,\nVecFlow can easily extend its support to high recall 99% regime, whereas strong\nGPU-based baselines plateau at around 80% recall. The source code is available\nat https://github.com/Supercomputing-System-AI-Lab/VecFlow.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152811
  },
  {
    "title": "Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings",
    "abstract": "Many modern search domains comprise high-dimensional vectors of floating\npoint numbers derived from neural networks, in the form of embeddings. Typical\nembeddings range in size from hundreds to thousands of dimensions, making the\nsize of the embeddings, and the speed of comparison, a significant issue.\n  Quantisation is a class of mechanism which replaces the floating point values\nwith a smaller representation, for example a short integer. This gives an\napproximation of the embedding space in return for a smaller data\nrepresentation and a faster comparison function.\n  Here we take this idea almost to its extreme: we show how vectors of\narbitrary-precision floating point values can be replaced by vectors whose\nelements are drawn from the set {-1,0,1}. This yields very significant savings\nin space and metric evaluation cost, while maintaining a strong correlation for\nsimilarity measurements.\n  This is achieved by way of a class of convex polytopes which exist in the\nhigh-dimensional space. In this article we give an outline description of these\nobjects, and show how they can be used for the basis of such radical\nquantisation while maintaining a surprising degree of accuracy.",
    "text": "Ultra-Quantisation: Efficient Embedding Search via 1.58-bit Encodings Many modern search domains comprise high-dimensional vectors of floating\npoint numbers derived from neural networks, in the form of embeddings. Typical\nembeddings range in size from hundreds to thousands of dimensions, making the\nsize of the embeddings, and the speed of comparison, a significant issue.\n  Quantisation is a class of mechanism which replaces the floating point values\nwith a smaller representation, for example a short integer. This gives an\napproximation of the embedding space in return for a smaller data\nrepresentation and a faster comparison function.\n  Here we take this idea almost to its extreme: we show how vectors of\narbitrary-precision floating point values can be replaced by vectors whose\nelements are drawn from the set {-1,0,1}. This yields very significant savings\nin space and metric evaluation cost, while maintaining a strong correlation for\nsimilarity measurements.\n  This is achieved by way of a class of convex polytopes which exist in the\nhigh-dimensional space. In this article we give an outline description of these\nobjects, and show how they can be used for the basis of such radical\nquantisation while maintaining a surprising degree of accuracy.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152815
  },
  {
    "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments",
    "abstract": "Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure.",
    "text": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152818
  },
  {
    "title": "Survey: Graph Databases",
    "abstract": "Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions.",
    "text": "Survey: Graph Databases Graph databases have become essential tools for managing complex and\ninterconnected data, which is common in areas like social networks,\nbioinformatics, and recommendation systems. Unlike traditional relational\ndatabases, graph databases offer a more natural way to model and query\nintricate relationships, making them particularly effective for applications\nthat demand flexibility and efficiency in handling interconnected data.\n  Despite their increasing use, graph databases face notable challenges. One\nsignificant issue is the irregular nature of graph data, often marked by\nstructural sparsity, such as in its adjacency matrix representation, which can\nlead to inefficiencies in data read and write operations. Other obstacles\ninclude the high computational demands of traversal-based queries, especially\nwithin large-scale networks, and complexities in managing transactions in\ndistributed graph environments. Additionally, the reliance on traditional\ncentralized architectures limits the scalability of Online Transaction\nProcessing (OLTP), creating bottlenecks due to contention, CPU overhead, and\nnetwork bandwidth constraints.\n  This paper presents a thorough survey of graph databases. It begins by\nexamining property models, query languages, and storage architectures,\noutlining the foundational aspects that users and developers typically engage\nwith. Following this, it provides a detailed analysis of recent advancements in\ngraph database technologies, evaluating these in the context of key aspects\nsuch as architecture, deployment, usage, and development, which collectively\ndefine the capabilities of graph database solutions.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152823
  },
  {
    "title": "Towards Scalable Schema Mapping using Large Language Models",
    "abstract": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
    "text": "Towards Scalable Schema Mapping using Large Language Models The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152826
  },
  {
    "title": "SSCard: Substring Cardinality Estimation using Suffix Tree-Guided\n  Learned FM-Index",
    "abstract": "Accurate cardinality estimation of substring queries, which are commonly\nexpressed using the SQL LIKE predicate, is crucial for query optimization in\ndatabase systems. While both rule-based methods and machine learning-based\nmethods have been developed to optimize various aspects of cardinality\nestimation, their absence of error bounds may result in substantial estimation\nerrors, leading to suboptimal execution plans. In this paper, we propose\nSSCard, a novel SubString Cardinality estimator that leverages a\nspace-efficient FM-Index into flexible database applications. SSCard first\nextends the FM-Index to support multiple strings naturally, and then organizes\nthe FM-index using a pruned suffix tree. The suffix tree structure enables\nprecise cardinality estimation for short patterns and achieves high compression\nvia a pushup operation, especially on a large alphabet with skewed character\ndistributions. Furthermore, SSCard incorporates a spline interpolation method\nwith an error bound to balance space usage and estimation accuracy. Additional\ninnovations include a bidirectional estimation algorithm and incremental update\nstrategies. Extensive experimental results in five real-life datasets show that\nSSCard outperforms both traditional methods and recent learning-based methods,\nwhich achieves an average reduction of 20% in the average q-error, 80% in the\nmaximum q-error, and 50% in the construction time, compared with second-best\napproaches.",
    "text": "SSCard: Substring Cardinality Estimation using Suffix Tree-Guided\n  Learned FM-Index Accurate cardinality estimation of substring queries, which are commonly\nexpressed using the SQL LIKE predicate, is crucial for query optimization in\ndatabase systems. While both rule-based methods and machine learning-based\nmethods have been developed to optimize various aspects of cardinality\nestimation, their absence of error bounds may result in substantial estimation\nerrors, leading to suboptimal execution plans. In this paper, we propose\nSSCard, a novel SubString Cardinality estimator that leverages a\nspace-efficient FM-Index into flexible database applications. SSCard first\nextends the FM-Index to support multiple strings naturally, and then organizes\nthe FM-index using a pruned suffix tree. The suffix tree structure enables\nprecise cardinality estimation for short patterns and achieves high compression\nvia a pushup operation, especially on a large alphabet with skewed character\ndistributions. Furthermore, SSCard incorporates a spline interpolation method\nwith an error bound to balance space usage and estimation accuracy. Additional\ninnovations include a bidirectional estimation algorithm and incremental update\nstrategies. Extensive experimental results in five real-life datasets show that\nSSCard outperforms both traditional methods and recent learning-based methods,\nwhich achieves an average reduction of 20% in the average q-error, 80% in the\nmaximum q-error, and 50% in the construction time, compared with second-best\napproaches.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15283
  },
  {
    "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management",
    "abstract": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
    "text": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data\n  Management Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152833
  },
  {
    "title": "An AI-powered Knowledge Hub for Potato Functional Genomics",
    "abstract": "Potato functional genomics lags due to unsystematic gene information\ncuration, gene identifier inconsistencies across reference genome versions, and\nthe increasing volume of research publications. To address these limitations,\nwe developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging\nLarge Language Models (LLMs) and a systematically curated collection of over\n3,200 high-quality potato research papers spanning over 120 years. This\nplatform integrates two key modules: a functional gene database containing\n2,571 literature-reported genes, meticulously mapped to the latest DMv8.1\nreference genome with resolved nomenclature discrepancies and links to original\npublications; and a potato knowledge base. The knowledge base, built using a\nRetrieval-Augmented Generation (RAG) architecture, accurately answers research\nqueries with literature citations, mitigating LLM \"hallucination.\" Users can\ninteract with the hub via a natural language AI agent, \"Potato Research\nAssistant,\" for querying specialized knowledge, retrieving gene information,\nand extracting sequences. The continuously updated Potato Knowledge Hub aims to\nbe a comprehensive resource, fostering advancements in potato functional\ngenomics and supporting breeding programs.",
    "text": "An AI-powered Knowledge Hub for Potato Functional Genomics Potato functional genomics lags due to unsystematic gene information\ncuration, gene identifier inconsistencies across reference genome versions, and\nthe increasing volume of research publications. To address these limitations,\nwe developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging\nLarge Language Models (LLMs) and a systematically curated collection of over\n3,200 high-quality potato research papers spanning over 120 years. This\nplatform integrates two key modules: a functional gene database containing\n2,571 literature-reported genes, meticulously mapped to the latest DMv8.1\nreference genome with resolved nomenclature discrepancies and links to original\npublications; and a potato knowledge base. The knowledge base, built using a\nRetrieval-Augmented Generation (RAG) architecture, accurately answers research\nqueries with literature citations, mitigating LLM \"hallucination.\" Users can\ninteract with the hub via a natural language AI agent, \"Potato Research\nAssistant,\" for querying specialized knowledge, retrieving gene information,\nand extracting sequences. The continuously updated Potato Knowledge Hub aims to\nbe a comprehensive resource, fostering advancements in potato functional\ngenomics and supporting breeding programs.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152836
  },
  {
    "title": "Searching Clinical Data Using Generative AI",
    "abstract": "Artificial Intelligence (AI) is making a major impact on healthcare,\nparticularly through its application in natural language processing (NLP) and\npredictive analytics. The healthcare sector has increasingly adopted AI for\ntasks such as clinical data analysis and medical code assignment. However,\nsearching for clinical information in large and often unorganized datasets\nremains a manual and error-prone process. Assisting this process with\nautomations can help physicians improve their operational productivity\nsignificantly.\n  In this paper, we present a generative AI approach, coined SearchAI, to\nenhance the accuracy and efficiency of searching clinical data. Unlike\ntraditional code assignment, which is a one-to-one problem, clinical data\nsearch is a one-to-many problem, i.e., a given search query can map to a family\nof codes. Healthcare professionals typically search for groups of related\ndiseases, drugs, or conditions that map to many codes, and therefore, they need\nsearch tools that can handle keyword synonyms, semantic variants, and broad\nopen-ended queries. SearchAI employs a hierarchical model that respects the\ncoding hierarchy and improves the traversal of relationships from parent to\nchild nodes. SearchAI navigates these hierarchies predictively and ensures that\nall paths are reachable without losing any relevant nodes.\n  To evaluate the effectiveness of SearchAI, we conducted a series of\nexperiments using both public and production datasets. Our results show that\nSearchAI outperforms default hierarchical traversals across several metrics,\nincluding accuracy, robustness, performance, and scalability. SearchAI can help\nmake clinical data more accessible, leading to streamlined workflows, reduced\nadministrative burden, and enhanced coding and diagnostic accuracy.",
    "text": "Searching Clinical Data Using Generative AI Artificial Intelligence (AI) is making a major impact on healthcare,\nparticularly through its application in natural language processing (NLP) and\npredictive analytics. The healthcare sector has increasingly adopted AI for\ntasks such as clinical data analysis and medical code assignment. However,\nsearching for clinical information in large and often unorganized datasets\nremains a manual and error-prone process. Assisting this process with\nautomations can help physicians improve their operational productivity\nsignificantly.\n  In this paper, we present a generative AI approach, coined SearchAI, to\nenhance the accuracy and efficiency of searching clinical data. Unlike\ntraditional code assignment, which is a one-to-one problem, clinical data\nsearch is a one-to-many problem, i.e., a given search query can map to a family\nof codes. Healthcare professionals typically search for groups of related\ndiseases, drugs, or conditions that map to many codes, and therefore, they need\nsearch tools that can handle keyword synonyms, semantic variants, and broad\nopen-ended queries. SearchAI employs a hierarchical model that respects the\ncoding hierarchy and improves the traversal of relationships from parent to\nchild nodes. SearchAI navigates these hierarchies predictively and ensures that\nall paths are reachable without losing any relevant nodes.\n  To evaluate the effectiveness of SearchAI, we conducted a series of\nexperiments using both public and production datasets. Our results show that\nSearchAI outperforms default hierarchical traversals across several metrics,\nincluding accuracy, robustness, performance, and scalability. SearchAI can help\nmake clinical data more accessible, leading to streamlined workflows, reduced\nadministrative burden, and enhanced coding and diagnostic accuracy.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528409
  },
  {
    "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional\n  Chinese Medicine",
    "abstract": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.",
    "text": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional\n  Chinese Medicine Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152844
  },
  {
    "title": "Towards Explainable Sequential Learning",
    "abstract": "This paper offers a hybrid explainable temporal data processing pipeline,\nDataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence\n(EMeriTAte+DF), bridging numerical-driven temporal data classification with an\nevent-based one through verified artificial intelligence principles, enabling\nhuman-explainable results. This was possible through a preliminary a posteriori\nexplainable phase describing the numerical input data in terms of concurrent\nconstituents with numerical payloads. This further required extending the\nevent-based literature to design specification mining algorithms supporting\nconcurrent constituents. Our previous and current solutions outperform\nstate-of-the-art solutions for multivariate time series classifications, thus\nshowcasing the effectiveness of the proposed methodology.",
    "text": "Towards Explainable Sequential Learning This paper offers a hybrid explainable temporal data processing pipeline,\nDataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence\n(EMeriTAte+DF), bridging numerical-driven temporal data classification with an\nevent-based one through verified artificial intelligence principles, enabling\nhuman-explainable results. This was possible through a preliminary a posteriori\nexplainable phase describing the numerical input data in terms of concurrent\nconstituents with numerical payloads. This further required extending the\nevent-based literature to design specification mining algorithms supporting\nconcurrent constituents. Our previous and current solutions outperform\nstate-of-the-art solutions for multivariate time series classifications, thus\nshowcasing the effectiveness of the proposed methodology.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528478
  },
  {
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "abstract": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "text": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152852
  },
  {
    "title": "LINEAGEX: A Column Lineage Extraction System for SQL",
    "abstract": "As enterprise data grows in size and complexity, column-level data lineage,\nwhich records the creation, transformation, and reference of each column in the\nwarehouse, has been the key to effective data governance that assists tasks\nlike data quality monitoring, storage refactoring, and workflow migration.\nUnfortunately, existing systems introduce overheads by integration with query\nexecution or fail to achieve satisfying accuracy for column lineage. In this\npaper, we demonstrate LINEAGEX, a lightweight Python library that infers column\nlevel lineage from SQL queries and visualizes it through an interactive\ninterface. LINEAGEX achieves high coverage and accuracy for column lineage\nextraction by intelligently traversing query parse trees and handling\nambiguities. The demonstration walks through use cases of building lineage\ngraphs and troubleshooting data quality issues. LINEAGEX is open sourced at\nhttps://github.com/sfu-db/lineagex and our video demonstration is at\nhttps://youtu.be/5LaBBDDitlw",
    "text": "LINEAGEX: A Column Lineage Extraction System for SQL As enterprise data grows in size and complexity, column-level data lineage,\nwhich records the creation, transformation, and reference of each column in the\nwarehouse, has been the key to effective data governance that assists tasks\nlike data quality monitoring, storage refactoring, and workflow migration.\nUnfortunately, existing systems introduce overheads by integration with query\nexecution or fail to achieve satisfying accuracy for column lineage. In this\npaper, we demonstrate LINEAGEX, a lightweight Python library that infers column\nlevel lineage from SQL queries and visualizes it through an interactive\ninterface. LINEAGEX achieves high coverage and accuracy for column lineage\nextraction by intelligently traversing query parse trees and handling\nambiguities. The demonstration walks through use cases of building lineage\ngraphs and troubleshooting data quality issues. LINEAGEX is open sourced at\nhttps://github.com/sfu-db/lineagex and our video demonstration is at\nhttps://youtu.be/5LaBBDDitlw",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152856
  },
  {
    "title": "TailorSQL: An NL2SQL System Tailored to Your Query Workload",
    "abstract": "NL2SQL (natural language to SQL) translates natural language questions into\nSQL queries, thereby making structured data accessible to non-technical users,\nserving as the foundation for intelligent data applications. State-of-the-art\nNL2SQL techniques typically perform translation by retrieving database-specific\ninformation, such as the database schema, and invoking a pre-trained large\nlanguage model (LLM) using the question and retrieved information to generate\nthe SQL query.\n  However, existing NL2SQL techniques miss a key opportunity which is present\nin real-world settings: NL2SQL is typically applied on existing databases which\nhave already served many SQL queries in the past. The past query workload\nimplicitly contains information which is helpful for accurate NL2SQL\ntranslation and is not apparent from the database schema alone, such as common\njoin paths and the semantics of obscurely-named tables and columns. We\nintroduce TailorSQL, a NL2SQL system that takes advantage of information in the\npast query workload to improve both the accuracy and latency of translating\nnatural language questions into SQL. By specializing to a given workload,\nTailorSQL achieves up to 2$\\times$ improvement in execution accuracy on\nstandardized benchmarks.",
    "text": "TailorSQL: An NL2SQL System Tailored to Your Query Workload NL2SQL (natural language to SQL) translates natural language questions into\nSQL queries, thereby making structured data accessible to non-technical users,\nserving as the foundation for intelligent data applications. State-of-the-art\nNL2SQL techniques typically perform translation by retrieving database-specific\ninformation, such as the database schema, and invoking a pre-trained large\nlanguage model (LLM) using the question and retrieved information to generate\nthe SQL query.\n  However, existing NL2SQL techniques miss a key opportunity which is present\nin real-world settings: NL2SQL is typically applied on existing databases which\nhave already served many SQL queries in the past. The past query workload\nimplicitly contains information which is helpful for accurate NL2SQL\ntranslation and is not apparent from the database schema alone, such as common\njoin paths and the semantics of obscurely-named tables and columns. We\nintroduce TailorSQL, a NL2SQL system that takes advantage of information in the\npast query workload to improve both the accuracy and latency of translating\nnatural language questions into SQL. By specializing to a given workload,\nTailorSQL achieves up to 2$\\times$ improvement in execution accuracy on\nstandardized benchmarks.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152859
  },
  {
    "title": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim\n  Verification with Interactive Graph Representation",
    "abstract": "Claim verification is a long-standing and challenging task that demands not\nonly high accuracy but also explainability of the verification process. This\ntask becomes an emerging research issue in the era of large language models\n(LLMs) since real-world claims are often complex, featuring intricate semantic\nstructures or obfuscated entities. Traditional approaches typically address\nthis by decomposing claims into sub-claims and querying a knowledge base to\nresolve hidden or ambiguous entities. However, the absence of effective\ndisambiguation strategies for these entities can compromise the entire\nverification process. To address these challenges, we propose\nVerify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and\ncomprehension abilities of LLM agents. VeGraph operates in three phases: (1)\nGraph Representation - an input claim is decomposed into structured triplets,\nforming a graph-based representation that integrates both structured and\nunstructured information; (2) Entity Disambiguation -VeGraph iteratively\ninteracts with the knowledge base to resolve ambiguous entities within the\ngraph for deeper sub-claim verification; and (3) Verification - remaining\ntriplets are verified to complete the fact-checking process. Experiments using\nMeta-Llama-3-70B (instruct version) show that VeGraph achieves competitive\nperformance compared to baselines on two benchmarks HoVer and FEVEROUS,\neffectively addressing claim verification challenges. Our source code and data\nare available for further exploitation.",
    "text": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim\n  Verification with Interactive Graph Representation Claim verification is a long-standing and challenging task that demands not\nonly high accuracy but also explainability of the verification process. This\ntask becomes an emerging research issue in the era of large language models\n(LLMs) since real-world claims are often complex, featuring intricate semantic\nstructures or obfuscated entities. Traditional approaches typically address\nthis by decomposing claims into sub-claims and querying a knowledge base to\nresolve hidden or ambiguous entities. However, the absence of effective\ndisambiguation strategies for these entities can compromise the entire\nverification process. To address these challenges, we propose\nVerify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and\ncomprehension abilities of LLM agents. VeGraph operates in three phases: (1)\nGraph Representation - an input claim is decomposed into structured triplets,\nforming a graph-based representation that integrates both structured and\nunstructured information; (2) Entity Disambiguation -VeGraph iteratively\ninteracts with the knowledge base to resolve ambiguous entities within the\ngraph for deeper sub-claim verification; and (3) Verification - remaining\ntriplets are verified to complete the fact-checking process. Experiments using\nMeta-Llama-3-70B (instruct version) show that VeGraph achieves competitive\nperformance compared to baselines on two benchmarks HoVer and FEVEROUS,\neffectively addressing claim verification challenges. Our source code and data\nare available for further exploitation.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152864
  },
  {
    "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems",
    "abstract": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
    "text": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152868
  },
  {
    "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM",
    "abstract": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.",
    "text": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation\n  with Lightweight Specialized LLM Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152871
  },
  {
    "title": "ChatPD: An LLM-driven Paper-Dataset Networking System",
    "abstract": "Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.",
    "text": "ChatPD: An LLM-driven Paper-Dataset Networking System Scientific research heavily depends on suitable datasets for method\nvalidation, but existing academic platforms with dataset management like\nPapersWithCode suffer from inefficiencies in their manual workflow. To overcome\nthis bottleneck, we present a system, called ChatPD, that utilizes Large\nLanguage Models (LLMs) to automate dataset information extraction from academic\npapers and construct a structured paper-dataset network. Our system consists of\nthree key modules: \\textit{paper collection}, \\textit{dataset information\nextraction}, and \\textit{dataset entity resolution} to construct paper-dataset\nnetworks. Specifically, we propose a \\textit{Graph Completion and Inference}\nstrategy to map dataset descriptions to their corresponding entities. Through\nextensive experiments, we demonstrate that ChatPD not only outperforms the\nexisting platform PapersWithCode in dataset usage extraction but also achieves\nabout 90\\% precision and recall in entity resolution tasks. Moreover, we have\ndeployed ChatPD to continuously extract which datasets are used in papers, and\nprovide a dataset discovery service, such as task-specific dataset queries and\nsimilar dataset recommendations. We open source ChatPD and the current\npaper-dataset network on this [GitHub\nrepository]{https://github.com/ChatPD-web/ChatPD}.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152874
  },
  {
    "title": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing",
    "abstract": "WiFi sensing has emerged as a compelling contactless modality for human\nactivity monitoring by capturing fine-grained variations in Channel State\nInformation (CSI). Its ability to operate continuously and non-intrusively\nwhile preserving user privacy makes it particularly suitable for health\nmonitoring. However, existing WiFi sensing systems struggle to generalize in\nreal-world settings, largely due to datasets collected in controlled\nenvironments with homogeneous hardware and fragmented, session-based recordings\nthat fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected\nusing commercial WiFi edge devices across 26 diverse indoor environments with\n35 real users. Spanning over 461 hours of effective data, CSI-Bench captures\nrealistic signal variability under natural conditions. It includes\ntask-specific datasets for fall detection, breathing monitoring, localization,\nand motion source recognition, as well as a co-labeled multitask dataset with\njoint annotations for user identity, activity, and proximity. To support the\ndevelopment of robust and generalizable models, CSI-Bench provides standardized\nevaluation splits and baseline results for both single-task and multi-task\nlearning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi\nsensing systems in health and broader human-centric applications.",
    "text": "CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing WiFi sensing has emerged as a compelling contactless modality for human\nactivity monitoring by capturing fine-grained variations in Channel State\nInformation (CSI). Its ability to operate continuously and non-intrusively\nwhile preserving user privacy makes it particularly suitable for health\nmonitoring. However, existing WiFi sensing systems struggle to generalize in\nreal-world settings, largely due to datasets collected in controlled\nenvironments with homogeneous hardware and fragmented, session-based recordings\nthat fail to reflect continuous daily activity.\n  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected\nusing commercial WiFi edge devices across 26 diverse indoor environments with\n35 real users. Spanning over 461 hours of effective data, CSI-Bench captures\nrealistic signal variability under natural conditions. It includes\ntask-specific datasets for fall detection, breathing monitoring, localization,\nand motion source recognition, as well as a co-labeled multitask dataset with\njoint annotations for user identity, activity, and proximity. To support the\ndevelopment of robust and generalizable models, CSI-Bench provides standardized\nevaluation splits and baseline results for both single-task and multi-task\nlearning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi\nsensing systems in health and broader human-centric applications.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152879
  },
  {
    "title": "GXJoin: Generalized Cell Transformations for Explainable Joinability",
    "abstract": "Describing real-world entities can vary across different sources, posing a\nchallenge when integrating or exchanging data. We study the problem of\njoinability under syntactic transformations, where two columns are not\nequi-joinable but can become equi-joinable after some transformations.\nDiscovering those transformations is a challenge because of the large space of\npossible candidates, which grows with the input length and the number of rows.\nOur focus is on the generality of transformations, aiming to make the relevant\nmodels applicable across various instances and domains. We explore a few\ngeneralization techniques, emphasizing those that yield transformations\ncovering a larger number of rows and are often easier to explain. Through\nextensive evaluation on two real-world datasets and employing diverse metrics\nfor measuring the coverage and simplicity of the transformations, our approach\ndemonstrates superior performance over state-of-the-art approaches by\ngenerating fewer, simpler and hence more explainable transformations as well as\nimproving the join performance.",
    "text": "GXJoin: Generalized Cell Transformations for Explainable Joinability Describing real-world entities can vary across different sources, posing a\nchallenge when integrating or exchanging data. We study the problem of\njoinability under syntactic transformations, where two columns are not\nequi-joinable but can become equi-joinable after some transformations.\nDiscovering those transformations is a challenge because of the large space of\npossible candidates, which grows with the input length and the number of rows.\nOur focus is on the generality of transformations, aiming to make the relevant\nmodels applicable across various instances and domains. We explore a few\ngeneralization techniques, emphasizing those that yield transformations\ncovering a larger number of rows and are often easier to explain. Through\nextensive evaluation on two real-world datasets and employing diverse metrics\nfor measuring the coverage and simplicity of the transformations, our approach\ndemonstrates superior performance over state-of-the-art approaches by\ngenerating fewer, simpler and hence more explainable transformations as well as\nimproving the join performance.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528819
  },
  {
    "title": "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data\n  via SQL Queries",
    "abstract": "Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.",
    "text": "Query, Don't Train: Privacy-Preserving Tabular Prediction from EHR Data\n  via SQL Queries Electronic health records (EHRs) contain richly structured, longitudinal data\nessential for predictive modeling, yet stringent privacy regulations (e.g.,\nHIPAA, GDPR) often restrict access to individual-level records. We introduce\nQuery, Don't Train (QDT): a structured-data foundation-model interface enabling\ntabular inference via LLM-generated SQL over EHRs. Instead of training on or\naccessing individual-level examples, QDT uses a large language model (LLM) as a\nschema-aware query planner to generate privacy-compliant SQL queries from a\nnatural language task description and a test-time input. The model then\nextracts summary-level population statistics through these SQL queries and the\nLLM performs, chain-of-thought reasoning over the results to make predictions.\nThis inference-time-only approach (1) eliminates the need for supervised model\ntraining or direct data access, (2) ensures interpretability through symbolic,\nauditable queries, (3) naturally handles missing features without imputation or\npreprocessing, and (4) effectively manages high-dimensional numerical data to\nenhance analytical capabilities. We validate QDT on the task of 30-day hospital\nreadmission prediction for Type 2 diabetes patients using a MIMIC-style EHR\ncohort, achieving F1 = 0.70, which outperforms TabPFN (F1 = 0.68). To our\nknowledge, this is the first demonstration of LLM-driven, privacy-preserving\nstructured prediction using only schema metadata and aggregate statistics -\noffering a scalable, interpretable, and regulation-compliant alternative to\nconventional foundation-model pipelines.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152885
  },
  {
    "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics",
    "abstract": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.",
    "text": "LazyVLM: Neuro-Symbolic Approach to Video Analytics Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528902
  },
  {
    "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models",
    "abstract": "Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.",
    "text": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from\n  Large Language Models Factuality in Large Language Models (LLMs) is a persistent challenge. Current\nbenchmarks often assess short factual answers, overlooking the critical ability\nto generate structured, multi-record tabular outputs from parametric knowledge.\nWe demonstrate that this relational fact retrieval is substantially more\ndifficult than isolated point-wise queries, even when individual facts are\nknown to the model, exposing distinct failure modes sensitive to output\ndimensionality (e.g., number of attributes or records). To systematically\nevaluate this under-explored capability, we introduce RelationalFactQA, a new\nbenchmark featuring diverse natural language questions (paired with SQL) and\ngold-standard tabular answers, specifically designed to assess knowledge\nretrieval in a structured format. RelationalFactQA enables analysis across\nvarying query complexities, output sizes, and data characteristics. Our\nexperiments reveal that even state-of-the-art LLMs struggle significantly, not\nexceeding 25% factual accuracy in generating relational outputs, with\nperformance notably degrading as output dimensionality increases. These\nfindings underscore critical limitations in current LLMs' ability to synthesize\nstructured factual knowledge and establish RelationalFactQA as a crucial\nresource for measuring future progress in LLM factuality.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528928
  },
  {
    "title": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table\n  Union Search Benchmarks",
    "abstract": "Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.",
    "text": "Something's Fishy In The Data Lake: A Critical Re-evaluation of Table\n  Union Search Benchmarks Recent table representation learning and data discovery methods tackle table\nunion search (TUS) within data lakes, which involves identifying tables that\ncan be unioned with a given query table to enrich its content. These methods\nare commonly evaluated using benchmarks that aim to assess semantic\nunderstanding in real-world TUS tasks. However, our analysis of prominent TUS\nbenchmarks reveals several limitations that allow simple baselines to perform\nsurprisingly well, often outperforming more sophisticated approaches. This\nsuggests that current benchmark scores are heavily influenced by\ndataset-specific characteristics and fail to effectively isolate the gains from\nsemantic understanding. To address this, we propose essential criteria for\nfuture benchmarks to enable a more realistic and reliable evaluation of\nprogress in semantic table union search.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1528971
  },
  {
    "title": "Streamlining Knowledge Graph Creation with PyRML",
    "abstract": "Knowledge Graphs (KGs) are increasingly adopted as a foundational technology\nfor integrating heterogeneous data in domains such as climate science, cultural\nheritage, and the life sciences. Declarative mapping languages like R2RML and\nRML have played a central role in enabling scalable and reusable KG\nconstruction, offering a transparent means of transforming structured and\nsemi-structured data into RDF. In this paper, we present PyRML, a lightweight,\nPython-native library for building Knowledge Graphs through declarative\nmappings. PyRML supports core RML constructs and provides a programmable\ninterface for authoring, executing, and testing mappings directly within Python\nenvironments. It integrates with popular data and semantic web libraries (e.g.,\nPandas and RDFlib), enabling transparent and modular workflows. By lowering the\nbarrier to entry for KG creation and fostering reproducible, ontology-aligned\ndata integration, PyRML bridges the gap between declarative semantics and\npractical KG engineering.",
    "text": "Streamlining Knowledge Graph Creation with PyRML Knowledge Graphs (KGs) are increasingly adopted as a foundational technology\nfor integrating heterogeneous data in domains such as climate science, cultural\nheritage, and the life sciences. Declarative mapping languages like R2RML and\nRML have played a central role in enabling scalable and reusable KG\nconstruction, offering a transparent means of transforming structured and\nsemi-structured data into RDF. In this paper, we present PyRML, a lightweight,\nPython-native library for building Knowledge Graphs through declarative\nmappings. PyRML supports core RML constructs and provides a programmable\ninterface for authoring, executing, and testing mappings directly within Python\nenvironments. It integrates with popular data and semantic web libraries (e.g.,\nPandas and RDFlib), enabling transparent and modular workflows. By lowering the\nbarrier to entry for KG creation and fostering reproducible, ontology-aligned\ndata integration, PyRML bridges the gap between declarative semantics and\npractical KG engineering.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529012
  },
  {
    "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering\n  System",
    "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in natural\nlanguage understanding (NLU), opening doors for innovative applications. We\nintroduce StreamLink - an LLM-driven distributed data system designed to\nimprove the efficiency and accessibility of data engineering tasks. We build\nStreamLink on top of distributed frameworks such as Apache Spark and Hadoop to\nhandle large data at scale. One of the important design philosophies of\nStreamLink is to respect user data privacy by utilizing local fine-tuned LLMs\ninstead of a public AI service like ChatGPT. With help from domain-adapted\nLLMs, we can improve our system's understanding of natural language queries\nfrom users in various scenarios and simplify the procedure of generating\ndatabase queries like the Structured Query Language (SQL) for information\nprocessing. We also incorporate LLM-based syntax and security checkers to\nguarantee the reliability and safety of each generated query. StreamLink\nillustrates the potential of merging generative LLMs with distributed data\nprocessing for comprehensive and user-centric data engineering. With this\narchitecture, we allow users to interact with complex database systems at\ndifferent scales in a user-friendly and security-ensured manner, where the SQL\ngeneration reaches over 10\\% of execution accuracy compared to baseline\nmethods, and allow users to find the most concerned item from hundreds of\nmillions of items within a few seconds using natural language.",
    "text": "StreamLink: Large-Language-Model Driven Distributed Data Engineering\n  System Large Language Models (LLMs) have shown remarkable proficiency in natural\nlanguage understanding (NLU), opening doors for innovative applications. We\nintroduce StreamLink - an LLM-driven distributed data system designed to\nimprove the efficiency and accessibility of data engineering tasks. We build\nStreamLink on top of distributed frameworks such as Apache Spark and Hadoop to\nhandle large data at scale. One of the important design philosophies of\nStreamLink is to respect user data privacy by utilizing local fine-tuned LLMs\ninstead of a public AI service like ChatGPT. With help from domain-adapted\nLLMs, we can improve our system's understanding of natural language queries\nfrom users in various scenarios and simplify the procedure of generating\ndatabase queries like the Structured Query Language (SQL) for information\nprocessing. We also incorporate LLM-based syntax and security checkers to\nguarantee the reliability and safety of each generated query. StreamLink\nillustrates the potential of merging generative LLMs with distributed data\nprocessing for comprehensive and user-centric data engineering. With this\narchitecture, we allow users to interact with complex database systems at\ndifferent scales in a user-friendly and security-ensured manner, where the SQL\ngeneration reaches over 10\\% of execution accuracy compared to baseline\nmethods, and allow users to find the most concerned item from hundreds of\nmillions of items within a few seconds using natural language.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152904
  },
  {
    "title": "In-memory Incremental Maintenance of Provenance Sketches [extended\n  version]",
    "abstract": "Provenance-based data skipping compactly over-approximates the provenance of\na query using so-called provenance sketches and utilizes such sketches to\nspeed-up the execution of subsequent queries by skipping irrelevant data.\nHowever, a sketch captured at some time in the past may become stale if the\ndata has been updated subsequently. Thus, there is a need to maintain\nprovenance sketches. In this work, we introduce In-Memory incremental\nMaintenance of Provenance sketches (IMP), a framework for maintaining sketches\nincrementally under updates. At the core of IMP is an incremental query engine\nfor data annotated with sketches that exploits the coarse-grained nature of\nsketches to enable novel optimizations. We experimentally demonstrate that IMP\nsignificantly reduces the cost of sketch maintenance, thereby enabling the use\nof provenance sketches for a broad range of workloads that involve updates.",
    "text": "In-memory Incremental Maintenance of Provenance Sketches [extended\n  version] Provenance-based data skipping compactly over-approximates the provenance of\na query using so-called provenance sketches and utilizes such sketches to\nspeed-up the execution of subsequent queries by skipping irrelevant data.\nHowever, a sketch captured at some time in the past may become stale if the\ndata has been updated subsequently. Thus, there is a need to maintain\nprovenance sketches. In this work, we introduce In-Memory incremental\nMaintenance of Provenance sketches (IMP), a framework for maintaining sketches\nincrementally under updates. At the core of IMP is an incremental query engine\nfor data annotated with sketches that exploits the coarse-grained nature of\nsketches to enable novel optimizations. We experimentally demonstrate that IMP\nsignificantly reduces the cost of sketch maintenance, thereby enabling the use\nof provenance sketches for a broad range of workloads that involve updates.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152909
  },
  {
    "title": "Effectiveness of Prompt Optimization in NL2SQL Systems",
    "abstract": "NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework.",
    "text": "Effectiveness of Prompt Optimization in NL2SQL Systems NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529129
  },
  {
    "title": "Probabilistic Kernel Function for Fast Angle Testing",
    "abstract": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW.",
    "text": "Probabilistic Kernel Function for Fast Angle Testing In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152917
  },
  {
    "title": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from\n  Application Code",
    "abstract": "In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the \\uschema{} unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the \\uschema{} logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process.",
    "text": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from\n  Application Code In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the \\uschema{} unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the \\uschema{} logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15292
  },
  {
    "title": "Automatic Metadata Extraction for Text-to-SQL",
    "abstract": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
    "text": "Automatic Metadata Extraction for Text-to-SQL Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152925
  },
  {
    "title": "A Unified Architecture for Efficient Binary and Worst-Case Optimal Join\n  Processing",
    "abstract": "Join processing is a fundamental operation in database management systems;\nhowever, traditional join algorithms often encounter efficiency challenges when\ndealing with complex queries that produce intermediate results much larger than\nthe final query output. The emergence of worst-case optimal join (WCOJ)\nalgorithms represents a significant advancement, offering asymptotically better\nperformance by avoiding the enumeration of potentially exploding intermediate\nresults. In this paper, we propose a unified architecture that efficiently\nsupports both traditional binary joins and WCOJ processing. As opposed to the\nstate-of-the-art, which only focuses on either hash-based or sort-based join\nimplementations, our system accommodates both physical implementations of\nbinary joins and WCOJ algorithms. Experimental evaluations demonstrate that our\nsystem achieves performance gains of up to 3.1x (on average 1.5x) and 4.8x (on\naverage 1.4x) over the state-of-the-art implementation of Generic Join and Free\nJoin methods, respectively, across acyclic and cyclic queries in standard query\nbenchmarks.",
    "text": "A Unified Architecture for Efficient Binary and Worst-Case Optimal Join\n  Processing Join processing is a fundamental operation in database management systems;\nhowever, traditional join algorithms often encounter efficiency challenges when\ndealing with complex queries that produce intermediate results much larger than\nthe final query output. The emergence of worst-case optimal join (WCOJ)\nalgorithms represents a significant advancement, offering asymptotically better\nperformance by avoiding the enumeration of potentially exploding intermediate\nresults. In this paper, we propose a unified architecture that efficiently\nsupports both traditional binary joins and WCOJ processing. As opposed to the\nstate-of-the-art, which only focuses on either hash-based or sort-based join\nimplementations, our system accommodates both physical implementations of\nbinary joins and WCOJ algorithms. Experimental evaluations demonstrate that our\nsystem achieves performance gains of up to 3.1x (on average 1.5x) and 4.8x (on\naverage 1.4x) over the state-of-the-art implementation of Generic Join and Free\nJoin methods, respectively, across acyclic and cyclic queries in standard query\nbenchmarks.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152929
  },
  {
    "title": "Adaptive Indexing for Approximate Query Processing in Exploratory Data\n  Analysis",
    "abstract": "Minimizing data-to-analysis time while enabling real-time interaction and\nefficient analytical computations on large datasets are fundamental objectives\nof contemporary exploratory systems. Although some of the recent adaptive\nindexing and on-the-fly processing approaches address most of these needs,\nthere are cases, where they do not always guarantee reliable performance. Some\nexamples of such cases include: exploring areas with a high density of objects;\nexecuting the first exploratory queries or exploring previously unseen areas\n(where the index has not yet adapted sufficiently); and working with very large\ndata files on commodity hardware, such as low-specification laptops. In such\ndemanding cases, approximate and incremental techniques can be exploited to\nensure efficiency and scalability by allowing users to prioritize response time\nover result accuracy, acknowledging that exact results are not always\nnecessary. Therefore, approximation mechanisms that enable smooth user\ninteraction by defining the trade-off between accuracy and performance based on\nvital factors (e.g., task, preferences, available resources) are of great\nimportance. Considering the aforementioned, in this work, we present an\nadaptive approximate query processing framework for interactive on-the-fly\nanalysis (with out a preprocessing phase) over large raw data. The core\ncomponent of the framework is a main-memory adaptive indexing scheme\n(VALINOR-A) that interoperates with user-driven sampling and incremental\naggregation computations. Additionally, an effective error-bounded\napproximation strategy is designed and integrated in the query processing\nprocess. We conduct extensive experiments using both real and synthetic\ndatasets, demonstrating the efficiency and effectiveness of the proposed\nframework.",
    "text": "Adaptive Indexing for Approximate Query Processing in Exploratory Data\n  Analysis Minimizing data-to-analysis time while enabling real-time interaction and\nefficient analytical computations on large datasets are fundamental objectives\nof contemporary exploratory systems. Although some of the recent adaptive\nindexing and on-the-fly processing approaches address most of these needs,\nthere are cases, where they do not always guarantee reliable performance. Some\nexamples of such cases include: exploring areas with a high density of objects;\nexecuting the first exploratory queries or exploring previously unseen areas\n(where the index has not yet adapted sufficiently); and working with very large\ndata files on commodity hardware, such as low-specification laptops. In such\ndemanding cases, approximate and incremental techniques can be exploited to\nensure efficiency and scalability by allowing users to prioritize response time\nover result accuracy, acknowledging that exact results are not always\nnecessary. Therefore, approximation mechanisms that enable smooth user\ninteraction by defining the trade-off between accuracy and performance based on\nvital factors (e.g., task, preferences, available resources) are of great\nimportance. Considering the aforementioned, in this work, we present an\nadaptive approximate query processing framework for interactive on-the-fly\nanalysis (with out a preprocessing phase) over large raw data. The core\ncomponent of the framework is a main-memory adaptive indexing scheme\n(VALINOR-A) that interoperates with user-driven sampling and incremental\naggregation computations. Additionally, an effective error-bounded\napproximation strategy is designed and integrated in the query processing\nprocess. We conduct extensive experiments using both real and synthetic\ndatasets, demonstrating the efficiency and effectiveness of the proposed\nframework.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529322
  },
  {
    "title": "Foundation Models for Tabular Data within Systemic Contexts Need\n  Grounding",
    "abstract": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.",
    "text": "Foundation Models for Tabular Data within Systemic Contexts Need\n  Grounding Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152937
  },
  {
    "title": "Curation and Analysis of MIMICEL -- An Event Log for MIMIC-IV Emergency\n  Department",
    "abstract": "The global issue of overcrowding in emergency departments (ED) necessitates\nthe analysis of patient flow through ED to enhance efficiency and alleviate\novercrowding. However, traditional analytical methods are time-consuming and\ncostly. The healthcare industry is embracing process mining tools to analyse\nhealthcare processes and patient flows. Process mining aims to discover,\nmonitor, and enhance processes by obtaining knowledge from event log data.\nHowever, the availability of event logs is a prerequisite for applying process\nmining techniques. Hence, this paper aims to generate an event log for\nanalysing processes in ED. In this study, we extract an event log from the\nMIMIC-IV-ED dataset and name it MIMICEL. MIMICEL captures the process of\npatient journey in ED, allowing for analysis of patient flows and improving ED\nefficiency. We present analyses conducted using MIMICEL to demonstrate the\nutility of the dataset. The curation of MIMICEL facilitates extensive use of\nMIMIC-IV-ED data for ED analysis using process mining techniques, while also\nproviding the process mining research communities with a valuable dataset for\nstudy.",
    "text": "Curation and Analysis of MIMICEL -- An Event Log for MIMIC-IV Emergency\n  Department The global issue of overcrowding in emergency departments (ED) necessitates\nthe analysis of patient flow through ED to enhance efficiency and alleviate\novercrowding. However, traditional analytical methods are time-consuming and\ncostly. The healthcare industry is embracing process mining tools to analyse\nhealthcare processes and patient flows. Process mining aims to discover,\nmonitor, and enhance processes by obtaining knowledge from event log data.\nHowever, the availability of event logs is a prerequisite for applying process\nmining techniques. Hence, this paper aims to generate an event log for\nanalysing processes in ED. In this study, we extract an event log from the\nMIMIC-IV-ED dataset and name it MIMICEL. MIMICEL captures the process of\npatient journey in ED, allowing for analysis of patient flows and improving ED\nefficiency. We present analyses conducted using MIMICEL to demonstrate the\nutility of the dataset. The curation of MIMICEL facilitates extensive use of\nMIMIC-IV-ED data for ED analysis using process mining techniques, while also\nproviding the process mining research communities with a valuable dataset for\nstudy.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15294
  },
  {
    "title": "ODIN: A NL2SQL Recommender to Handle Schema Ambiguity",
    "abstract": "NL2SQL (natural language to SQL) systems translate natural language into SQL\nqueries, allowing users with no technical background to interact with databases\nand create tools like reports or visualizations. While recent advancements in\nlarge language models (LLMs) have significantly improved NL2SQL accuracy,\nschema ambiguity remains a major challenge in enterprise environments with\ncomplex schemas, where multiple tables and columns with semantically similar\nnames often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL\nrecommendation engine. Instead of producing a single SQL query given a natural\nlanguage question, ODIN generates a set of potential SQL queries by accounting\nfor different interpretations of ambiguous schema components. ODIN dynamically\nadjusts the number of suggestions based on the level of ambiguity, and ODIN\nlearns from user feedback to personalize future SQL query recommendations. Our\nevaluation shows that ODIN improves the likelihood of generating the correct\nSQL query by 1.5-2$\\times$ compared to baselines.",
    "text": "ODIN: A NL2SQL Recommender to Handle Schema Ambiguity NL2SQL (natural language to SQL) systems translate natural language into SQL\nqueries, allowing users with no technical background to interact with databases\nand create tools like reports or visualizations. While recent advancements in\nlarge language models (LLMs) have significantly improved NL2SQL accuracy,\nschema ambiguity remains a major challenge in enterprise environments with\ncomplex schemas, where multiple tables and columns with semantically similar\nnames often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL\nrecommendation engine. Instead of producing a single SQL query given a natural\nlanguage question, ODIN generates a set of potential SQL queries by accounting\nfor different interpretations of ambiguous schema components. ODIN dynamically\nadjusts the number of suggestions based on the level of ambiguity, and ODIN\nlearns from user feedback to personalize future SQL query recommendations. Our\nevaluation shows that ODIN improves the likelihood of generating the correct\nSQL query by 1.5-2$\\times$ compared to baselines.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529431
  },
  {
    "title": "POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval",
    "abstract": "Although Multi-Vector Retrieval (MVR) has achieved the state of the art on\nmany information retrieval (IR) tasks, its performance highly depends on how to\ndecompose queries into smaller pieces, say phrases or tokens. However,\noptimizing query decomposition for MVR performance is not end-to-end\ndifferentiable. Even worse, jointly solving this problem and training the\ndownstream retrieval-based systems, say RAG systems could be highly\ninefficient. To overcome these challenges, we propose Performance-Oriented\nQuery Decomposer (POQD), a novel query decomposition framework for MVR. POQD\nleverages one LLM for query decomposition and searches the optimal prompt with\nan LLM-based optimizer. We further propose an end-to-end training algorithm to\nalternatively optimize the prompt for query decomposition and the downstream\nmodels. This algorithm can achieve superior MVR performance at a reasonable\ntraining cost as our theoretical analysis suggests. POQD can be integrated\nseamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented\nGeneration (RAG) systems. Extensive empirical studies on representative\nRAG-based QA tasks show that POQD outperforms existing query decomposition\nstrategies in both retrieval performance and end-to-end QA accuracy. POQD is\navailable at https://github.com/PKU-SDS-lab/POQD-ICML25.",
    "text": "POQD: Performance-Oriented Query Decomposer for Multi-vector retrieval Although Multi-Vector Retrieval (MVR) has achieved the state of the art on\nmany information retrieval (IR) tasks, its performance highly depends on how to\ndecompose queries into smaller pieces, say phrases or tokens. However,\noptimizing query decomposition for MVR performance is not end-to-end\ndifferentiable. Even worse, jointly solving this problem and training the\ndownstream retrieval-based systems, say RAG systems could be highly\ninefficient. To overcome these challenges, we propose Performance-Oriented\nQuery Decomposer (POQD), a novel query decomposition framework for MVR. POQD\nleverages one LLM for query decomposition and searches the optimal prompt with\nan LLM-based optimizer. We further propose an end-to-end training algorithm to\nalternatively optimize the prompt for query decomposition and the downstream\nmodels. This algorithm can achieve superior MVR performance at a reasonable\ntraining cost as our theoretical analysis suggests. POQD can be integrated\nseamlessly into arbitrary retrieval-based systems such as Retrieval-Augmented\nGeneration (RAG) systems. Extensive empirical studies on representative\nRAG-based QA tasks show that POQD outperforms existing query decomposition\nstrategies in both retrieval performance and end-to-end QA accuracy. POQD is\navailable at https://github.com/PKU-SDS-lab/POQD-ICML25.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152946
  },
  {
    "title": "SQUiD: Synthesizing Relational Databases from Unstructured Text",
    "abstract": "Relational databases are central to modern data management, yet most data\nexists in unstructured forms like text documents. To bridge this gap, we\nleverage large language models (LLMs) to automatically synthesize a relational\ndatabase by generating its schema and populating its tables from raw text. We\nintroduce SQUiD, a novel neurosymbolic framework that decomposes this task into\nfour stages, each with specialized techniques. Our experiments show that SQUiD\nconsistently outperforms baselines across diverse datasets.",
    "text": "SQUiD: Synthesizing Relational Databases from Unstructured Text Relational databases are central to modern data management, yet most data\nexists in unstructured forms like text documents. To bridge this gap, we\nleverage large language models (LLMs) to automatically synthesize a relational\ndatabase by generating its schema and populating its tables from raw text. We\nintroduce SQUiD, a novel neurosymbolic framework that decomposes this task into\nfour stages, each with specialized techniques. Our experiments show that SQUiD\nconsistently outperforms baselines across diverse datasets.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.15295
  },
  {
    "title": "DARTH: Declarative Recall Through Early Termination for Approximate\n  Nearest Neighbor Search",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) presents an inherent tradeoff\nbetween performance and recall (i.e., result quality). Each ANNS algorithm\nprovides its own algorithm-dependent parameters to allow applications to\ninfluence the recall/performance tradeoff of their searches. This situation is\ndoubly problematic. First, the application developers have to experiment with\nthese algorithm-dependent parameters to fine-tune the parameters that produce\nthe desired recall for each use case. This process usually takes a lot of\neffort. Even worse, the chosen parameters may produce good recall for some\nqueries, but bad recall for hard queries. To solve these problems, we present\nDARTH, a method that uses target declarative recall. DARTH uses a novel method\nfor providing target declarative recall on top of an ANNS index by employing an\nadaptive early termination strategy integrated into the search algorithm.\nThrough a wide range of experiments, we demonstrate that DARTH effectively\nmeets user-defined recall targets while achieving significant speedups, up to\n14.6x (average: 6.8x; median: 5.7x) faster than the search without early\ntermination for HNSW and up to 41.8x (average: 13.6x; median: 8.1x) for IVF.\nThis paper appeared in ACM SIGMOD 2026.",
    "text": "DARTH: Declarative Recall Through Early Termination for Approximate\n  Nearest Neighbor Search Approximate Nearest Neighbor Search (ANNS) presents an inherent tradeoff\nbetween performance and recall (i.e., result quality). Each ANNS algorithm\nprovides its own algorithm-dependent parameters to allow applications to\ninfluence the recall/performance tradeoff of their searches. This situation is\ndoubly problematic. First, the application developers have to experiment with\nthese algorithm-dependent parameters to fine-tune the parameters that produce\nthe desired recall for each use case. This process usually takes a lot of\neffort. Even worse, the chosen parameters may produce good recall for some\nqueries, but bad recall for hard queries. To solve these problems, we present\nDARTH, a method that uses target declarative recall. DARTH uses a novel method\nfor providing target declarative recall on top of an ANNS index by employing an\nadaptive early termination strategy integrated into the search algorithm.\nThrough a wide range of experiments, we demonstrate that DARTH effectively\nmeets user-defined recall targets while achieving significant speedups, up to\n14.6x (average: 6.8x; median: 5.7x) faster than the search without early\ntermination for HNSW and up to 41.8x (average: 13.6x; median: 8.1x) for IVF.\nThis paper appeared in ACM SIGMOD 2026.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529548
  },
  {
    "title": "Anonymity-washing",
    "abstract": "Anonymization is a foundational principle of data privacy regulation, yet its\npractical application remains riddled with ambiguity and inconsistency. This\npaper introduces the concept of anonymity-washing -- the misrepresentation of\nthe anonymity level of ``sanitized'' personal data -- as a critical privacy\nconcern. While both legal and technical critiques of anonymization exist, they\ntend to address isolated aspects of the problem. In contrast, this paper offers\na comprehensive overview of the conditions that enable anonymity-washing. It\nsynthesizes fragmented legal interpretations, technical misunderstandings, and\noutdated regulatory guidance and complements them with a systematic review of\nnational and international resources, including legal cases, data protection\nauthority guidelines, and technical documentation. Our findings reveal a lack\nof coherent support for practitioners, contributing to the persistent misuse of\npseudonymization and obsolete anonymization techniques. We conclude by\nrecommending targeted education, clearer technical guidance, and closer\ncooperation between regulators, researchers, and industry to bridge the gap\nbetween legal norms and technical reality.",
    "text": "Anonymity-washing Anonymization is a foundational principle of data privacy regulation, yet its\npractical application remains riddled with ambiguity and inconsistency. This\npaper introduces the concept of anonymity-washing -- the misrepresentation of\nthe anonymity level of ``sanitized'' personal data -- as a critical privacy\nconcern. While both legal and technical critiques of anonymization exist, they\ntend to address isolated aspects of the problem. In contrast, this paper offers\na comprehensive overview of the conditions that enable anonymity-washing. It\nsynthesizes fragmented legal interpretations, technical misunderstandings, and\noutdated regulatory guidance and complements them with a systematic review of\nnational and international resources, including legal cases, data protection\nauthority guidelines, and technical documentation. Our findings reveal a lack\nof coherent support for practitioners, contributing to the persistent misuse of\npseudonymization and obsolete anonymization techniques. We conclude by\nrecommending targeted education, clearer technical guidance, and closer\ncooperation between regulators, researchers, and industry to bridge the gap\nbetween legal norms and technical reality.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152958
  },
  {
    "title": "A Survey of LLM $\\times$ DATA",
    "abstract": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
    "text": "A Survey of LLM $\\times$ DATA The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529632
  },
  {
    "title": "On the Complexity of Checking Mixed Isolation Levels for SQL\n  Transactions",
    "abstract": "Concurrent accesses to databases are typically grouped in transactions which\ndefine units of work that should be isolated from other concurrent computations\nand resilient to failures. Modern databases provide different levels of\nisolation for transactions that correspond to different trade-offs between\nconsistency and throughput. Quite often, an application can use transactions\nwith different isolation levels at the same time. In this work, we investigate\nthe problem of testing isolation level implementations in databases, i.e.,\nchecking whether a given execution composed of multiple transactions adheres to\nthe prescribed isolation level semantics. We particularly focus on transactions\nformed of SQL queries and the use of multiple isolation levels at the same\ntime. We show that many restrictions of this problem are NP-complete and\nprovide an algorithm which is exponential-time in the worst-case,\npolynomial-time in relevant cases, and practically efficient.",
    "text": "On the Complexity of Checking Mixed Isolation Levels for SQL\n  Transactions Concurrent accesses to databases are typically grouped in transactions which\ndefine units of work that should be isolated from other concurrent computations\nand resilient to failures. Modern databases provide different levels of\nisolation for transactions that correspond to different trade-offs between\nconsistency and throughput. Quite often, an application can use transactions\nwith different isolation levels at the same time. In this work, we investigate\nthe problem of testing isolation level implementations in databases, i.e.,\nchecking whether a given execution composed of multiple transactions adheres to\nthe prescribed isolation level semantics. We particularly focus on transactions\nformed of SQL queries and the use of multiple isolation levels at the same\ntime. We show that many restrictions of this problem are NP-complete and\nprovide an algorithm which is exponential-time in the worst-case,\npolynomial-time in relevant cases, and practically efficient.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152968
  },
  {
    "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph\n  Algorithms for Text-to-SQL on Large-Scale Databases",
    "abstract": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes.",
    "text": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph\n  Algorithms for Text-to-SQL on Large-Scale Databases Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152971
  },
  {
    "title": "Persona Alchemy: Designing, Evaluating, and Implementing\n  Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation",
    "abstract": "Despite advances in designing personas for Large Language Models (LLM),\nchallenges remain in aligning them with human cognitive processes and\nrepresenting diverse stakeholder perspectives. We introduce a Social Cognitive\nTheory (SCT) agent design framework for designing, evaluating, and implementing\npsychologically grounded LLMs with consistent behavior. Our framework\noperationalizes SCT through four personal factors (cognitive, motivational,\nbiological, and affective) for designing, six quantifiable constructs for\nevaluating, and a graph database-backed architecture for implementing\nstakeholder personas. Experiments tested agents' responses to contradicting\ninformation of varying reliability. In the highly polarized renewable energy\ntransition discourse, we design five diverse agents with distinct ideologies,\nroles, and stakes to examine stakeholder representation. The evaluation of\nthese agents in contradictory scenarios occurs through comprehensive processes\nthat implement the SCT. Results show consistent response patterns ($R^2$ range:\n$0.58-0.61$) and systematic temporal development of SCT construct effects.\nPrincipal component analysis identifies two dimensions explaining $73$% of\nvariance, validating the theoretical structure. Our framework offers improved\nexplainability and reproducibility compared to black-box approaches. This work\ncontributes to ongoing efforts to improve diverse stakeholder representation\nwhile maintaining psychological consistency in LLM personas.",
    "text": "Persona Alchemy: Designing, Evaluating, and Implementing\n  Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation Despite advances in designing personas for Large Language Models (LLM),\nchallenges remain in aligning them with human cognitive processes and\nrepresenting diverse stakeholder perspectives. We introduce a Social Cognitive\nTheory (SCT) agent design framework for designing, evaluating, and implementing\npsychologically grounded LLMs with consistent behavior. Our framework\noperationalizes SCT through four personal factors (cognitive, motivational,\nbiological, and affective) for designing, six quantifiable constructs for\nevaluating, and a graph database-backed architecture for implementing\nstakeholder personas. Experiments tested agents' responses to contradicting\ninformation of varying reliability. In the highly polarized renewable energy\ntransition discourse, we design five diverse agents with distinct ideologies,\nroles, and stakes to examine stakeholder representation. The evaluation of\nthese agents in contradictory scenarios occurs through comprehensive processes\nthat implement the SCT. Results show consistent response patterns ($R^2$ range:\n$0.58-0.61$) and systematic temporal development of SCT construct effects.\nPrincipal component analysis identifies two dimensions explaining $73$% of\nvariance, validating the theoretical structure. Our framework offers improved\nexplainability and reproducibility compared to black-box approaches. This work\ncontributes to ongoing efforts to improve diverse stakeholder representation\nwhile maintaining psychological consistency in LLM personas.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.152975
  },
  {
    "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
    "abstract": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.",
    "text": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529799
  },
  {
    "title": "Improved Algorithms for Overlapping and Robust Clustering of\n  Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
    "abstract": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature.",
    "text": "Improved Algorithms for Overlapping and Robust Clustering of\n  Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529841
  },
  {
    "title": "Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark\n  based on the rdf2pg Framework and Plant Biology Data",
    "abstract": "Linked Data and labelled property graphs (LPG) are two data management\napproaches with complementary strengths and weaknesses, making their\nintegration beneficial for sharing datasets and supporting software ecosystems.\nIn this paper, we introduce rdf2pg, an extensible framework for mapping RDF\ndata to semantically equivalent LPG formats and data-bases. Utilising this\nframework, we perform a comparative analysis of three popular graph databases -\nVirtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages\nSPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments\nunderline the strengths and limitations of these graph database technologies.\nAdditionally, we highlight the potential of rdf2pg as a versatile tool for\nenabling polyglot access to knowledge graphs, aligning with established\nstandards of Linked Data and the Semantic Web.",
    "text": "Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark\n  based on the rdf2pg Framework and Plant Biology Data Linked Data and labelled property graphs (LPG) are two data management\napproaches with complementary strengths and weaknesses, making their\nintegration beneficial for sharing datasets and supporting software ecosystems.\nIn this paper, we introduce rdf2pg, an extensible framework for mapping RDF\ndata to semantically equivalent LPG formats and data-bases. Utilising this\nframework, we perform a comparative analysis of three popular graph databases -\nVirtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages\nSPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments\nunderline the strengths and limitations of these graph database technologies.\nAdditionally, we highlight the potential of rdf2pg as a versatile tool for\nenabling polyglot access to knowledge graphs, aligning with established\nstandards of Linked Data and the Semantic Web.",
    "url": "",
    "category": "cs.DB",
    "source": "arxiv",
    "timestamp": 1750404542.1529868
  },
  {
    "title": "PSM: Policy Synchronised Deterministic Memory",
    "abstract": "Concurrency and determinacy do not go well with each other when resources\nmust be shared. Haskell provides parallel programming abstractions such as IVar\nand LVar in the Par monad and concurrent abstractions such as MVar and TVar in\nthe in IO and STM monads, respectively. The former are determinate but have no\ndestructive updates and the latter have destructive updates but do not\nguarantee determinacy. Programming patterns that are both concurrent and\ndeterminate, such as those provided by Kahn or Berry require memory\nabstractions at a higher level than is currently available. In this paper we\ndescribe a new type context PSM for policy synchronised memory in Haskell. Like\nSTM and IO, the computations in PSM can access persistent state and, as a\nside-effect, update the memory in imperative style. Like the Par and IO monads,\nPSM supports concurrent threads and shared state. However, in contrast to IO,\nour PSM contexts are race-free since concurrent accesses are policy coordinated\nwhich guarantees determinacy.Well-typed transactions in the PSM context can\naccommodate abstract data structures that are imperative, concurrently\nshareable and still behave deterministically, by construction.",
    "text": "PSM: Policy Synchronised Deterministic Memory Concurrency and determinacy do not go well with each other when resources\nmust be shared. Haskell provides parallel programming abstractions such as IVar\nand LVar in the Par monad and concurrent abstractions such as MVar and TVar in\nthe in IO and STM monads, respectively. The former are determinate but have no\ndestructive updates and the latter have destructive updates but do not\nguarantee determinacy. Programming patterns that are both concurrent and\ndeterminate, such as those provided by Kahn or Berry require memory\nabstractions at a higher level than is currently available. In this paper we\ndescribe a new type context PSM for policy synchronised memory in Haskell. Like\nSTM and IO, the computations in PSM can access persistent state and, as a\nside-effect, update the memory in imperative style. Like the Par and IO monads,\nPSM supports concurrent threads and shared state. However, in contrast to IO,\nour PSM contexts are race-free since concurrent accesses are policy coordinated\nwhich guarantees determinacy.Well-typed transactions in the PSM context can\naccommodate abstract data structures that are imperative, concurrently\nshareable and still behave deterministically, by construction.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732896
  },
  {
    "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
    "abstract": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
    "text": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7329211
  },
  {
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "abstract": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "text": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732936
  },
  {
    "title": "Optimized Execution of FreeCHR",
    "abstract": "Constraint Handling Rules (CHR) is a rule-based programming language that\nrewrites collections of constraints. It is typically embedded into a\ngeneral-purpose language. There exists a plethora of implementations for\nnumerous host languages. However, the existing implementations often re-invent\nthe method of embedding, which impedes maintenance and weakens assertions of\ncorrectness. To formalize and thereby unify the embedding of a ground subset of\nCHR into arbitrary host languages, we introduced the framework FreeCHR and\nproved it to be a valid representation of classical CHR. For the sake of\nsimplicity, abstract implementations of our framework did not yet include a\nconcrete matching algorithm nor optimizations. In this paper, we introduce an\nimproved execution algorithm for FreeCHR. We also provide an evaluation of the\nalgorithm via benchmarks which suggest the effectiveness of our implemented\noptimizations.",
    "text": "Optimized Execution of FreeCHR Constraint Handling Rules (CHR) is a rule-based programming language that\nrewrites collections of constraints. It is typically embedded into a\ngeneral-purpose language. There exists a plethora of implementations for\nnumerous host languages. However, the existing implementations often re-invent\nthe method of embedding, which impedes maintenance and weakens assertions of\ncorrectness. To formalize and thereby unify the embedding of a ground subset of\nCHR into arbitrary host languages, we introduced the framework FreeCHR and\nproved it to be a valid representation of classical CHR. For the sake of\nsimplicity, abstract implementations of our framework did not yet include a\nconcrete matching algorithm nor optimizations. In this paper, we introduce an\nimproved execution algorithm for FreeCHR. We also provide an evaluation of the\nalgorithm via benchmarks which suggest the effectiveness of our implemented\noptimizations.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732948
  },
  {
    "title": "Positive Sharing and Abstract Machines",
    "abstract": "Wu's positive $\\lambda$-calculus is a recent call-by-value $\\lambda$-calculus\nwith sharing coming from Miller and Wu's study of the proof-theoretical concept\nof focalization. Accattoli and Wu showed that it simplifies a technical aspect\nof the study of sharing; namely it rules out the recurrent issue of renaming\nchains, that often causes a quadratic time slowdown.\n  In this paper, we define the natural abstract machine for the positive\n$\\lambda$-calculus and show that it suffers from an inefficiency: the quadratic\nslowdown somehow reappears when analyzing the cost of the machine. We then\ndesign an optimized machine for the positive $\\lambda$-calculus, which we prove\nefficient. The optimization is based on a new slicing technique which is dual\nto the standard structure of machine environments.",
    "text": "Positive Sharing and Abstract Machines Wu's positive $\\lambda$-calculus is a recent call-by-value $\\lambda$-calculus\nwith sharing coming from Miller and Wu's study of the proof-theoretical concept\nof focalization. Accattoli and Wu showed that it simplifies a technical aspect\nof the study of sharing; namely it rules out the recurrent issue of renaming\nchains, that often causes a quadratic time slowdown.\n  In this paper, we define the natural abstract machine for the positive\n$\\lambda$-calculus and show that it suffers from an inefficiency: the quadratic\nslowdown somehow reappears when analyzing the cost of the machine. We then\ndesign an optimized machine for the positive $\\lambda$-calculus, which we prove\nefficient. The optimization is based on a new slicing technique which is dual\nto the standard structure of machine environments.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73296
  },
  {
    "title": "Towards Bug-Free Distributed Go Programs",
    "abstract": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.",
    "text": "Towards Bug-Free Distributed Go Programs Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732974
  },
  {
    "title": "StacKAT: Infinite State Network Verification",
    "abstract": "We develop StacKAT, a network verification language featuring loops, finite\nstate variables, nondeterminism, and - most importantly - access to a stack\nwith accompanying push and pop operations. By viewing the variables and stack\nas the (parsed) headers and (to-be-parsed) contents of a network packet,\nStacKAT can express a wide range of network behaviors including parsing, source\nrouting, and telemetry. These behaviors are difficult or impossible to model\nusing existing languages like NetKAT. We develop a decision procedure for\nStacKAT program equivalence, based on finite automata. This decision procedure\nprovides the theoretical basis for verifying network-wide properties and is\nable to provide counterexamples for inequivalent programs. Finally, we provide\nan axiomatization of StacKAT equivalence and establish its completeness.",
    "text": "StacKAT: Infinite State Network Verification We develop StacKAT, a network verification language featuring loops, finite\nstate variables, nondeterminism, and - most importantly - access to a stack\nwith accompanying push and pop operations. By viewing the variables and stack\nas the (parsed) headers and (to-be-parsed) contents of a network packet,\nStacKAT can express a wide range of network behaviors including parsing, source\nrouting, and telemetry. These behaviors are difficult or impossible to model\nusing existing languages like NetKAT. We develop a decision procedure for\nStacKAT program equivalence, based on finite automata. This decision procedure\nprovides the theoretical basis for verifying network-wide properties and is\nable to provide counterexamples for inequivalent programs. Finally, we provide\nan axiomatization of StacKAT equivalence and establish its completeness.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732985
  },
  {
    "title": "Freer Arrows and Why You Need Them in Haskell",
    "abstract": "Freer monads are a useful structure commonly used in various domains due to\ntheir expressiveness. However, a known issue with freer monads is that they are\nnot amenable to static analysis. This paper explores freer arrows, a relatively\nexpressive structure that is amenable to static analysis. We propose several\nvariants of freer arrows. We conduct a case study on choreographic programming\nto demonstrate the usefulness of freer arrows in Haskell.",
    "text": "Freer Arrows and Why You Need Them in Haskell Freer monads are a useful structure commonly used in various domains due to\ntheir expressiveness. However, a known issue with freer monads is that they are\nnot amenable to static analysis. This paper explores freer arrows, a relatively\nexpressive structure that is amenable to static analysis. We propose several\nvariants of freer arrows. We conduct a case study on choreographic programming\nto demonstrate the usefulness of freer arrows in Haskell.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.732994
  },
  {
    "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with\n  Code Actions",
    "abstract": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level.",
    "text": "A Fast, Reliable, and Secure Programming Language for LLM Agents with\n  Code Actions Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7330072
  },
  {
    "title": "ALEA IACTA EST: A Declarative Domain-Specific Language for Manually\n  Performable Random Experiments",
    "abstract": "Random experiments that are simple and clear enough to be performed by human\nagents feature prominently in the teaching of elementary stochastics as well as\nin games. We present Alea, a domain-specific language for the specification of\nrandom experiments. Alea code can either be analyzed statically to obtain and\ninspect probability distributions of outcomes, or be executed with a source\npseudo-randomness for simulation or as a game assistant. The language is\nintended for ease of use by non-expert programmers, in particular students of\nelementary stochastics, and players and designers of games of chance, by\nfocusing on concepts common to functional programming and basic mathematics.\nBoth the design of the language and the implementation of runtime environments\nare work in progress.",
    "text": "ALEA IACTA EST: A Declarative Domain-Specific Language for Manually\n  Performable Random Experiments Random experiments that are simple and clear enough to be performed by human\nagents feature prominently in the teaching of elementary stochastics as well as\nin games. We present Alea, a domain-specific language for the specification of\nrandom experiments. Alea code can either be analyzed statically to obtain and\ninspect probability distributions of outcomes, or be executed with a source\npseudo-randomness for simulation or as a game assistant. The language is\nintended for ease of use by non-expert programmers, in particular students of\nelementary stochastics, and players and designers of games of chance, by\nfocusing on concepts common to functional programming and basic mathematics.\nBoth the design of the language and the implementation of runtime environments\nare work in progress.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7330172
  },
  {
    "title": "PermRust: A Token-based Permission System for Rust",
    "abstract": "Permission systems which restrict access to system resources are a\nwell-established technology in operating systems, especially for smartphones.\nHowever, as such systems are implemented in the operating system they can at\nmost manage access on the process-level. Since moderns software often (re)uses\ncode from third-parties libraries, a permission system for libraries can be\ndesirable to enhance security. In this short-paper, we adapt concepts from\ncapability systems building a novel theoretical foundation for permission\nsystem at the level of the programming language. This leads to PermRust, a\ntoken-based permission system for the Rust programming language as a zero cost\nabstraction on top of its type-system. With it access to system resources can\nbe managed per library.",
    "text": "PermRust: A Token-based Permission System for Rust Permission systems which restrict access to system resources are a\nwell-established technology in operating systems, especially for smartphones.\nHowever, as such systems are implemented in the operating system they can at\nmost manage access on the process-level. Since moderns software often (re)uses\ncode from third-parties libraries, a permission system for libraries can be\ndesirable to enhance security. In this short-paper, we adapt concepts from\ncapability systems building a novel theoretical foundation for permission\nsystem at the level of the programming language. This leads to PermRust, a\ntoken-based permission system for the Rust programming language as a zero cost\nabstraction on top of its type-system. With it access to system resources can\nbe managed per library.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733027
  },
  {
    "title": "A Performance Model for Warp Specialization Kernels",
    "abstract": "This paper presents a performance model tailored for warp specialization\nkernels, focusing on factors such as warp size, tilling size, input matrix\nsize, memory bandwidth, and thread divergence. Our model offers accurate\npredictions of execution time by leveraging differential equations validated\nthrough simulations and experiments. The insights gained from this model not\nonly enhance our understanding of warp specialization techniques but also have\npractical implications for optimizing GPU-accelerated applications through\ncompiler optimizations, kernel parameter tuning, and algorithm design.",
    "text": "A Performance Model for Warp Specialization Kernels This paper presents a performance model tailored for warp specialization\nkernels, focusing on factors such as warp size, tilling size, input matrix\nsize, memory bandwidth, and thread divergence. Our model offers accurate\npredictions of execution time by leveraging differential equations validated\nthrough simulations and experiments. The insights gained from this model not\nonly enhance our understanding of warp specialization techniques but also have\npractical implications for optimizing GPU-accelerated applications through\ncompiler optimizations, kernel parameter tuning, and algorithm design.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733035
  },
  {
    "title": "Choreographic Quick Changes: First-Class Location (Set) Polymorphism",
    "abstract": "Choreographic programming is a promising new paradigm for programming\nconcurrent systems where a developer writes a single centralized program that\ncompiles to individual programs for each node. Existing choreographic\nlanguages, however, lack critical features integral to modern systems, like the\nability of one node to dynamically compute who should perform a computation and\nsend that decision to others. This work addresses this gap with $\\lambda_{QC}$,\nthe first typed choreographic language with \\emph{first class process names}\nand polymorphism over both types and (sets of) locations. $\\lambda_{QC}$ also\nimproves expressive power over previous work by supporting algebraic and\nrecursive data types as well as multiply-located values. We formalize and\nmechanically verify our results in Rocq, including the standard choreographic\nguarantee of deadlock freedom.",
    "text": "Choreographic Quick Changes: First-Class Location (Set) Polymorphism Choreographic programming is a promising new paradigm for programming\nconcurrent systems where a developer writes a single centralized program that\ncompiles to individual programs for each node. Existing choreographic\nlanguages, however, lack critical features integral to modern systems, like the\nability of one node to dynamically compute who should perform a computation and\nsend that decision to others. This work addresses this gap with $\\lambda_{QC}$,\nthe first typed choreographic language with \\emph{first class process names}\nand polymorphism over both types and (sets of) locations. $\\lambda_{QC}$ also\nimproves expressive power over previous work by supporting algebraic and\nrecursive data types as well as multiply-located values. We formalize and\nmechanically verify our results in Rocq, including the standard choreographic\nguarantee of deadlock freedom.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733046
  },
  {
    "title": "Solving Package Management via Hypergraph Dependency Resolution",
    "abstract": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.",
    "text": "Solving Package Management via Hypergraph Dependency Resolution Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733057
  },
  {
    "title": "Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations",
    "abstract": "Students in programming languages and formal logic courses often struggle\nwith constructing rule-based derivation trees due to the complexity of applying\ninference rules, the lack of immediate feedback, and the manual effort required\nfor handwritten proofs. We present Hazel Deriver, a live, web-based editor\ndesigned to scaffold derivation construction through multiple layers of\nsupport. Built on the Hazel live programming environment, it provides a\nstructured, interactive experience that encourages iterative exploration and\nreal-time feedback. A preliminary user study with former students suggests that\nHazel Deriver reduces the perceived difficulty of derivation tasks while\nimproving conceptual understanding and engagement. We discuss the design of its\nlayered scaffolding features and raise questions about balancing system\nguidance with learner autonomy.",
    "text": "Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations Students in programming languages and formal logic courses often struggle\nwith constructing rule-based derivation trees due to the complexity of applying\ninference rules, the lack of immediate feedback, and the manual effort required\nfor handwritten proofs. We present Hazel Deriver, a live, web-based editor\ndesigned to scaffold derivation construction through multiple layers of\nsupport. Built on the Hazel live programming environment, it provides a\nstructured, interactive experience that encourages iterative exploration and\nreal-time feedback. A preliminary user study with former students suggests that\nHazel Deriver reduces the perceived difficulty of derivation tasks while\nimproving conceptual understanding and engagement. We discuss the design of its\nlayered scaffolding features and raise questions about balancing system\nguidance with learner autonomy.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733066
  },
  {
    "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput",
    "abstract": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.",
    "text": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733078
  },
  {
    "title": "Gradual Metaprogramming",
    "abstract": "Data engineers increasingly use domain-specific languages (DSLs) to generate\nthe code for data pipelines. Such DSLs are often embedded in Python.\nUnfortunately, there are challenges in debugging the generation of data\npipelines: an error in a Python DSL script is often detected too late, after\nthe execution of the script, and the source code location that triggers the\nerror is hard to pinpoint.\n  In this paper, we focus on the scenario where a DSL embedded in Python (so it\nis dynamically-typed) generates data pipeline description code that is\nstatically-typed. We propose gradual metaprogramming to (1) provide a migration\npath toward statically typed DSLs, (2) immediately provide earlier detection of\ncode generation type errors, and (3) report the source code location\nresponsible for the type error. Gradual metaprogramming accomplishes this by\ntype checking code fragments and incrementally performing runtime checks as\nthey are spliced together. We define MetaGTLC, a metaprogramming calculus in\nwhich a gradually-typed metalanguage manipulates a statically-typed object\nlanguage, and give semantics to it by translation to the cast calculus MetaCC.\nWe prove that successful metaevaluation always generates a well-typed object\nprogram and mechanize the proof in Agda.",
    "text": "Gradual Metaprogramming Data engineers increasingly use domain-specific languages (DSLs) to generate\nthe code for data pipelines. Such DSLs are often embedded in Python.\nUnfortunately, there are challenges in debugging the generation of data\npipelines: an error in a Python DSL script is often detected too late, after\nthe execution of the script, and the source code location that triggers the\nerror is hard to pinpoint.\n  In this paper, we focus on the scenario where a DSL embedded in Python (so it\nis dynamically-typed) generates data pipeline description code that is\nstatically-typed. We propose gradual metaprogramming to (1) provide a migration\npath toward statically typed DSLs, (2) immediately provide earlier detection of\ncode generation type errors, and (3) report the source code location\nresponsible for the type error. Gradual metaprogramming accomplishes this by\ntype checking code fragments and incrementally performing runtime checks as\nthey are spliced together. We define MetaGTLC, a metaprogramming calculus in\nwhich a gradually-typed metalanguage manipulates a statically-typed object\nlanguage, and give semantics to it by translation to the cast calculus MetaCC.\nWe prove that successful metaevaluation always generates a well-typed object\nprogram and mechanize the proof in Agda.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733092
  },
  {
    "title": "Linguine: A Natural-Language Programming Language with Formal Semantics\n  and a Clean Compiler Pipeline",
    "abstract": "Linguine is a natural-language-inspired programming language that enables\nusers to write programs in a fluent, controlled subset of English while\npreserving formal semantics. The language introduces anaphoric constructs, such\nas pronoun variables (e.g., \"it\", \"them\"), that are statically resolved through\nreferent-tracking analysis combined with a Hindley-Milner-style type system.\nEach pronoun is guaranteed to be unambiguous and well-typed at compile time.\n  The Linguine compiler pipeline includes lexing, parsing, clause graph\nconstruction, desugaring into a typed intermediate representation, type\ninference, and abstract interpretation. This enables the early detection of\nsemantic errors, such as undefined or type-inconsistent references. A\nlightweight backend currently generates Python code.\n  This paper formalizes the core language, defines its typing and operational\nsemantics, and proves the soundness of its pronoun resolution mechanism. An\ninitial evaluation shows that Linguine allows the expression of concise and\nreadable programs while supporting static verification.\n  Linguine represents a step toward programming systems that prioritize human\nlinguistic intuition while remaining grounded in formal methods and\ntype-theoretic rigor.",
    "text": "Linguine: A Natural-Language Programming Language with Formal Semantics\n  and a Clean Compiler Pipeline Linguine is a natural-language-inspired programming language that enables\nusers to write programs in a fluent, controlled subset of English while\npreserving formal semantics. The language introduces anaphoric constructs, such\nas pronoun variables (e.g., \"it\", \"them\"), that are statically resolved through\nreferent-tracking analysis combined with a Hindley-Milner-style type system.\nEach pronoun is guaranteed to be unambiguous and well-typed at compile time.\n  The Linguine compiler pipeline includes lexing, parsing, clause graph\nconstruction, desugaring into a typed intermediate representation, type\ninference, and abstract interpretation. This enables the early detection of\nsemantic errors, such as undefined or type-inconsistent references. A\nlightweight backend currently generates Python code.\n  This paper formalizes the core language, defines its typing and operational\nsemantics, and proves the soundness of its pronoun resolution mechanism. An\ninitial evaluation shows that Linguine allows the expression of concise and\nreadable programs while supporting static verification.\n  Linguine represents a step toward programming systems that prioritize human\nlinguistic intuition while remaining grounded in formal methods and\ntype-theoretic rigor.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733106
  },
  {
    "title": "A Language-Agnostic Logical Relation for Message-Passing Protocols",
    "abstract": "Today's computing landscape has been gradually shifting to applications\ntargeting distributed and *heterogeneous* systems, such as cloud computing and\nInternet of Things (IoT) applications. These applications are predominantly\n*concurrent*, employ *message-passing*, and interface with *foreign objects*,\nranging from externally implemented code to actual physical devices such as\nsensors. Verifying that the resulting systems adhere to the intended protocol\nof interaction is challenging -- the usual assumption of a common\nimplementation language, let alone a type system, no longer applies, ruling out\nany verification method based on them. This paper develops a framework for\ncertifying *protocol compliance* of heterogeneous message-passing systems. It\ncontributes the first mechanization of a *language-agnostic logical relation*,\nasserting that its inhabitants comply with the protocol specified. This\ndefinition relies entirely on a labelled transition-based semantics,\naccommodating arbitrary inhabitants, typed and untyped alike, including foreign\nobjects. As a case study, the paper considers two scenarios: (1) *per-instance\nverification* of a specific application or hardware device, and (2)\n*once-and-for-all verification* of well-typed applications for a given type\nsystem. The logical relation and both scenarios are mechanized in the Coq\ntheorem prover.",
    "text": "A Language-Agnostic Logical Relation for Message-Passing Protocols Today's computing landscape has been gradually shifting to applications\ntargeting distributed and *heterogeneous* systems, such as cloud computing and\nInternet of Things (IoT) applications. These applications are predominantly\n*concurrent*, employ *message-passing*, and interface with *foreign objects*,\nranging from externally implemented code to actual physical devices such as\nsensors. Verifying that the resulting systems adhere to the intended protocol\nof interaction is challenging -- the usual assumption of a common\nimplementation language, let alone a type system, no longer applies, ruling out\nany verification method based on them. This paper develops a framework for\ncertifying *protocol compliance* of heterogeneous message-passing systems. It\ncontributes the first mechanization of a *language-agnostic logical relation*,\nasserting that its inhabitants comply with the protocol specified. This\ndefinition relies entirely on a labelled transition-based semantics,\naccommodating arbitrary inhabitants, typed and untyped alike, including foreign\nobjects. As a case study, the paper considers two scenarios: (1) *per-instance\nverification* of a specific application or hardware device, and (2)\n*once-and-for-all verification* of well-typed applications for a given type\nsystem. The logical relation and both scenarios are mechanized in the Coq\ntheorem prover.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73312
  },
  {
    "title": "Verification of the Release-Acquire Semantics",
    "abstract": "The Release-Acquire (RA) semantics and its variants are some of the most\nfundamental models of concurrent semantics for architectures, programming\nlanguages, and distributed systems. Several steps have been taken in the\ndirection of testing such semantics, where one is interested in whether a\nsingle program execution is consistent with a memory model. The more general\nverification problem, i.e., checking whether all allowed program runs are\nconsistent with a memory model, has still not been studied as much. The purpose\nof this work is to bridge this gap. We tackle the verification problem, where,\ngiven an implementation described as a register machine, we check if any of its\nruns violates the RA semantics or its Strong (SRA) and Weak (WRA) variants. We\nshow that verifying WRA in this setup is in O([)n5 ], while verifying the RA\nand SRA is in both NP- and coNP-hard, and provide a PSPACE upper bound. This\nboth answers some fundamental questions about the complexity of these problems,\nbut also provides insights on the expressive power of register machines as a\nmodel.",
    "text": "Verification of the Release-Acquire Semantics The Release-Acquire (RA) semantics and its variants are some of the most\nfundamental models of concurrent semantics for architectures, programming\nlanguages, and distributed systems. Several steps have been taken in the\ndirection of testing such semantics, where one is interested in whether a\nsingle program execution is consistent with a memory model. The more general\nverification problem, i.e., checking whether all allowed program runs are\nconsistent with a memory model, has still not been studied as much. The purpose\nof this work is to bridge this gap. We tackle the verification problem, where,\ngiven an implementation described as a register machine, we check if any of its\nruns violates the RA semantics or its Strong (SRA) and Weak (WRA) variants. We\nshow that verifying WRA in this setup is in O([)n5 ], while verifying the RA\nand SRA is in both NP- and coNP-hard, and provide a PSPACE upper bound. This\nboth answers some fundamental questions about the complexity of these problems,\nbut also provides insights on the expressive power of register machines as a\nmodel.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733131
  },
  {
    "title": "Execution-Aware Program Reduction for WebAssembly via Record and Replay",
    "abstract": "WebAssembly (Wasm) programs may trigger bugs in their engine implementations.\nTo aid debugging, program reduction techniques try to produce a smaller variant\nof the input program that still triggers the bug. However, existing\nexecution-unaware program reduction techniques struggle with large and complex\nWasm programs, because they rely on static information and apply syntactic\ntransformations, while ignoring the valuable information offered by the input\nprogram's execution behavior.\n  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program\nreduction techniques that leverage execution behaviors via record and replay.\nRR-Reduce identifies a bug-triggering function as the target function, isolates\nthat function from the rest of the program, and generates a reduced program\nthat replays only the interactions between the target function and the rest of\nthe program. Hybrid-Reduce combines a complementary execution-unaware reduction\ntechnique with RR-Reduce to further reduce program size.\n  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a\ndiverse set of bugs in three engines. On average, RR-Reduce reduces the\nprograms to 1.20 percent of their original size in 14.5 minutes, which\noutperforms the state of the art by 33.15 times in terms of reduction time.\nHybrid-Reduce reduces the programs to 0.13 percent of their original size in\n3.5 hours, which outperforms the state of the art by 3.42 times in terms of\nreduced program size and 2.26 times in terms of reduction time. We envision\nRR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and\nHybrid-Reduce for scenarios where developers require the smallest possible\nprograms.",
    "text": "Execution-Aware Program Reduction for WebAssembly via Record and Replay WebAssembly (Wasm) programs may trigger bugs in their engine implementations.\nTo aid debugging, program reduction techniques try to produce a smaller variant\nof the input program that still triggers the bug. However, existing\nexecution-unaware program reduction techniques struggle with large and complex\nWasm programs, because they rely on static information and apply syntactic\ntransformations, while ignoring the valuable information offered by the input\nprogram's execution behavior.\n  We present RR-Reduce and Hybrid-Reduce, novel execution-aware program\nreduction techniques that leverage execution behaviors via record and replay.\nRR-Reduce identifies a bug-triggering function as the target function, isolates\nthat function from the rest of the program, and generates a reduced program\nthat replays only the interactions between the target function and the rest of\nthe program. Hybrid-Reduce combines a complementary execution-unaware reduction\ntechnique with RR-Reduce to further reduce program size.\n  We evaluate RR-Reduce and Hybrid-Reduce on 28 Wasm programs that trigger a\ndiverse set of bugs in three engines. On average, RR-Reduce reduces the\nprograms to 1.20 percent of their original size in 14.5 minutes, which\noutperforms the state of the art by 33.15 times in terms of reduction time.\nHybrid-Reduce reduces the programs to 0.13 percent of their original size in\n3.5 hours, which outperforms the state of the art by 3.42 times in terms of\nreduced program size and 2.26 times in terms of reduction time. We envision\nRR-Reduce as the go-to tool for rapid, on-demand debugging in minutes, and\nHybrid-Reduce for scenarios where developers require the smallest possible\nprograms.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733145
  },
  {
    "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp\n  Metaprogramming Loop",
    "abstract": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.",
    "text": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp\n  Metaprogramming Loop We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733155
  },
  {
    "title": "Hadamard-$$: Equational Quantum Programming",
    "abstract": "Quantum computing offers advantages over classical computation, yet the\nprecise features that set the two apart remain unclear. In the standard quantum\ncircuit model, adding a 1-qubit basis-changing gate -- commonly chosen to be\nthe Hadamard gate -- to a universal set of classical reversible gates yields\ncomputationally universal quantum computation. However, the computational\nbehaviours enabled by this addition are not fully characterised. We give such a\ncharacterisation by introducing a small quantum programming language extending\nthe universal classical reversible programming language $\\Pi$ with a single\nprimitive corresponding to the Hadamard gate. The language comes equipped with\na sound and complete categorical semantics that is specified by a purely\nequational theory, enabling reasoning about the equivalence of quantum programs\nin a way that can be automated. Completeness is shown by means of a novel\nfinite presentation, and corresponding synthesis algorithm, for the groups of\northogonal matrices with entries in the ring $\\mathbb{Z}[\\tfrac{1}{\\sqrt{2}}]$.",
    "text": "Hadamard-$$: Equational Quantum Programming Quantum computing offers advantages over classical computation, yet the\nprecise features that set the two apart remain unclear. In the standard quantum\ncircuit model, adding a 1-qubit basis-changing gate -- commonly chosen to be\nthe Hadamard gate -- to a universal set of classical reversible gates yields\ncomputationally universal quantum computation. However, the computational\nbehaviours enabled by this addition are not fully characterised. We give such a\ncharacterisation by introducing a small quantum programming language extending\nthe universal classical reversible programming language $\\Pi$ with a single\nprimitive corresponding to the Hadamard gate. The language comes equipped with\na sound and complete categorical semantics that is specified by a purely\nequational theory, enabling reasoning about the equivalence of quantum programs\nin a way that can be automated. Completeness is shown by means of a novel\nfinite presentation, and corresponding synthesis algorithm, for the groups of\northogonal matrices with entries in the ring $\\mathbb{Z}[\\tfrac{1}{\\sqrt{2}}]$.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7331681
  },
  {
    "title": "Reasoning about External Calls",
    "abstract": "In today's complex software, internal trusted code is tightly intertwined\nwith external untrusted code. To reason about internal code, programmers must\nreason about the potential effects of calls to external code, even though that\ncode is not trusted and may not even be available. The effects of external\ncalls can be limited, if internal code is programmed defensively, limiting\npotential effects by limiting access to the capabilities necessary to cause\nthose effects.\n  This paper addresses the specification and verification of internal code that\nrelies on encapsulation and object capabilities to limit the effects of\nexternal calls. We propose new assertions for access to capabilities, new\nspecifications for limiting effects, and a Hoare logic to verify that a module\nsatisfies its specification, even while making external calls. We illustrate\nthe approach though a running example with mechanised proofs, and prove\nsoundness of the Hoare logic.",
    "text": "Reasoning about External Calls In today's complex software, internal trusted code is tightly intertwined\nwith external untrusted code. To reason about internal code, programmers must\nreason about the potential effects of calls to external code, even though that\ncode is not trusted and may not even be available. The effects of external\ncalls can be limited, if internal code is programmed defensively, limiting\npotential effects by limiting access to the capabilities necessary to cause\nthose effects.\n  This paper addresses the specification and verification of internal code that\nrelies on encapsulation and object capabilities to limit the effects of\nexternal calls. We propose new assertions for access to capabilities, new\nspecifications for limiting effects, and a Hoare logic to verify that a module\nsatisfies its specification, even while making external calls. We illustrate\nthe approach though a running example with mechanised proofs, and prove\nsoundness of the Hoare logic.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7331781
  },
  {
    "title": "CompilerGPT: Leveraging Large Language Models for Analyzing and Acting\n  on Compiler Optimization Reports",
    "abstract": "Current compiler optimization reports often present complex, technical\ninformation that is difficult for programmers to interpret and act upon\neffectively. This paper assesses the capability of large language models (LLM)\nto understand compiler optimization reports and automatically rewrite the code\naccordingly.\n  To this end, the paper introduces CompilerGPT, a novel framework that\nautomates the interaction between compilers, LLMs, and user defined test and\nevaluation harness. CompilerGPT's workflow runs several iterations and reports\non the obtained results.\n  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),\noptimization reports from two compilers (Clang and GCC), and five benchmark\ncodes demonstrate the potential of this approach. Speedups of up to 6.5x were\nobtained, though not consistently in every test. This method holds promise for\nimproving compiler usability and streamlining the software optimization\nprocess.",
    "text": "CompilerGPT: Leveraging Large Language Models for Analyzing and Acting\n  on Compiler Optimization Reports Current compiler optimization reports often present complex, technical\ninformation that is difficult for programmers to interpret and act upon\neffectively. This paper assesses the capability of large language models (LLM)\nto understand compiler optimization reports and automatically rewrite the code\naccordingly.\n  To this end, the paper introduces CompilerGPT, a novel framework that\nautomates the interaction between compilers, LLMs, and user defined test and\nevaluation harness. CompilerGPT's workflow runs several iterations and reports\non the obtained results.\n  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),\noptimization reports from two compilers (Clang and GCC), and five benchmark\ncodes demonstrate the potential of this approach. Speedups of up to 6.5x were\nobtained, though not consistently in every test. This method holds promise for\nimproving compiler usability and streamlining the software optimization\nprocess.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7331882
  },
  {
    "title": "A Sound and Complete Characterization of Fair Asynchronous Session\n  Subtyping",
    "abstract": "Session types are abstractions of communication protocols enabling the static\nanalysis of message-passing processes. Refinement notions for session types are\nkey to support safe forms of process substitution while preserving their\ncompatibility with the rest of the system. Recently, a fair refinement relation\nfor asynchronous session types has been defined allowing the anticipation of\nmessage outputs with respect to an unbounded number of message inputs. This\nrefinement is useful to capture common patterns in communication protocols that\ntake advantage of asynchrony. However, while the semantic (\\`a la testing)\ndefinition of such refinement is straightforward, its characterization has\nproved to be quite challenging. In fact, only a sound but not complete\ncharacterization is known so far. In this paper we close this open problem by\npresenting a sound and complete characterization of asynchronous fair\nrefinement for session types. We relate this characterization to those given in\nthe literature for synchronous session types by leveraging a novel labelled\ntransition system of session types that embeds their asynchronous semantics.",
    "text": "A Sound and Complete Characterization of Fair Asynchronous Session\n  Subtyping Session types are abstractions of communication protocols enabling the static\nanalysis of message-passing processes. Refinement notions for session types are\nkey to support safe forms of process substitution while preserving their\ncompatibility with the rest of the system. Recently, a fair refinement relation\nfor asynchronous session types has been defined allowing the anticipation of\nmessage outputs with respect to an unbounded number of message inputs. This\nrefinement is useful to capture common patterns in communication protocols that\ntake advantage of asynchrony. However, while the semantic (\\`a la testing)\ndefinition of such refinement is straightforward, its characterization has\nproved to be quite challenging. In fact, only a sound but not complete\ncharacterization is known so far. In this paper we close this open problem by\npresenting a sound and complete characterization of asynchronous fair\nrefinement for session types. We relate this characterization to those given in\nthe literature for synchronous session types by leveraging a novel labelled\ntransition system of session types that embeds their asynchronous semantics.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7331998
  },
  {
    "title": "An Execution Model for RICE",
    "abstract": "In this paper, we build on the previous work of the RICE compiler by giving\nits execution model. We show the restrictions to the FlatCurry language that\nwere made to produce executable code, and present the execution model using\noperational semantics similar to Launchbury. Finally, we show that the\nexecution model conforms with the standard operational semantics for Curry.",
    "text": "An Execution Model for RICE In this paper, we build on the previous work of the RICE compiler by giving\nits execution model. We show the restrictions to the FlatCurry language that\nwere made to produce executable code, and present the execution model using\noperational semantics similar to Launchbury. Finally, we show that the\nexecution model conforms with the standard operational semantics for Curry.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73321
  },
  {
    "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation",
    "abstract": "Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.",
    "text": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733221
  },
  {
    "title": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking",
    "abstract": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.",
    "text": "CETBench: A Novel Dataset constructed via Transformations over Programs\n  for Benchmarking LLMs for Code-Equivalence Checking LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733234
  },
  {
    "title": "Towards a Characterization of Two-way Bijections in a Reversible\n  Computational Model",
    "abstract": "We introduce an imperative, stack-based, and reversible computational model\nthat characterizes Two-way Bijections both implicitly, concerning their\ncomputational complexity, and with zero-garbage.",
    "text": "Towards a Characterization of Two-way Bijections in a Reversible\n  Computational Model We introduce an imperative, stack-based, and reversible computational model\nthat characterizes Two-way Bijections both implicitly, concerning their\ncomputational complexity, and with zero-garbage.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733243
  },
  {
    "title": "HEC: Equivalence Verification Checking for Code Transformation via\n  Equality Saturation",
    "abstract": "In modern computing systems, compilation employs numerous optimization\ntechniques to enhance code performance. Source-to-source code transformations,\nwhich include control flow and datapath transformations, have been widely used\nin High-Level Synthesis (HLS) and compiler optimization.\n  While researchers actively investigate methods to improve performance with\nsource-to-source code transformations, they often overlook the significance of\nverifying their correctness. Current tools cannot provide a holistic\nverification of these transformations. This paper introduces HEC, a framework\nfor equivalence checking that leverages the e-graph data structure to\ncomprehensively verify functional equivalence between programs. HEC utilizes\nthe MLIR as its frontend and integrates MLIR into the e-graph framework.\nThrough the combination of dynamic and static e-graph rewriting, HEC\nfacilitates the validation of comprehensive code transformations.\n  We demonstrate effectiveness of HEC on PolyBenchC benchmarks, successfully\nverifying loop unrolling, tiling, and fusion transformations. HEC processes\nover 100,000 lines of MLIR code in 40 minutes with predictable runtime scaling.\nImportantly, HEC identified two critical compilation errors in mlir-opt: loop\nboundary check errors causing unintended executions during unrolling, and\nmemory read-after-write violations in loop fusion that alter program semantics.\nThese findings demonstrate HEC practical value in detecting real-world compiler\nbugs and highlight the importance of formal verification in optimization\npipelines.",
    "text": "HEC: Equivalence Verification Checking for Code Transformation via\n  Equality Saturation In modern computing systems, compilation employs numerous optimization\ntechniques to enhance code performance. Source-to-source code transformations,\nwhich include control flow and datapath transformations, have been widely used\nin High-Level Synthesis (HLS) and compiler optimization.\n  While researchers actively investigate methods to improve performance with\nsource-to-source code transformations, they often overlook the significance of\nverifying their correctness. Current tools cannot provide a holistic\nverification of these transformations. This paper introduces HEC, a framework\nfor equivalence checking that leverages the e-graph data structure to\ncomprehensively verify functional equivalence between programs. HEC utilizes\nthe MLIR as its frontend and integrates MLIR into the e-graph framework.\nThrough the combination of dynamic and static e-graph rewriting, HEC\nfacilitates the validation of comprehensive code transformations.\n  We demonstrate effectiveness of HEC on PolyBenchC benchmarks, successfully\nverifying loop unrolling, tiling, and fusion transformations. HEC processes\nover 100,000 lines of MLIR code in 40 minutes with predictable runtime scaling.\nImportantly, HEC identified two critical compilation errors in mlir-opt: loop\nboundary check errors causing unintended executions during unrolling, and\nmemory read-after-write violations in loop fusion that alter program semantics.\nThese findings demonstrate HEC practical value in detecting real-world compiler\nbugs and highlight the importance of formal verification in optimization\npipelines.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7332559
  },
  {
    "title": "Improving compiler support for SIMD offload using Arm Streaming SVE",
    "abstract": "The wider adoption of tightly coupled core-adjacent accelerators, such as Arm\nScalable Matrix Extension (SME), hinges on lowering software programming\ncomplexity. In this paper, we focus on enabling the use of SME architecture in\nStreaming Scalable Vector Extension (SSVE) mode for workloads written in C/C++.\nWhile current compilers optimize loops for all types of SIMD instructions,\nthese techniques primarily target vector units within the core and falter when\napplied to disaggregated, core-adjacent SIMD accelerators. Our goal is to\nenable the compiler to automatically generate code for such accelerators only\nwhen profitable.\n  To this end, we investigate a path towards performant, precise, and\nrepeatable computation offloading through two compiler ecosystems. We revisit\nLLVM compiler passes, MLIR transforms and their associated cost models, and\nheuristics. We hope that these insights can provide directions for evolving\ncompiler capabilities towards automatic code generation for this\nnext-generation vector processing paradigm.",
    "text": "Improving compiler support for SIMD offload using Arm Streaming SVE The wider adoption of tightly coupled core-adjacent accelerators, such as Arm\nScalable Matrix Extension (SME), hinges on lowering software programming\ncomplexity. In this paper, we focus on enabling the use of SME architecture in\nStreaming Scalable Vector Extension (SSVE) mode for workloads written in C/C++.\nWhile current compilers optimize loops for all types of SIMD instructions,\nthese techniques primarily target vector units within the core and falter when\napplied to disaggregated, core-adjacent SIMD accelerators. Our goal is to\nenable the compiler to automatically generate code for such accelerators only\nwhen profitable.\n  To this end, we investigate a path towards performant, precise, and\nrepeatable computation offloading through two compiler ecosystems. We revisit\nLLVM compiler passes, MLIR transforms and their associated cost models, and\nheuristics. We hope that these insights can provide directions for evolving\ncompiler capabilities towards automatic code generation for this\nnext-generation vector processing paradigm.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7332668
  },
  {
    "title": "Spegion: Implicit and Non-Lexical Regions with Sized Allocations",
    "abstract": "Region based memory management is a powerful tool designed with the goal of\nensuring memory safety statically. The region calculus of Tofte and Talpin is a\nwell known example of a region based system, which uses regions to manage\nmemory in a stack-like fashion. However, the region calculus is lexically\nscoped and requires explicit annotation of memory regions, which can be\ncumbersome for the programmer. Other systems have addressed non-lexical\nregions, but these approaches typically require the use of a substructural type\nsystem to track the lifetimes of regions. We present Spegion, a language with\nimplicit non-lexical regions, which provides these same memory safety\nguarantees for programs that go beyond using memory allocation in a stack-like\nmanner. We are able to achieve this with a concise syntax, and without the use\nof substructural types, relying instead on an effect system to enforce\nconstraints on region allocation and deallocation. These regions may be divided\ninto sub-regions, i.e., Splittable rEgions, allowing fine grained control over\nmemory allocation. Furthermore, Spegion permits sized allocations, where each\nvalue has an associated size which is used to ensure that regions are not\nover-allocated into. We present a type system for Spegion and prove it is type\nsafe with respect to a small-step operational semantics.",
    "text": "Spegion: Implicit and Non-Lexical Regions with Sized Allocations Region based memory management is a powerful tool designed with the goal of\nensuring memory safety statically. The region calculus of Tofte and Talpin is a\nwell known example of a region based system, which uses regions to manage\nmemory in a stack-like fashion. However, the region calculus is lexically\nscoped and requires explicit annotation of memory regions, which can be\ncumbersome for the programmer. Other systems have addressed non-lexical\nregions, but these approaches typically require the use of a substructural type\nsystem to track the lifetimes of regions. We present Spegion, a language with\nimplicit non-lexical regions, which provides these same memory safety\nguarantees for programs that go beyond using memory allocation in a stack-like\nmanner. We are able to achieve this with a concise syntax, and without the use\nof substructural types, relying instead on an effect system to enforce\nconstraints on region allocation and deallocation. These regions may be divided\ninto sub-regions, i.e., Splittable rEgions, allowing fine grained control over\nmemory allocation. Furthermore, Spegion permits sized allocations, where each\nvalue has an associated size which is used to ensure that regions are not\nover-allocated into. We present a type system for Spegion and prove it is type\nsafe with respect to a small-step operational semantics.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733278
  },
  {
    "title": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning",
    "abstract": "Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
    "text": "Pearl: Automatic Code Optimization Using Deep Reinforcement Learning Compilers are crucial in optimizing programs and accelerating their\nexecution. However, optimizing programs automatically using compilers is not\ntrivial. Recent work has attempted to use reinforcement learning (RL) to solve\nthis problem. It has limitations though. Current methods either do not support\nthe optimization of general loop nests or can only be used to optimize loop\nnests seen during training. In this paper, we propose Pearl, a novel framework\nthat uses deep reinforcement learning to automate compiler code optimization.\nIt uses an RL agent to select the sequence of code optimizations a compiler\nshould apply to make the input code run faster. This agent can optimize general\nloop nests and can generalize to programs unseen during training. To enable the\noptimization of general loop nests, we propose a novel representation of the\naction space that allows the RL agent to select on which part of the loop nest\na given code optimization should be applied. Training RL agents for loop nest\noptimization is slow and data-intensive. We accelerate this process by caching\nresults and pre-training the agent. Integrated with the Tiramisu compiler, our\napproach streamlines optimization and outperforms existing methods. To the best\nof our knowledge, Pearl is the first RL-based system to support general\nprograms composed of loop nests manipulating tensors while still being able to\ngeneralize to programs unseen during training. It is also the first to support\nthe class of polyhedral optimizations, a class of advanced loop nest\noptimizations. We evaluate Pearl on a set of benchmarks, and demonstrate\ncompetitive performance improvements over state-of-the-art compilers. Notably,\nPearl achieves a geometric mean speedup of 2.02x compared to Tiramisu and 3.36x\ncompared to Pluto.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733292
  },
  {
    "title": "Policy as Code, Policy as Type",
    "abstract": "Policies are designed to distinguish between correct and incorrect actions;\nthey are types. But badly typed actions may cause not compile errors, but\nfinancial and reputational harm We demonstrate how even the most complex ABAC\npolicies can be expressed as types in dependently typed languages such as Agda\nand Lean, providing a single framework to express, analyze, and implement\npolicies. We then go head-to-head with Rego, the popular and powerful\nopen-source ABAC policy language. We show the superior safety that comes with a\npowerful type system and built-in proof assistant. In passing, we discuss\nvarious access control models, sketch how to integrate in a future when\nattributes are distributed and signed (as discussed at the W3C), and show how\npolicies can be communicated using just the syntax of the language. Our\nexamples are in Agda.",
    "text": "Policy as Code, Policy as Type Policies are designed to distinguish between correct and incorrect actions;\nthey are types. But badly typed actions may cause not compile errors, but\nfinancial and reputational harm We demonstrate how even the most complex ABAC\npolicies can be expressed as types in dependently typed languages such as Agda\nand Lean, providing a single framework to express, analyze, and implement\npolicies. We then go head-to-head with Rego, the popular and powerful\nopen-source ABAC policy language. We show the superior safety that comes with a\npowerful type system and built-in proof assistant. In passing, we discuss\nvarious access control models, sketch how to integrate in a future when\nattributes are distributed and signed (as discussed at the W3C), and show how\npolicies can be communicated using just the syntax of the language. Our\nexamples are in Agda.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733302
  },
  {
    "title": "Compiler Optimization via LLM Reasoning for Efficient Model Serving",
    "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of\nserving large-scale models continues to be a significant barrier to widespread\naccessibility and rapid innovation. Compiler optimizations have long driven\nsubstantial performance improvements, but existing compilers struggle with\nneural workloads due to the exponentially large and highly interdependent space\nof possible transformations. Although existing stochastic search techniques can\nbe effective, they are often sample-inefficient and fail to leverage the\nstructural context underlying compilation decisions. We set out to investigate\nthe research question of whether reasoning with large language models (LLMs),\nwithout any retraining, can leverage the context-aware decision space of\ncompiler optimization to significantly improve sample efficiency. To that end,\nwe introduce a novel compilation framework (dubbed REASONING COMPILER) that\nformulates optimization as a sequential, context-aware decision process, guided\nby a large language model and structured Monte Carlo tree search (MCTS). The\nLLM acts as a proposal mechanism, suggesting hardware-aware transformations\nthat reflect the current program state and accumulated performance feedback.\nMonte Carlo tree search (MCTS) incorporates the LLM-generated proposals to\nbalance exploration and exploitation, facilitating structured,\ncontext-sensitive traversal of the expansive compiler optimization space. By\nachieving substantial speedups with markedly fewer samples than leading neural\ncompilers, our approach demonstrates the potential of LLM-guided reasoning to\ntransform the landscape of compiler optimization.",
    "text": "Compiler Optimization via LLM Reasoning for Efficient Model Serving While model serving has unlocked unprecedented capabilities, the high cost of\nserving large-scale models continues to be a significant barrier to widespread\naccessibility and rapid innovation. Compiler optimizations have long driven\nsubstantial performance improvements, but existing compilers struggle with\nneural workloads due to the exponentially large and highly interdependent space\nof possible transformations. Although existing stochastic search techniques can\nbe effective, they are often sample-inefficient and fail to leverage the\nstructural context underlying compilation decisions. We set out to investigate\nthe research question of whether reasoning with large language models (LLMs),\nwithout any retraining, can leverage the context-aware decision space of\ncompiler optimization to significantly improve sample efficiency. To that end,\nwe introduce a novel compilation framework (dubbed REASONING COMPILER) that\nformulates optimization as a sequential, context-aware decision process, guided\nby a large language model and structured Monte Carlo tree search (MCTS). The\nLLM acts as a proposal mechanism, suggesting hardware-aware transformations\nthat reflect the current program state and accumulated performance feedback.\nMonte Carlo tree search (MCTS) incorporates the LLM-generated proposals to\nbalance exploration and exploitation, facilitating structured,\ncontext-sensitive traversal of the expansive compiler optimization space. By\nachieving substantial speedups with markedly fewer samples than leading neural\ncompilers, our approach demonstrates the potential of LLM-guided reasoning to\ntransform the landscape of compiler optimization.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733313
  },
  {
    "title": "Optimizing Optimizations: Case Study on Detecting Specific Types of\n  Mathematical Optimization Constraints with E-Graphs in JijModeling",
    "abstract": "In solving mathematical optimization problems efficiently, it is crucial to\nmake use of information about specific types of constraints, such as the\none-hot or Special-Ordered Set (SOS) constraints. In many cases, exploiting\nsuch information gives asymptotically better execution time. JijModeling, an\nindustrial-strength mathematical optimization modeller, achieves this by\nseparating the symbolic representation of an optimization problem from the\ninput data. In this paper, we will report a real-world case study on a\nconstraint detection mechanism modulo the algebraic congruence using e-graphs,\nand describe heuristic criteria for designing rewriting systems. We give\nbenchmarking result that shows the performance impact of the constraint\ndetection mechanism.\n  We also introduce egg_recursive, a utility library for writing egg-terms as\nrecursive abstract syntax trees, reducing the burden of writing and maintaining\ncomplex terms in S-expressions.",
    "text": "Optimizing Optimizations: Case Study on Detecting Specific Types of\n  Mathematical Optimization Constraints with E-Graphs in JijModeling In solving mathematical optimization problems efficiently, it is crucial to\nmake use of information about specific types of constraints, such as the\none-hot or Special-Ordered Set (SOS) constraints. In many cases, exploiting\nsuch information gives asymptotically better execution time. JijModeling, an\nindustrial-strength mathematical optimization modeller, achieves this by\nseparating the symbolic representation of an optimization problem from the\ninput data. In this paper, we will report a real-world case study on a\nconstraint detection mechanism modulo the algebraic congruence using e-graphs,\nand describe heuristic criteria for designing rewriting systems. We give\nbenchmarking result that shows the performance impact of the constraint\ndetection mechanism.\n  We also introduce egg_recursive, a utility library for writing egg-terms as\nrecursive abstract syntax trees, reducing the burden of writing and maintaining\ncomplex terms in S-expressions.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733325
  },
  {
    "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
    "abstract": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
    "text": "How Programming Concepts and Neurons Are Shared in Code Language Models Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7333388
  },
  {
    "title": "Using Code Snippets to Teach Programming Languages",
    "abstract": "Coding is a fundamental skill required in the engineering discipline, and\nmuch work exists exploring better ways of teaching coding in the higher\neducation context. In particular, Code Snippets (CSs) are approved to be an\neffective way of introducing programming language units to students. CSs are\nportions of source code of varying size and content. They can be used in a\nmyriad of ways, one of which is to teach the code they contain as well as its\nfunction. To further explore the use of CSs, a pedagogical summer internship\nproject was set up at the Warwick Manufacturing Group (WMG). The scope of the\nconsiderations for the study derives from an educational standpoint. Within the\nevaluations made, the focus was primarily given to pieces of information which\nproved to provide evidence pertaining to the methodology involved in either\nteaching or developing teaching materials. By taking the results produced into\naccount from a pedagogical perspective, it was found that several qualities of\npopular code snippet tutorials which benefit or hinder the learning process,\nincluding code length, interactivity, further support, and quality of\nexplanation. These qualities are then combined and used to present a plan for\nthe design of an effective learning resource which makes use of code snippets.",
    "text": "Using Code Snippets to Teach Programming Languages Coding is a fundamental skill required in the engineering discipline, and\nmuch work exists exploring better ways of teaching coding in the higher\neducation context. In particular, Code Snippets (CSs) are approved to be an\neffective way of introducing programming language units to students. CSs are\nportions of source code of varying size and content. They can be used in a\nmyriad of ways, one of which is to teach the code they contain as well as its\nfunction. To further explore the use of CSs, a pedagogical summer internship\nproject was set up at the Warwick Manufacturing Group (WMG). The scope of the\nconsiderations for the study derives from an educational standpoint. Within the\nevaluations made, the focus was primarily given to pieces of information which\nproved to provide evidence pertaining to the methodology involved in either\nteaching or developing teaching materials. By taking the results produced into\naccount from a pedagogical perspective, it was found that several qualities of\npopular code snippet tutorials which benefit or hinder the learning process,\nincluding code length, interactivity, further support, and quality of\nexplanation. These qualities are then combined and used to present a plan for\nthe design of an effective learning resource which makes use of code snippets.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733396
  },
  {
    "title": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated\n  Code Evaluation",
    "abstract": "In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes.",
    "text": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated\n  Code Evaluation In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7334101
  },
  {
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "abstract": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "text": "CodeV-R1: Reasoning-Enhanced Verilog Generation Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7334259
  },
  {
    "title": "Is spreadsheet syntax better than numeric indexing for cell selection?",
    "abstract": "Selecting a subset of cells is a common task in data engineering, for\nexample, to remove errors or select only specific parts of a table. Multiple\napproaches to express this selection exist. One option is numeric indexing,\ncommonly found in general programming languages, where a tuple of numbers\nidentifies the cell. Alternatively, the separate dimensions can be referred to\nusing different enumeration schemes like \"A1\" for the first cell, commonly\nfound in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for\ndata practitioners, we compare the two options with respect to speed and\ncorrectness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes\nusing spreadsheet-style syntax. Additionally, when writing code, they make\nfewer mistakes and are faster when using spreadsheet syntax compared to numeric\nsyntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data\nengineering, appears to be a promising alternative to explore in future tools\nto support practitioners without a software engineering background.",
    "text": "Is spreadsheet syntax better than numeric indexing for cell selection? Selecting a subset of cells is a common task in data engineering, for\nexample, to remove errors or select only specific parts of a table. Multiple\napproaches to express this selection exist. One option is numeric indexing,\ncommonly found in general programming languages, where a tuple of numbers\nidentifies the cell. Alternatively, the separate dimensions can be referred to\nusing different enumeration schemes like \"A1\" for the first cell, commonly\nfound in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for\ndata practitioners, we compare the two options with respect to speed and\ncorrectness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes\nusing spreadsheet-style syntax. Additionally, when writing code, they make\nfewer mistakes and are faster when using spreadsheet syntax compared to numeric\nsyntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data\nengineering, appears to be a promising alternative to explore in future tools\nto support practitioners without a software engineering background.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733439
  },
  {
    "title": "VERINA: Benchmarking Verifiable Code Generation",
    "abstract": "Large language models (LLMs) are increasingly integrated in software\ndevelopment, but ensuring correctness in LLM-generated code remains challenging\nand often requires costly manual review. Verifiable code generation -- jointly\ngenerating code, specifications, and proofs of code-specification alignment --\noffers a promising path to address this limitation and further unleash LLMs'\nbenefits in coding. Yet, there exists a significant gap in evaluation: current\nbenchmarks often lack support for end-to-end verifiable code generation. In\nthis paper, we introduce Verina (Verifiable Code Generation Arena), a\nhigh-quality benchmark enabling a comprehensive and modular evaluation of code,\nspecification, and proof generation as well as their compositions. Verina\nconsists of 189 manually curated coding tasks in Lean, with detailed problem\ndescriptions, reference implementations, formal specifications, and extensive\ntest suites. Our extensive evaluation of state-of-the-art LLMs reveals\nsignificant challenges in verifiable code generation, especially in proof\ngeneration, underscoring the need for improving LLM-based theorem provers in\nverification domains. The best model, OpenAI o4-mini, generates only 61.4%\ncorrect code, 51.0% sound and complete specifications, and 3.6% successful\nproofs, with one trial per task. We hope Verina will catalyze progress in\nverifiable code generation by providing a rigorous and comprehensive benchmark.\nWe release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina\nand our evaluation code on https://github.com/sunblaze-ucb/verina.",
    "text": "VERINA: Benchmarking Verifiable Code Generation Large language models (LLMs) are increasingly integrated in software\ndevelopment, but ensuring correctness in LLM-generated code remains challenging\nand often requires costly manual review. Verifiable code generation -- jointly\ngenerating code, specifications, and proofs of code-specification alignment --\noffers a promising path to address this limitation and further unleash LLMs'\nbenefits in coding. Yet, there exists a significant gap in evaluation: current\nbenchmarks often lack support for end-to-end verifiable code generation. In\nthis paper, we introduce Verina (Verifiable Code Generation Arena), a\nhigh-quality benchmark enabling a comprehensive and modular evaluation of code,\nspecification, and proof generation as well as their compositions. Verina\nconsists of 189 manually curated coding tasks in Lean, with detailed problem\ndescriptions, reference implementations, formal specifications, and extensive\ntest suites. Our extensive evaluation of state-of-the-art LLMs reveals\nsignificant challenges in verifiable code generation, especially in proof\ngeneration, underscoring the need for improving LLM-based theorem provers in\nverification domains. The best model, OpenAI o4-mini, generates only 61.4%\ncorrect code, 51.0% sound and complete specifications, and 3.6% successful\nproofs, with one trial per task. We hope Verina will catalyze progress in\nverifiable code generation by providing a rigorous and comprehensive benchmark.\nWe release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina\nand our evaluation code on https://github.com/sunblaze-ucb/verina.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733452
  },
  {
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "text": "DINGO: Constrained Inference for Diffusion LLMs Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733463
  },
  {
    "title": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding",
    "abstract": "While AI programming tools hold the promise of increasing programmers'\ncapabilities and productivity to a remarkable degree, they often exclude users\nfrom essential decision-making processes, causing many to effectively \"turn off\ntheir brains\" and over-rely on solutions provided by these systems. These\nbehaviors can have severe consequences in critical domains, like software\nsecurity. We propose Human-in-the-loop Decoding, a novel interaction technique\nthat allows users to observe and directly influence LLM decisions during code\ngeneration, in order to align the model's output with their personal\nrequirements. We implement this technique in HiLDe, a code completion assistant\nthat highlights critical decisions made by the LLM and provides local\nalternatives for the user to explore. In a within-subjects study (N=18) on\nsecurity-related tasks, we found that HiLDe led participants to generate\nsignificantly fewer vulnerabilities and better align code generation with their\ngoals compared to a traditional code completion assistant.",
    "text": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding While AI programming tools hold the promise of increasing programmers'\ncapabilities and productivity to a remarkable degree, they often exclude users\nfrom essential decision-making processes, causing many to effectively \"turn off\ntheir brains\" and over-rely on solutions provided by these systems. These\nbehaviors can have severe consequences in critical domains, like software\nsecurity. We propose Human-in-the-loop Decoding, a novel interaction technique\nthat allows users to observe and directly influence LLM decisions during code\ngeneration, in order to align the model's output with their personal\nrequirements. We implement this technique in HiLDe, a code completion assistant\nthat highlights critical decisions made by the LLM and provides local\nalternatives for the user to explore. In a within-subjects study (N=18) on\nsecurity-related tasks, we found that HiLDe led participants to generate\nsignificantly fewer vulnerabilities and better align code generation with their\ngoals compared to a traditional code completion assistant.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733473
  },
  {
    "title": "TPDE: A Fast Adaptable Compiler Back-End Framework",
    "abstract": "Fast machine code generation is especially important for fast start-up\njust-in-time compilation, where the compilation time is part of the end-to-end\nlatency. However, widely used compiler frameworks like LLVM do not prioritize\nfast compilation and require an extra IR translation step increasing latency\neven further; and rolling a custom code generator is a substantial engineering\neffort, especially when targeting multiple architectures.\n  Therefore, in this paper, we present TPDE, a compiler back-end framework that\nadapts to existing code representations in SSA form. Using an IR-specific\nadapter providing canonical access to IR data structures and a specification of\nthe IR semantics, the framework performs one analysis pass and then performs\nthe compilation in just a single pass, combining instruction selection,\nregister allocation, and instruction encoding. The generated target\ninstructions are primarily derived code written in high-level language through\nLLVM's Machine IR, easing portability to different architectures while enabling\noptimizations during code generation.\n  To show the generality of our framework, we build a new back-end for LLVM\nfrom scratch targeting x86-64 and AArch64. Performance results on SPECint 2017\nshow that we can compile LLVM-IR 8--24x faster than LLVM -O0 while being on-par\nin terms of run-time performance. We also demonstrate the benefits of adapting\nto domain-specific IRs in JIT contexts, particularly WebAssembly and database\nquery compilation, where avoiding the extra IR translation further reduces\ncompilation latency.",
    "text": "TPDE: A Fast Adaptable Compiler Back-End Framework Fast machine code generation is especially important for fast start-up\njust-in-time compilation, where the compilation time is part of the end-to-end\nlatency. However, widely used compiler frameworks like LLVM do not prioritize\nfast compilation and require an extra IR translation step increasing latency\neven further; and rolling a custom code generator is a substantial engineering\neffort, especially when targeting multiple architectures.\n  Therefore, in this paper, we present TPDE, a compiler back-end framework that\nadapts to existing code representations in SSA form. Using an IR-specific\nadapter providing canonical access to IR data structures and a specification of\nthe IR semantics, the framework performs one analysis pass and then performs\nthe compilation in just a single pass, combining instruction selection,\nregister allocation, and instruction encoding. The generated target\ninstructions are primarily derived code written in high-level language through\nLLVM's Machine IR, easing portability to different architectures while enabling\noptimizations during code generation.\n  To show the generality of our framework, we build a new back-end for LLVM\nfrom scratch targeting x86-64 and AArch64. Performance results on SPECint 2017\nshow that we can compile LLVM-IR 8--24x faster than LLVM -O0 while being on-par\nin terms of run-time performance. We also demonstrate the benefits of adapting\nto domain-specific IRs in JIT contexts, particularly WebAssembly and database\nquery compilation, where avoiding the extra IR translation further reduces\ncompilation latency.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733485
  },
  {
    "title": "An instance of FreeCHR with refined operational semantics",
    "abstract": "Constraint Handling Rules (CHR) is a rule-based programming language which is\ntypically embedded into a general-purpose language. There exists a plethora of\nimplementations of CHR for numerous host languages. However, the existing\nimplementations often reinvent the way to embed CHR, which impedes maintenance\nand weakens assertions of correctness. To formalize and thereby unify the\nembedding of CHR into arbitrary host languages, we introduced the framework\nFreeCHR and proved it to be a valid representation of classical CHR. Until now,\nthis framework only includes a translation of the very abstract operational\nsemantics of CHR which, due to its abstract nature, introduces several\npractical issues. In this paper, we introduce an execution algorithm for\nFreeCHR. We derive it from the refined operational semantics of CHR, which\nresolve the issues introduced by the very abstract semantics. We also prove\nsoundness of the algorithm with respect to the very abstract semantics of\nFreeCHR. Hereby we provide a unified and an easy to implement guideline for new\nCHR implementations, as well as an algorithmic definition of the refined\noperational semantics.",
    "text": "An instance of FreeCHR with refined operational semantics Constraint Handling Rules (CHR) is a rule-based programming language which is\ntypically embedded into a general-purpose language. There exists a plethora of\nimplementations of CHR for numerous host languages. However, the existing\nimplementations often reinvent the way to embed CHR, which impedes maintenance\nand weakens assertions of correctness. To formalize and thereby unify the\nembedding of CHR into arbitrary host languages, we introduced the framework\nFreeCHR and proved it to be a valid representation of classical CHR. Until now,\nthis framework only includes a translation of the very abstract operational\nsemantics of CHR which, due to its abstract nature, introduces several\npractical issues. In this paper, we introduce an execution algorithm for\nFreeCHR. We derive it from the refined operational semantics of CHR, which\nresolve the issues introduced by the very abstract semantics. We also prove\nsoundness of the algorithm with respect to the very abstract semantics of\nFreeCHR. Hereby we provide a unified and an easy to implement guideline for new\nCHR implementations, as well as an algorithmic definition of the refined\noperational semantics.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733495
  },
  {
    "title": "Linear Layouts: Robust Code Generation of Efficient Tensor Computation\n  Using $\\mathbb{F}_2$",
    "abstract": "Efficient tensor computation is a cornerstone of modern deep learning (DL)\nworkloads, yet existing approaches struggle to achieve flexible and performant\ndesign and implementation of tensor layouts -- mappings between logical tensors\nand hardware resources. The increasing complexity of DL algorithms and hardware\ndemands a generic and systematic approach to handling tensor layouts. In this\nwork, we introduce Linear Layouts, a novel approach that models tensor layouts\nusing linear algebra over $\\mathbb{F}_2$. By representing tensor layouts as\nbinary matrices acting on the bits of the hardware representation, our approach\nenables a generic layout definition -- as opposed to the classical case-by-case\napproach -- and allows for generic layout-to-layout conversions, eliminating\nthe quadratic explosion that plagues existing solutions. We integrate linear\nlayouts with Triton and demonstrate their effectiveness in optimizing\nindividual Triton operators as well as kernels written in Triton. We also show\nthat linear layouts reduce engineering effort in the compiler backend while\nfixing several bugs in Triton's legacy layout system.",
    "text": "Linear Layouts: Robust Code Generation of Efficient Tensor Computation\n  Using $\\mathbb{F}_2$ Efficient tensor computation is a cornerstone of modern deep learning (DL)\nworkloads, yet existing approaches struggle to achieve flexible and performant\ndesign and implementation of tensor layouts -- mappings between logical tensors\nand hardware resources. The increasing complexity of DL algorithms and hardware\ndemands a generic and systematic approach to handling tensor layouts. In this\nwork, we introduce Linear Layouts, a novel approach that models tensor layouts\nusing linear algebra over $\\mathbb{F}_2$. By representing tensor layouts as\nbinary matrices acting on the bits of the hardware representation, our approach\nenables a generic layout definition -- as opposed to the classical case-by-case\napproach -- and allows for generic layout-to-layout conversions, eliminating\nthe quadratic explosion that plagues existing solutions. We integrate linear\nlayouts with Triton and demonstrate their effectiveness in optimizing\nindividual Triton operators as well as kernels written in Triton. We also show\nthat linear layouts reduce engineering effort in the compiler backend while\nfixing several bugs in Triton's legacy layout system.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733505
  },
  {
    "title": "KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel\n  Performance Tooling on Modern AI Workloads",
    "abstract": "In this work, we propose KPerfIR, a novel multilevel compiler-centric\ninfrastructure to enable the development of customizable, extendable, and\nportable profiling tools tailored for modern artificial intelligence (AI)\nworkloads on modern GPUs. Our approach integrates profiling capabilities\ndirectly into the compiler workflow, allowing profiling functionalities to be\nimplemented as compiler passes, offering a programmable and reusable framework\nfor performance analysis. This design bridges the gap between compilers and\nprofilers, enabling fine-grained insights into complex optimization challenges\nsuch as overlapping the execution of fine-grained function units on GPUs.\nKPerfIR is integrated into the Triton infrastructure to highlight the power of\na compiler-centric approach to advance performance analysis and optimization in\nthe ever-evolving landscape of AI compilers. Our evaluation shows that our tool\nincurs low overhead (8.2%), provides accurate measurements (2% relative error),\nand delivers actionable insights into complicated GPU intra-kernel\noptimizations.",
    "text": "KPerfIR: Towards an Open and Compiler-centric Ecosystem for GPU Kernel\n  Performance Tooling on Modern AI Workloads In this work, we propose KPerfIR, a novel multilevel compiler-centric\ninfrastructure to enable the development of customizable, extendable, and\nportable profiling tools tailored for modern artificial intelligence (AI)\nworkloads on modern GPUs. Our approach integrates profiling capabilities\ndirectly into the compiler workflow, allowing profiling functionalities to be\nimplemented as compiler passes, offering a programmable and reusable framework\nfor performance analysis. This design bridges the gap between compilers and\nprofilers, enabling fine-grained insights into complex optimization challenges\nsuch as overlapping the execution of fine-grained function units on GPUs.\nKPerfIR is integrated into the Triton infrastructure to highlight the power of\na compiler-centric approach to advance performance analysis and optimization in\nthe ever-evolving landscape of AI compilers. Our evaluation shows that our tool\nincurs low overhead (8.2%), provides accurate measurements (2% relative error),\nand delivers actionable insights into complicated GPU intra-kernel\noptimizations.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733514
  },
  {
    "title": "Custom Representations of Inductive Families",
    "abstract": "Inductive families provide a convenient way of programming with dependent\ntypes. Yet, when it comes to compilation, their default linked-tree runtime\nrepresentations, as well as the need to convert between different indexed views\nof the same data, can lead to unsatisfactory runtime performance. In this\npaper, we introduce a language with dependent types, and inductive families\nwith customisable representations. Representations are a version of Wadler's\nviews, refined to inductive families like in Epigram, but with compilation\nguarantees: a represented inductive family will not leave any runtime traces\nbehind, without relying on heuristics such as deforestation. This way, we can\nbuild a library of convenient inductive families based on a minimal set of\nprimitives, whose re-indexing and conversion functions are erased during\ncompilation. We show how we can express optimisation techniques such as\nrepresenting Nat-like types as GMP-style big integers, without special casing\nin the compiler. With dependent types, reasoning about data representations is\nalso possible through a provided modality. This yields computationally\nirrelevant isomorphisms between the original and represented data.",
    "text": "Custom Representations of Inductive Families Inductive families provide a convenient way of programming with dependent\ntypes. Yet, when it comes to compilation, their default linked-tree runtime\nrepresentations, as well as the need to convert between different indexed views\nof the same data, can lead to unsatisfactory runtime performance. In this\npaper, we introduce a language with dependent types, and inductive families\nwith customisable representations. Representations are a version of Wadler's\nviews, refined to inductive families like in Epigram, but with compilation\nguarantees: a represented inductive family will not leave any runtime traces\nbehind, without relying on heuristics such as deforestation. This way, we can\nbuild a library of convenient inductive families based on a minimal set of\nprimitives, whose re-indexing and conversion functions are erased during\ncompilation. We show how we can express optimisation techniques such as\nrepresenting Nat-like types as GMP-style big integers, without special casing\nin the compiler. With dependent types, reasoning about data representations is\nalso possible through a provided modality. This yields computationally\nirrelevant isomorphisms between the original and represented data.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733525
  },
  {
    "title": "Local Type Inference for Context-Free Session Types",
    "abstract": "We address the problem of local type inference for a language based on System\nF with context-free session types. We present an algorithm that leverages the\nbidirectional type checking approach to propagate type information, enabling\nfirst class polymorphism while addressing the intricacies brought about by the\nsequential composition operator and type equivalence. The algorithm improves\nthe language's usability by eliminating the need for type annotations at type\napplication sites.",
    "text": "Local Type Inference for Context-Free Session Types We address the problem of local type inference for a language based on System\nF with context-free session types. We present an algorithm that leverages the\nbidirectional type checking approach to propagate type information, enabling\nfirst class polymorphism while addressing the intricacies brought about by the\nsequential composition operator and type equivalence. The algorithm improves\nthe language's usability by eliminating the need for type annotations at type\napplication sites.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7335339
  },
  {
    "title": "An Efficient Implementation of Guard-Based Synchronization for an\n  Object-Oriented Programming Language",
    "abstract": "In the shared variable model of concurrency, guarded atomic actions restrict\nthe possible interference between processes by regions of atomic execution. The\nguard specifies the condition for entering an atomic region. That is a\nconvenient model for the specification and verification of concurrent programs,\nbut has eschewed efficient execution so far. This article shows how guarded\natomic actions, when attached to objects, can be implemented highly efficiently\nusing a combination of coroutines, operating-system worker threads, and\ndedicated management of object queues and stacks. The efficiency of an\nexperimental language, Lime, is shown to compare favourably with that of\nC/Pthreads, Go, Erlang, Java, and Haskell on synthetic benchmarks.",
    "text": "An Efficient Implementation of Guard-Based Synchronization for an\n  Object-Oriented Programming Language In the shared variable model of concurrency, guarded atomic actions restrict\nthe possible interference between processes by regions of atomic execution. The\nguard specifies the condition for entering an atomic region. That is a\nconvenient model for the specification and verification of concurrent programs,\nbut has eschewed efficient execution so far. This article shows how guarded\natomic actions, when attached to objects, can be implemented highly efficiently\nusing a combination of coroutines, operating-system worker threads, and\ndedicated management of object queues and stacks. The efficiency of an\nexperimental language, Lime, is shown to compare favourably with that of\nC/Pthreads, Go, Erlang, Java, and Haskell on synthetic benchmarks.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733545
  },
  {
    "title": "Thread and Memory-Safe Programming with CLASS",
    "abstract": "CLASS is a proof-of-concept general purpose linear programming language,\nflexibly supporting realistic concurrent programming idioms, and featuring an\nexpressive linear type system ensuring that programs (1) never misuse or leak\nstateful resources or memory, (2) never deadlock, and (3) always terminate. The\ndesign of CLASS and the strong static guarantees of its type system originates\nin its Linear Logic and proposition-as-types foundations. However, instead of\nfocusing on its theoretical foundations, this paper briefly illustrates, in a\ntutorial form, an identifiable CLASS session-based programming style where\nstrong correctness properties are automatically ensured by type-checking. Our\nmore challenging examples include concurrent thread and memory-safe mutable\nADTs, lazy stream programming, and manipulation of linear digital assets as\nused in smart contracts.",
    "text": "Thread and Memory-Safe Programming with CLASS CLASS is a proof-of-concept general purpose linear programming language,\nflexibly supporting realistic concurrent programming idioms, and featuring an\nexpressive linear type system ensuring that programs (1) never misuse or leak\nstateful resources or memory, (2) never deadlock, and (3) always terminate. The\ndesign of CLASS and the strong static guarantees of its type system originates\nin its Linear Logic and proposition-as-types foundations. However, instead of\nfocusing on its theoretical foundations, this paper briefly illustrates, in a\ntutorial form, an identifiable CLASS session-based programming style where\nstrong correctness properties are automatically ensured by type-checking. Our\nmore challenging examples include concurrent thread and memory-safe mutable\nADTs, lazy stream programming, and manipulation of linear digital assets as\nused in smart contracts.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733554
  },
  {
    "title": "Choreographies as Macros",
    "abstract": "Concurrent programming often entails meticulous pairing of sends and receives\nbetween participants to avoid deadlock. Choreographic programming alleviates\nthis burden by specifying the system as a single program. However, there are\nmore applications than implementations of choreographies, and developing new\nimplementations takes a lot of time and effort. Our work uses Racket to\nexpedite building a new choreographic language called Choret. Racket has a\npowerful macro system which allows Choret to reuse much of its infrastructure\nfor greater functionality and correctness.",
    "text": "Choreographies as Macros Concurrent programming often entails meticulous pairing of sends and receives\nbetween participants to avoid deadlock. Choreographic programming alleviates\nthis burden by specifying the system as a single program. However, there are\nmore applications than implementations of choreographies, and developing new\nimplementations takes a lot of time and effort. Our work uses Racket to\nexpedite building a new choreographic language called Choret. Racket has a\npowerful macro system which allows Choret to reuse much of its infrastructure\nfor greater functionality and correctness.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733563
  },
  {
    "title": "INTERLEAVE: A Faster Symbolic Algorithm for Maximal End Component\n  Decomposition",
    "abstract": "This paper presents a novel symbolic algorithm for the Maximal End Component\n(MEC) decomposition of a Markov Decision Process (MDP). The key idea behind our\nalgorithm INTERLEAVE is to interleave the computation of Strongly Connected\nComponents (SCCs) with eager elimination of redundant state-action pairs,\nrather than performing these computations sequentially as done by existing\nstate-of-the-art algorithms. Even though our approach has the same complexity\nas prior works, an empirical evaluation of INTERLEAVE on the standardized\nQuantitative Verification Benchmark Set demonstrates that it solves 19 more\nbenchmarks (out of 379) than the closest previous algorithm. On the 149\nbenchmarks that prior approaches can solve, we demonstrate a 3.81x average\nspeedup in runtime.",
    "text": "INTERLEAVE: A Faster Symbolic Algorithm for Maximal End Component\n  Decomposition This paper presents a novel symbolic algorithm for the Maximal End Component\n(MEC) decomposition of a Markov Decision Process (MDP). The key idea behind our\nalgorithm INTERLEAVE is to interleave the computation of Strongly Connected\nComponents (SCCs) with eager elimination of redundant state-action pairs,\nrather than performing these computations sequentially as done by existing\nstate-of-the-art algorithms. Even though our approach has the same complexity\nas prior works, an empirical evaluation of INTERLEAVE on the standardized\nQuantitative Verification Benchmark Set demonstrates that it solves 19 more\nbenchmarks (out of 379) than the closest previous algorithm. On the 149\nbenchmarks that prior approaches can solve, we demonstrate a 3.81x average\nspeedup in runtime.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7335732
  },
  {
    "title": "GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency",
    "abstract": "GPU computing is embracing weak memory concurrency for performance\nimprovement. However, compared to CPUs, modern GPUs provide more fine-grained\nconcurrency features such as scopes, have additional properties like\ndivergence, and thereby follow different weak memory consistency models. These\nfeatures and properties make concurrent programming on GPUs more complex and\nerror-prone. To this end, we present GPUMC, a stateless model checker to check\nthe correctness of GPU shared-memory concurrent programs under scoped-RC11 weak\nmemory concurrency model. GPUMC explores all possible executions in GPU\nprograms to reveal various errors - races, barrier divergence, and assertion\nviolations. In addition, GPUMC also automatically repairs these errors in the\nappropriate cases.\n  We evaluate GPUMC with benchmarks and real-life GPU programs. GPUMC is\nefficient both in time and memory in verifying large GPU programs where\nstate-of-the-art tools are timed out. In addition, GPUMC identifies all known\nerrors in these benchmarks compared to the state-of-the-art tools.",
    "text": "GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency GPU computing is embracing weak memory concurrency for performance\nimprovement. However, compared to CPUs, modern GPUs provide more fine-grained\nconcurrency features such as scopes, have additional properties like\ndivergence, and thereby follow different weak memory consistency models. These\nfeatures and properties make concurrent programming on GPUs more complex and\nerror-prone. To this end, we present GPUMC, a stateless model checker to check\nthe correctness of GPU shared-memory concurrent programs under scoped-RC11 weak\nmemory concurrency model. GPUMC explores all possible executions in GPU\nprograms to reveal various errors - races, barrier divergence, and assertion\nviolations. In addition, GPUMC also automatically repairs these errors in the\nappropriate cases.\n  We evaluate GPUMC with benchmarks and real-life GPU programs. GPUMC is\nefficient both in time and memory in verifying large GPU programs where\nstate-of-the-art tools are timed out. In addition, GPUMC identifies all known\nerrors in these benchmarks compared to the state-of-the-art tools.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7335832
  },
  {
    "title": "LEGO-Compiler: Enhancing Neural Compilation Through Translation\n  Composability",
    "abstract": "Large language models (LLMs) have the potential to revolutionize how we\ndesign and implement compilers and code translation tools. However, existing\nLLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,\na novel neural compilation system that leverages LLMs to translate high-level\nlanguages into assembly code. Our approach centers on three key innovations:\nLEGO translation, which decomposes the input program into manageable blocks;\nbreaking down the complex compilation process into smaller, simpler verifiable\nsteps by organizing it as a verifiable LLM workflow by external tests; and a\nfeedback mechanism for self-correction. Supported by formal proofs of\ntranslation composability, LEGO-Compiler demonstrates high accuracy on multiple\ndatasets, including over 99% on ExeBench and 97.9% on industrial-grade\nAnsiBench. Additionally, LEGO-Compiler has also acheived near one\norder-of-magnitude improvement on compilable code size scalability. This work\nopens new avenues for applying LLMs to system-level tasks, complementing\ntraditional compiler technologies.",
    "text": "LEGO-Compiler: Enhancing Neural Compilation Through Translation\n  Composability Large language models (LLMs) have the potential to revolutionize how we\ndesign and implement compilers and code translation tools. However, existing\nLLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,\na novel neural compilation system that leverages LLMs to translate high-level\nlanguages into assembly code. Our approach centers on three key innovations:\nLEGO translation, which decomposes the input program into manageable blocks;\nbreaking down the complex compilation process into smaller, simpler verifiable\nsteps by organizing it as a verifiable LLM workflow by external tests; and a\nfeedback mechanism for self-correction. Supported by formal proofs of\ntranslation composability, LEGO-Compiler demonstrates high accuracy on multiple\ndatasets, including over 99% on ExeBench and 97.9% on industrial-grade\nAnsiBench. Additionally, LEGO-Compiler has also acheived near one\norder-of-magnitude improvement on compilable code size scalability. This work\nopens new avenues for applying LLMs to system-level tasks, complementing\ntraditional compiler technologies.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733592
  },
  {
    "title": "Proceedings 16th International Workshop on Programming Language\n  Approaches to Concurrency and Communication-cEntric Software",
    "abstract": "This volume contains the proceedings of PLACES 2025, the 16th edition of the\nWorkshop on Programming Language Approaches to Concurrency and\nCommunication-cEntric Software. The workshop is scheduled to take place in\nHamilton, Canada, on May 4, 2025, as a satellite event of ETAPS, the European\nJoint Conferences on Theory and Practice of Software. PLACES offers a forum for\nexchanging new ideas on how to address the challenges of concurrent and\ndistributed programming and how to improve the foundations of modern and future\ncomputer applications. PLACES welcomes researchers from various fields, and its\ntopics include the design of new programming languages, models for concurrent\nand distributed systems, type systems, program verification, and applications\nin various areas (e.g., microservices, sensor networks, blockchains, event\nprocessing, business process management).",
    "text": "Proceedings 16th International Workshop on Programming Language\n  Approaches to Concurrency and Communication-cEntric Software This volume contains the proceedings of PLACES 2025, the 16th edition of the\nWorkshop on Programming Language Approaches to Concurrency and\nCommunication-cEntric Software. The workshop is scheduled to take place in\nHamilton, Canada, on May 4, 2025, as a satellite event of ETAPS, the European\nJoint Conferences on Theory and Practice of Software. PLACES offers a forum for\nexchanging new ideas on how to address the challenges of concurrent and\ndistributed programming and how to improve the foundations of modern and future\ncomputer applications. PLACES welcomes researchers from various fields, and its\ntopics include the design of new programming languages, models for concurrent\nand distributed systems, type systems, program verification, and applications\nin various areas (e.g., microservices, sensor networks, blockchains, event\nprocessing, business process management).",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7336042
  },
  {
    "title": "Automated Verification of Monotonic Data Structure Traversals in C",
    "abstract": "Bespoke data structure operations are common in real-world C code. We\nidentify one common subclass, monotonic data structure traversals (MDSTs), that\niterate monotonically through the structure. For example, strlen iterates from\nstart to end of a character array until a null byte is found, and a binary\nsearch tree insert iterates from the tree root towards a leaf. We describe a\nnew automated verification tool, Shrinker, to verify MDSTs written in C.\nShrinker uses a new program analysis strategy called scapegoating size descent,\nwhich is designed to take advantage of the fact that many MDSTs produce very\nsimilar traces when executed on an input (e.g., some large list) as when\nexecuted on a 'shrunk' version of the input (e.g., the same list but with its\nfirst element deleted). We introduce a new benchmark set containing over one\nhundred instances proving correctness, equivalence, and memory safety\nproperties of dozens of MDSTs found in major C codebases including Linux,\nNetBSD, OpenBSD, QEMU, Git, and Musl. Shrinker significantly increases the\nnumber of monotonic string and list traversals that can be verified vs. a\nportfolio of state-of-the-art tools.",
    "text": "Automated Verification of Monotonic Data Structure Traversals in C Bespoke data structure operations are common in real-world C code. We\nidentify one common subclass, monotonic data structure traversals (MDSTs), that\niterate monotonically through the structure. For example, strlen iterates from\nstart to end of a character array until a null byte is found, and a binary\nsearch tree insert iterates from the tree root towards a leaf. We describe a\nnew automated verification tool, Shrinker, to verify MDSTs written in C.\nShrinker uses a new program analysis strategy called scapegoating size descent,\nwhich is designed to take advantage of the fact that many MDSTs produce very\nsimilar traces when executed on an input (e.g., some large list) as when\nexecuted on a 'shrunk' version of the input (e.g., the same list but with its\nfirst element deleted). We introduce a new benchmark set containing over one\nhundred instances proving correctness, equivalence, and memory safety\nproperties of dozens of MDSTs found in major C codebases including Linux,\nNetBSD, OpenBSD, QEMU, Git, and Musl. Shrinker significantly increases the\nnumber of monotonic string and list traversals that can be verified vs. a\nportfolio of state-of-the-art tools.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733614
  },
  {
    "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators",
    "abstract": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget.",
    "text": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733624
  },
  {
    "title": "On the Complexity of Checking Mixed Isolation Levels for SQL\n  Transactions",
    "abstract": "Concurrent accesses to databases are typically grouped in transactions which\ndefine units of work that should be isolated from other concurrent computations\nand resilient to failures. Modern databases provide different levels of\nisolation for transactions that correspond to different trade-offs between\nconsistency and throughput. Quite often, an application can use transactions\nwith different isolation levels at the same time. In this work, we investigate\nthe problem of testing isolation level implementations in databases, i.e.,\nchecking whether a given execution composed of multiple transactions adheres to\nthe prescribed isolation level semantics. We particularly focus on transactions\nformed of SQL queries and the use of multiple isolation levels at the same\ntime. We show that many restrictions of this problem are NP-complete and\nprovide an algorithm which is exponential-time in the worst-case,\npolynomial-time in relevant cases, and practically efficient.",
    "text": "On the Complexity of Checking Mixed Isolation Levels for SQL\n  Transactions Concurrent accesses to databases are typically grouped in transactions which\ndefine units of work that should be isolated from other concurrent computations\nand resilient to failures. Modern databases provide different levels of\nisolation for transactions that correspond to different trade-offs between\nconsistency and throughput. Quite often, an application can use transactions\nwith different isolation levels at the same time. In this work, we investigate\nthe problem of testing isolation level implementations in databases, i.e.,\nchecking whether a given execution composed of multiple transactions adheres to\nthe prescribed isolation level semantics. We particularly focus on transactions\nformed of SQL queries and the use of multiple isolation levels at the same\ntime. We show that many restrictions of this problem are NP-complete and\nprovide an algorithm which is exponential-time in the worst-case,\npolynomial-time in relevant cases, and practically efficient.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733633
  },
  {
    "title": "ZeroML: A Next Generation AutoML Language",
    "abstract": "ZeroML is a new generation programming language for AutoML to drive the ML\npipeline in a compiled and multi-paradigm way, with a pure functional core.\nMeeting the shortcomings introduced by Python, R, or Julia such as slow-running\ntime, brittle pipelines or high dependency cost ZeroML brings the\nMicroservices-based architecture adding the modular, reusable pieces such as\nDataCleaner, FeatureEngineer or ModelSelector. As a native multithread and\nmemory-aware search optimized toolkit, and with one command deployability\nability, ZeroML ensures non-coders and ML professionals to create high-accuracy\nmodels super fast and in a more reproducible way. The verbosity of the language\nensures that when it comes to dropping into the backend, the code we will be\ncreating is extremely clear but the level of repetition and boilerplate\nrequired when developing on the front end is now removed.",
    "text": "ZeroML: A Next Generation AutoML Language ZeroML is a new generation programming language for AutoML to drive the ML\npipeline in a compiled and multi-paradigm way, with a pure functional core.\nMeeting the shortcomings introduced by Python, R, or Julia such as slow-running\ntime, brittle pipelines or high dependency cost ZeroML brings the\nMicroservices-based architecture adding the modular, reusable pieces such as\nDataCleaner, FeatureEngineer or ModelSelector. As a native multithread and\nmemory-aware search optimized toolkit, and with one command deployability\nability, ZeroML ensures non-coders and ML professionals to create high-accuracy\nmodels super fast and in a more reproducible way. The verbosity of the language\nensures that when it comes to dropping into the backend, the code we will be\ncreating is extremely clear but the level of repetition and boilerplate\nrequired when developing on the front end is now removed.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733643
  },
  {
    "title": "Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces",
    "abstract": "Automatic program repair seeks to generate correct code from buggy programs,\nwith most approaches searching the correct program in a discrete, symbolic\nspace of source code tokens. This symbolic search is fundamentally limited by\nits inability to directly reason about program behavior. We introduce\nGradient-Based Program Repair (GBPR), a new paradigm that reframes program\nrepair as continuous optimization in a differentiable numerical program space.\nOur core insight is to compile symbolic programs into differentiable numerical\nrepresentations, enabling search in the numerical program space directly guided\nby program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of\n1,466 buggy symbolic RASP programs and their respective numerical\nrepresentations. Our experiments demonstrate that GBPR can effectively repair\nbuggy symbolic programs by gradient-based optimization in the numerical program\nspace, with convincing repair trajectories. To our knowledge, we are the first\nto state program repair as continuous optimization in a numerical program\nspace. Our work establishes a new direction for program repair research,\nbridging two rich worlds: continuous optimization and program behavior.",
    "text": "Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces Automatic program repair seeks to generate correct code from buggy programs,\nwith most approaches searching the correct program in a discrete, symbolic\nspace of source code tokens. This symbolic search is fundamentally limited by\nits inability to directly reason about program behavior. We introduce\nGradient-Based Program Repair (GBPR), a new paradigm that reframes program\nrepair as continuous optimization in a differentiable numerical program space.\nOur core insight is to compile symbolic programs into differentiable numerical\nrepresentations, enabling search in the numerical program space directly guided\nby program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of\n1,466 buggy symbolic RASP programs and their respective numerical\nrepresentations. Our experiments demonstrate that GBPR can effectively repair\nbuggy symbolic programs by gradient-based optimization in the numerical program\nspace, with convincing repair trajectories. To our knowledge, we are the first\nto state program repair as continuous optimization in a numerical program\nspace. Our work establishes a new direction for program repair research,\nbridging two rich worlds: continuous optimization and program behavior.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7336519
  },
  {
    "title": "Asynchronous Global Protocols, Precisely: Full Proofs",
    "abstract": "Asynchronous multiparty session types are a type-based framework that ensures\nthe compatibility of components in a distributed system by specifying a global\nprotocol. Each component can be independently developed and refined locally,\nbefore being integrated into a larger system, leading to higher quality\ndistributed software. This paper studies the interplay between global protocols\nand an asynchronous refinement relation, precise asynchronous multiparty\nsubtyping. This subtyping relation locally optimises asynchronous messaging,\nenabling a permutation of two actions in a component while still preserving the\nsafety and liveness of the overall composed system. In this paper, we first\ndefine the asynchronous association between a global protocol and a set of\nlocal (optimised) specifications. We then prove the soundness and completeness\nof the operational correspondence of this asynchronous association. We\ndemonstrate that the association acts as an invariant to provide type\nsoundness, deadlock-freedom and liveness of a collection of components\noptimised from the end-point projections of a given global protocol.",
    "text": "Asynchronous Global Protocols, Precisely: Full Proofs Asynchronous multiparty session types are a type-based framework that ensures\nthe compatibility of components in a distributed system by specifying a global\nprotocol. Each component can be independently developed and refined locally,\nbefore being integrated into a larger system, leading to higher quality\ndistributed software. This paper studies the interplay between global protocols\nand an asynchronous refinement relation, precise asynchronous multiparty\nsubtyping. This subtyping relation locally optimises asynchronous messaging,\nenabling a permutation of two actions in a component while still preserving the\nsafety and liveness of the overall composed system. In this paper, we first\ndefine the asynchronous association between a global protocol and a set of\nlocal (optimised) specifications. We then prove the soundness and completeness\nof the operational correspondence of this asynchronous association. We\ndemonstrate that the association acts as an invariant to provide type\nsoundness, deadlock-freedom and liveness of a collection of components\noptimised from the end-point projections of a given global protocol.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733662
  },
  {
    "title": "Secure Parsing and Serializing with Separation Logic Applied to CBOR,\n  CDDL, and COSE",
    "abstract": "Incorrect handling of security-critical data formats, particularly in\nlow-level languages, are the root cause of many security vulnerabilities.\nProvably correct parsing and serialization tools that target languages like C\ncan help. Towards this end, we present PulseParse, a library of verified parser\nand serializer combinators for non-malleable binary formats. Specifications and\nproofs in PulseParse are in separation logic, offering a more abstract and\ncompositional interface, with full support for data validation, parsing, and\nserialization. PulseParse also supports a class of recursive formats -- with a\nfocus on security and handling adversarial inputs, we show how to parse such\nformats with only a constant amount of stack space.\n  We use PulseParse at scale by providing the first formalization of CBOR, a\nrecursive, binary data format standard, with growing adoption in various\nindustrial standards. We prove that the deterministic fragment of CBOR is\nnon-malleable and provide EverCBOR, a verified library in both C and Rust to\nvalidate, parse, and serialize CBOR objects implemented using PulseParse. Next,\nwe provide the first formalization of CDDL, a schema definition language for\nCBOR. We identify well-formedness conditions on CDDL definitions that ensure\nthat they yield unambiguous, non-malleable formats, and implement EverCDDL, a\ntool that checks that a CDDL definition is well-formed, and then produces\nverified parsers and serializers for it.\n  To evaluate our work, we use EverCDDL to generate verified parsers and\nserializers for various security-critical applications. Notably, we build a\nformally verified implementation of COSE signing, a standard for\ncryptographically signed objects. We also use our toolchain to generate\nverified code for other standards specified in CDDL, including DICE Protection\nEnvironment, a secure boot protocol standard.",
    "text": "Secure Parsing and Serializing with Separation Logic Applied to CBOR,\n  CDDL, and COSE Incorrect handling of security-critical data formats, particularly in\nlow-level languages, are the root cause of many security vulnerabilities.\nProvably correct parsing and serialization tools that target languages like C\ncan help. Towards this end, we present PulseParse, a library of verified parser\nand serializer combinators for non-malleable binary formats. Specifications and\nproofs in PulseParse are in separation logic, offering a more abstract and\ncompositional interface, with full support for data validation, parsing, and\nserialization. PulseParse also supports a class of recursive formats -- with a\nfocus on security and handling adversarial inputs, we show how to parse such\nformats with only a constant amount of stack space.\n  We use PulseParse at scale by providing the first formalization of CBOR, a\nrecursive, binary data format standard, with growing adoption in various\nindustrial standards. We prove that the deterministic fragment of CBOR is\nnon-malleable and provide EverCBOR, a verified library in both C and Rust to\nvalidate, parse, and serialize CBOR objects implemented using PulseParse. Next,\nwe provide the first formalization of CDDL, a schema definition language for\nCBOR. We identify well-formedness conditions on CDDL definitions that ensure\nthat they yield unambiguous, non-malleable formats, and implement EverCDDL, a\ntool that checks that a CDDL definition is well-formed, and then produces\nverified parsers and serializers for it.\n  To evaluate our work, we use EverCDDL to generate verified parsers and\nserializers for various security-critical applications. Notably, we build a\nformally verified implementation of COSE signing, a standard for\ncryptographically signed objects. We also use our toolchain to generate\nverified code for other standards specified in CDDL, including DICE Protection\nEnvironment, a secure boot protocol standard.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733674
  },
  {
    "title": "$$-Nets: Interaction-Based System for Optimal Parallel\n  $$-Reduction",
    "abstract": "I present a model of universal parallel computation called $\\Delta$-Nets, and\na method to translate $\\lambda$-terms into $\\Delta$-nets and back. Together,\nthe model and the method constitute an algorithm for optimal parallel\n$\\lambda$-reduction, solving the longstanding enigma with groundbreaking\nclarity. I show that the $\\lambda$-calculus can be understood as a projection\nof $\\Delta$-Nets -- one that severely restricts the structure of sharing, among\nother drawbacks. Unhindered by these restrictions, the $\\Delta$-Nets model\nopens the door to new highly parallel programming language implementations and\ncomputer architectures that are more efficient and performant than previously\npossible.",
    "text": "$$-Nets: Interaction-Based System for Optimal Parallel\n  $$-Reduction I present a model of universal parallel computation called $\\Delta$-Nets, and\na method to translate $\\lambda$-terms into $\\Delta$-nets and back. Together,\nthe model and the method constitute an algorithm for optimal parallel\n$\\lambda$-reduction, solving the longstanding enigma with groundbreaking\nclarity. I show that the $\\lambda$-calculus can be understood as a projection\nof $\\Delta$-Nets -- one that severely restricts the structure of sharing, among\nother drawbacks. Unhindered by these restrictions, the $\\Delta$-Nets model\nopens the door to new highly parallel programming language implementations and\ncomputer architectures that are more efficient and performant than previously\npossible.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7336829
  },
  {
    "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation",
    "abstract": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.",
    "text": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733693
  },
  {
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "abstract": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\n<--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The\ndataset comprises 70k verified code pairs across host and device, addressing a\ncritical gap in low-level GPU code portability. Leveraging this resource, we\ntrain the CASS family of domain-specific language models, achieving 95% source\ntranslation accuracy and 37.5% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85% of test cases, preserving\nruntime and memory behavior. To support rigorous evaluation, we introduce\nCASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth\nexecution. All data, models, and evaluation tools are released as open source\nto foster progress in GPU compiler tooling, binary compatibility, and\nLLM-guided hardware translation.",
    "text": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\n<--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The\ndataset comprises 70k verified code pairs across host and device, addressing a\ncritical gap in low-level GPU code portability. Leveraging this resource, we\ntrain the CASS family of domain-specific language models, achieving 95% source\ntranslation accuracy and 37.5% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85% of test cases, preserving\nruntime and memory behavior. To support rigorous evaluation, we introduce\nCASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth\nexecution. All data, models, and evaluation tools are released as open source\nto foster progress in GPU compiler tooling, binary compatibility, and\nLLM-guided hardware translation.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733702
  },
  {
    "title": "Can a domain-specific language improve program structure comprehension\n  of data pipelines? A mixed-methods study",
    "abstract": "In many application domains, domain-specific languages can allow domain\nexperts to contribute to collaborative projects more correctly and efficiently.\nTo do so, they must be able to understand program structure from reading\nexisting source code. With high-quality data becoming an increasingly important\nresource, the creation of data pipelines is an important application domain for\ndomain-specific languages. We execute a mixed-method study consisting of a\ncontrolled experiment and a follow-up descriptive survey among the participants\nto understand the effects of a domain-specific language on bottom-up program\nunderstanding and generate hypotheses for future research. During the\nexperiment, participants need the same time to solve program structure\ncomprehension tasks, but are significantly more correct when using the\ndomain-specific language. In the descriptive survey, participants describe\nreasons related to the programming language itself, such as a better pipeline\noverview, more enforced code structure, and a closer alignment to the mental\nmodel of a data pipeline. In addition, human factors such as less required\nprogramming experience and the ability to reuse experience from other data\nengineering tools are discussed. Based on these results, domain-specific\nlanguages are a promising tool for creating data pipelines that can increase\ncorrect understanding of program structure and lower barriers to entry for\ndomain experts. Open questions exist to make more informed implementation\ndecisions for domain-specific languages for data pipelines in the future.",
    "text": "Can a domain-specific language improve program structure comprehension\n  of data pipelines? A mixed-methods study In many application domains, domain-specific languages can allow domain\nexperts to contribute to collaborative projects more correctly and efficiently.\nTo do so, they must be able to understand program structure from reading\nexisting source code. With high-quality data becoming an increasingly important\nresource, the creation of data pipelines is an important application domain for\ndomain-specific languages. We execute a mixed-method study consisting of a\ncontrolled experiment and a follow-up descriptive survey among the participants\nto understand the effects of a domain-specific language on bottom-up program\nunderstanding and generate hypotheses for future research. During the\nexperiment, participants need the same time to solve program structure\ncomprehension tasks, but are significantly more correct when using the\ndomain-specific language. In the descriptive survey, participants describe\nreasons related to the programming language itself, such as a better pipeline\noverview, more enforced code structure, and a closer alignment to the mental\nmodel of a data pipeline. In addition, human factors such as less required\nprogramming experience and the ability to reuse experience from other data\nengineering tools are discussed. Based on these results, domain-specific\nlanguages are a promising tool for creating data pipelines that can increase\ncorrect understanding of program structure and lower barriers to entry for\ndomain experts. Open questions exist to make more informed implementation\ndecisions for domain-specific languages for data pipelines in the future.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733713
  },
  {
    "title": "AutoMCQ -- Automatically Generate Code Comprehension Questions using\n  GenAI",
    "abstract": "Students often do not fully understand the code they have written. This\nsometimes does not become evident until later in their education, which can\nmean it is harder to fix their incorrect knowledge or misunderstandings. In\naddition, being able to fully understand code is increasingly important in a\nworld where students have access to generative artificial intelligence (GenAI)\ntools, such as GitHub Copilot. One effective solution is to utilise code\ncomprehension questions, where a marker asks questions about a submission to\ngauge understanding, this can also have the side effect of helping to detect\nplagiarism. However, this approach is time consuming and can be difficult\nand/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for\nthe automatic generation of multiple-choice code comprehension questions. This\nis integrated with the CodeRunner automated assessment platform.",
    "text": "AutoMCQ -- Automatically Generate Code Comprehension Questions using\n  GenAI Students often do not fully understand the code they have written. This\nsometimes does not become evident until later in their education, which can\nmean it is harder to fix their incorrect knowledge or misunderstandings. In\naddition, being able to fully understand code is increasingly important in a\nworld where students have access to generative artificial intelligence (GenAI)\ntools, such as GitHub Copilot. One effective solution is to utilise code\ncomprehension questions, where a marker asks questions about a submission to\ngauge understanding, this can also have the side effect of helping to detect\nplagiarism. However, this approach is time consuming and can be difficult\nand/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for\nthe automatic generation of multiple-choice code comprehension questions. This\nis integrated with the CodeRunner automated assessment platform.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733723
  },
  {
    "title": "Data-driven Verification of Procedural Programs with Integer Arrays",
    "abstract": "We address the problem of verifying automatically procedural programs\nmanipulating parametric-size arrays of integers, encoded as a constrained Horn\nclauses solving problem. We propose a new algorithmic method for synthesizing\nloop invariants and procedure pre/post-conditions represented as universally\nquantified first-order formulas constraining the array elements and program\nvariables. We adopt a data-driven approach that extends the decision tree\nHorn-ICE framework to handle arrays. We provide a powerful learning technique\nbased on reducing a complex classification problem of vectors of integer arrays\nto a simpler classification problem of vectors of integers. The obtained\nclassifier is generalized to get universally quantified invariants and\nprocedure pre/post-conditions. We have implemented our method and shown its\nefficiency and competitiveness w.r.t. state-of-the-art tools on a significant\nbenchmark.",
    "text": "Data-driven Verification of Procedural Programs with Integer Arrays We address the problem of verifying automatically procedural programs\nmanipulating parametric-size arrays of integers, encoded as a constrained Horn\nclauses solving problem. We propose a new algorithmic method for synthesizing\nloop invariants and procedure pre/post-conditions represented as universally\nquantified first-order formulas constraining the array elements and program\nvariables. We adopt a data-driven approach that extends the decision tree\nHorn-ICE framework to handle arrays. We provide a powerful learning technique\nbased on reducing a complex classification problem of vectors of integer arrays\nto a simpler classification problem of vectors of integers. The obtained\nclassifier is generalized to get universally quantified invariants and\nprocedure pre/post-conditions. We have implemented our method and shown its\nefficiency and competitiveness w.r.t. state-of-the-art tools on a significant\nbenchmark.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733732
  },
  {
    "title": "Abstractions-of-Thought: Intermediate Representations for LLM Reasoning\n  in Hardware Design",
    "abstract": "Large language models (LLMs) have achieved impressive proficiency on logic\nand programming tasks, often rivaling expert-level performance. However,\ngenerating functionally correct hardware description language (HDL) code from\nnatural language specifications remains challenging, primarily in data-scarce\ndomains.\n  Therefore, we present Abstractions-of-Thought (AoT) - a training-free,\ninference-only prompting framework to mitigate misinterpretations and reasoning\npitfalls of LLMs through a series of task-based abstractions within the\nprompting procedure, assisting in the transition from high-level to low-level\nrepresentations of hardware. Furthermore, AoT consists of the following stages:\n(1) an LLM-based classification of hardware design patterns, (2) a structured\nintermediate representation (IR) to separate functional decomposition from code\nsyntax, and (3) a line-by-line pseudocode solution enabling a more direct\nmapping to the final Verilog implementation. Experimental results on the\nVerilogEval benchmark depict that AoT demonstrates improvements in\nfunctionality when applied to large non-reasoning models (such as GPT-4o,\noutperforming all baseline techniques (including 1-shot, Chain-of-Thought, and\nTree-of-Thought) while significantly reducing the generated tokens by 1.8-5.2x\ncompared to popular Tree-of-Thought prompting.",
    "text": "Abstractions-of-Thought: Intermediate Representations for LLM Reasoning\n  in Hardware Design Large language models (LLMs) have achieved impressive proficiency on logic\nand programming tasks, often rivaling expert-level performance. However,\ngenerating functionally correct hardware description language (HDL) code from\nnatural language specifications remains challenging, primarily in data-scarce\ndomains.\n  Therefore, we present Abstractions-of-Thought (AoT) - a training-free,\ninference-only prompting framework to mitigate misinterpretations and reasoning\npitfalls of LLMs through a series of task-based abstractions within the\nprompting procedure, assisting in the transition from high-level to low-level\nrepresentations of hardware. Furthermore, AoT consists of the following stages:\n(1) an LLM-based classification of hardware design patterns, (2) a structured\nintermediate representation (IR) to separate functional decomposition from code\nsyntax, and (3) a line-by-line pseudocode solution enabling a more direct\nmapping to the final Verilog implementation. Experimental results on the\nVerilogEval benchmark depict that AoT demonstrates improvements in\nfunctionality when applied to large non-reasoning models (such as GPT-4o,\noutperforming all baseline techniques (including 1-shot, Chain-of-Thought, and\nTree-of-Thought) while significantly reducing the generated tokens by 1.8-5.2x\ncompared to popular Tree-of-Thought prompting.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7337449
  },
  {
    "title": "Let's Take Esoteric Programming Languages Seriously",
    "abstract": "Esoteric programming languages are challenging to learn, but their unusual\nfeatures and constraints may serve to improve programming ability. From\nlanguages designed to be intentionally obtuse (e.g. INTERCAL) to others\ntargeting artistic expression (e.g. Piet) or exploring the nature of\ncomputation (e.g. Fractan), there is rich variety in the realm of esoteric\nprogramming languages. This essay examines the counterintuitive appeal of\nesoteric languages and seeks to analyse reasons for this popularity. We will\nexplore why people are attracted to esoteric languages in terms of (a) program\ncomprehension and construction, as well as (b) language design and\nimplementation. Our assertion is that esoteric languages can improve general PL\nawareness, at the same time as enabling the esoteric programmer to impress\ntheir peers with obscure knowledge. We will also consider pedagogic principles\nand the use of AI, in relation to esoteric languages. Emerging from the\nspecific discussion, we identify a general set of 'good' reasons for designing\nnew programming languages. It may not be possible to be exhaustive on this\ntopic, and it is certain we have not achieved that goal here. However we\nbelieve our most important contribution is to draw attention to the varied and\noften implicit motivations involved in programming language design.",
    "text": "Let's Take Esoteric Programming Languages Seriously Esoteric programming languages are challenging to learn, but their unusual\nfeatures and constraints may serve to improve programming ability. From\nlanguages designed to be intentionally obtuse (e.g. INTERCAL) to others\ntargeting artistic expression (e.g. Piet) or exploring the nature of\ncomputation (e.g. Fractan), there is rich variety in the realm of esoteric\nprogramming languages. This essay examines the counterintuitive appeal of\nesoteric languages and seeks to analyse reasons for this popularity. We will\nexplore why people are attracted to esoteric languages in terms of (a) program\ncomprehension and construction, as well as (b) language design and\nimplementation. Our assertion is that esoteric languages can improve general PL\nawareness, at the same time as enabling the esoteric programmer to impress\ntheir peers with obscure knowledge. We will also consider pedagogic principles\nand the use of AI, in relation to esoteric languages. Emerging from the\nspecific discussion, we identify a general set of 'good' reasons for designing\nnew programming languages. It may not be possible to be exhaustive on this\ntopic, and it is certain we have not achieved that goal here. However we\nbelieve our most important contribution is to draw attention to the varied and\noften implicit motivations involved in programming language design.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7337542
  },
  {
    "title": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code\n  Generation",
    "abstract": "We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.",
    "text": "SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code\n  Generation We introduce SIMCOPILOT, a benchmark that simulates the role of large\nlanguage models (LLMs) as interactive, \"copilot\"-style coding assistants.\nTargeting both completion (finishing incomplete methods or code blocks) and\ninfill tasks (filling missing segments within existing code), SIMCOPILOT\nprovides a comprehensive framework for evaluating LLM coding capabilities. The\nbenchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python\n(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our\nkey contributions include: (a) establishing a realistic, detailed evaluation\nenvironment to assess LLM utility in practical coding scenarios, and (b)\nproviding fine-grained analyses that address critical factors frequently\noverlooked by existing benchmarks, such as task-specific performance nuances,\ncontextual understanding across code segments, and sensitivity to variable\nscope. Evaluations conducted across domains-including algorithms, databases,\ncomputer vision, and neural networks-offer insights into model strengths and\nhighlight persistent challenges in maintaining logical consistency within\ncomplex dependency structures. Beyond benchmarking, our study sheds light on\nthe current limitations of LLM-driven code generation and underscores the\nongoing transition of LLMs from merely syntax-aware generators toward reliable,\nintelligent software development partners.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7337651
  },
  {
    "title": "Large Language Model-Powered Agent for C to Rust Code Translation",
    "abstract": "The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks.",
    "text": "Large Language Model-Powered Agent for C to Rust Code Translation The C programming language has been foundational in building system-level\nsoftware. However, its manual memory management model frequently leads to\nmemory safety issues. In response, a modern system programming language, Rust,\nhas emerged as a memory-safe alternative. Moreover, automating the C-to-Rust\ntranslation empowered by the rapid advancements of the generative capabilities\nof LLMs is gaining growing interest for large volumes of legacy C code. Despite\nsome success, existing LLM-based approaches have constrained the role of LLMs\nto static prompt-response behavior and have not explored their agentic\nproblem-solving capability. Applying the LLM agentic capability for the\nC-to-Rust translation introduces distinct challenges, as this task differs from\nthe traditional LLM agent applications, such as math or commonsense QA domains.\nFirst, the scarcity of parallel C-to-Rust datasets hinders the retrieval of\nsuitable code translation exemplars for in-context learning. Second, unlike\nmath or commonsense QA, the intermediate steps required for C-to-Rust are not\nwell-defined. Third, it remains unclear how to organize and cascade these\nintermediate steps to construct a correct translation trajectory. To address\nthese challenges in the C-to-Rust translation, we propose a novel intermediate\nstep, the Virtual Fuzzing-based equivalence Test (VFT), and an agentic planning\nframework, the LLM-powered Agent for C-to-Rust code translation (LAC2R). The\nVFT guides LLMs to identify input arguments that induce divergent behaviors\nbetween an original C function and its Rust counterpart and to generate\ninformative diagnoses to refine the unsafe Rust code. LAC2R uses the MCTS to\nsystematically organize the LLM-induced intermediate steps for correct\ntranslation. We experimentally demonstrated that LAC2R effectively conducts\nC-to-Rust translation on large-scale, real-world benchmarks.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7337759
  },
  {
    "title": "Unraveling the iterative CHAD",
    "abstract": "Combinatory Homomorphic Automatic Differentiation (CHAD) was originally\nformulated as a semantics-driven source transformation for reverse-mode AD in\ntotal programming languages. We extend this framework to partial languages with\nfeatures such as potentially non-terminating operations, real-valued\nconditionals, and iteration constructs like while-loops, while preserving\nCHAD's structure-preserving semantics principle. A key contribution is the\nintroduction of iteration-extensive indexed categories, which allow iteration\nin the base category to lift to parameterized initial algebras in the indexed\ncategory. This enables iteration to be interpreted in the Grothendieck\nconstruction of the target language in a principled way. The resulting fibred\niterative structure cleanly models iteration in the categorical semantics.\nConsequently, the extended CHAD transformation remains the unique\nstructure-preserving functor (an iterative Freyd category morphism) from the\nfreely generated iterative Freyd category of the source language to the\nGrothendieck construction of the target's syntactic semantics, mapping each\nprimitive operation to its derivative. We prove the correctness of this\ntransformation using the universal property of the source language's syntax,\nshowing that the transformed programs compute correct reverse-mode derivatives.\nOur development also contributes to understanding iteration constructs within\ndependently typed languages and categories of containers. As our primary\nmotivation and application, we generalize CHAD to languages with data types,\npartial features, and iteration, providing the first rigorous categorical\nsemantics for reverse-mode CHAD in such settings and formally guaranteeing the\ncorrectness of the source-to-source CHAD technique.",
    "text": "Unraveling the iterative CHAD Combinatory Homomorphic Automatic Differentiation (CHAD) was originally\nformulated as a semantics-driven source transformation for reverse-mode AD in\ntotal programming languages. We extend this framework to partial languages with\nfeatures such as potentially non-terminating operations, real-valued\nconditionals, and iteration constructs like while-loops, while preserving\nCHAD's structure-preserving semantics principle. A key contribution is the\nintroduction of iteration-extensive indexed categories, which allow iteration\nin the base category to lift to parameterized initial algebras in the indexed\ncategory. This enables iteration to be interpreted in the Grothendieck\nconstruction of the target language in a principled way. The resulting fibred\niterative structure cleanly models iteration in the categorical semantics.\nConsequently, the extended CHAD transformation remains the unique\nstructure-preserving functor (an iterative Freyd category morphism) from the\nfreely generated iterative Freyd category of the source language to the\nGrothendieck construction of the target's syntactic semantics, mapping each\nprimitive operation to its derivative. We prove the correctness of this\ntransformation using the universal property of the source language's syntax,\nshowing that the transformed programs compute correct reverse-mode derivatives.\nOur development also contributes to understanding iteration constructs within\ndependently typed languages and categories of containers. As our primary\nmotivation and application, we generalize CHAD to languages with data types,\npartial features, and iteration, providing the first rigorous categorical\nsemantics for reverse-mode CHAD in such settings and formally guaranteeing the\ncorrectness of the source-to-source CHAD technique.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733786
  },
  {
    "title": "Design and Evaluation of a Microservices Cloud Framework for Online\n  Travel Platforms",
    "abstract": "Handling online travel agents globally requires efficient and flexible\nsoftware solution architectures. When it needs to handle thousands of agents\nand billions of clients data globally. Microservices architecture is used to\nbreak down a large program into numerous, smaller services which can run\nindividually and perform individual tasks. This paper analyses and integrates a\nunique Microservices Cloud Framework designed to support Online Travel\nPlatforms (MCF-OTP). MCF-OTPs main goal is to increase the performance,\nflexibility, and maintenance of online travel platforms via cloud computing and\nmicroservice technologies. Large-scale travel apps, including managing numerous\ndata sources, dealing with traffic peaks, and providing fault tolerance, can be\naddressed by the suggested framework. The framework increases good\ninterpretation between flawless data synchronization, microservices, and\ndynamic scaling based on demand technology. An organization framework that\noptimizes service borders and minimizes inter-service dependencies is\nrecommended. Thus, this can result in elevated development adaptability. In\nthis research, the principal goal is to evaluate MCF-OTPs efficiency using the\nindicators of fault tolerance and response time. It is indicated by the\nfindings that the MCF-OTP structure excels traditional monolithic designs in\nterms of dependability and scalability, managing traffic spikes seamlessly and\ndecreasing downtime. The cost-effective analysis helps ascertain the net gain\nattained by the startup fees and the ongoing operational costs. The cloud-based\nenvironment is used to reduce the fracture cost which also helps to increase\nthe efficiency of resource allocation, according to the research.",
    "text": "Design and Evaluation of a Microservices Cloud Framework for Online\n  Travel Platforms Handling online travel agents globally requires efficient and flexible\nsoftware solution architectures. When it needs to handle thousands of agents\nand billions of clients data globally. Microservices architecture is used to\nbreak down a large program into numerous, smaller services which can run\nindividually and perform individual tasks. This paper analyses and integrates a\nunique Microservices Cloud Framework designed to support Online Travel\nPlatforms (MCF-OTP). MCF-OTPs main goal is to increase the performance,\nflexibility, and maintenance of online travel platforms via cloud computing and\nmicroservice technologies. Large-scale travel apps, including managing numerous\ndata sources, dealing with traffic peaks, and providing fault tolerance, can be\naddressed by the suggested framework. The framework increases good\ninterpretation between flawless data synchronization, microservices, and\ndynamic scaling based on demand technology. An organization framework that\noptimizes service borders and minimizes inter-service dependencies is\nrecommended. Thus, this can result in elevated development adaptability. In\nthis research, the principal goal is to evaluate MCF-OTPs efficiency using the\nindicators of fault tolerance and response time. It is indicated by the\nfindings that the MCF-OTP structure excels traditional monolithic designs in\nterms of dependability and scalability, managing traffic spikes seamlessly and\ndecreasing downtime. The cost-effective analysis helps ascertain the net gain\nattained by the startup fees and the ongoing operational costs. The cloud-based\nenvironment is used to reduce the fracture cost which also helps to increase\nthe efficiency of resource allocation, according to the research.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7337961
  },
  {
    "title": "From Reasoning to Code: GRPO Optimization for Underrepresented Languages",
    "abstract": "Generating accurate and executable code using large language models (LLMs) is\nchallenging for languages with limited public training data compared to popular\nlanguages such as Python. This paper introduces a generalizable approach that\nuses small-scale code versions of the Qwen 2.5 model combined with Group\nRelative Policy Optimization (GRPO) to enable effective code generation through\nexplicit reasoning steps, which is particularly beneficial for languages with\nsmaller source code databases. Using Prolog as a representative use case --\ngiven its limited online presence -- the initial model faced challenges in\ngenerating executable code. After some training steps, the model successfully\nproduces logically consistent and syntactically accurate code by directly\nintegrating reasoning-driven feedback into the reinforcement learning loop.\nExperimental evaluations using mathematical logic problem benchmarks illustrate\nsignificant improvements in reasoning quality, code accuracy, and logical\ncorrectness, underscoring the potential of this approach to benefit a wide\nrange of programming languages lacking extensive training resources.",
    "text": "From Reasoning to Code: GRPO Optimization for Underrepresented Languages Generating accurate and executable code using large language models (LLMs) is\nchallenging for languages with limited public training data compared to popular\nlanguages such as Python. This paper introduces a generalizable approach that\nuses small-scale code versions of the Qwen 2.5 model combined with Group\nRelative Policy Optimization (GRPO) to enable effective code generation through\nexplicit reasoning steps, which is particularly beneficial for languages with\nsmaller source code databases. Using Prolog as a representative use case --\ngiven its limited online presence -- the initial model faced challenges in\ngenerating executable code. After some training steps, the model successfully\nproduces logically consistent and syntactically accurate code by directly\nintegrating reasoning-driven feedback into the reinforcement learning loop.\nExperimental evaluations using mathematical logic problem benchmarks illustrate\nsignificant improvements in reasoning quality, code accuracy, and logical\ncorrectness, underscoring the potential of this approach to benefit a wide\nrange of programming languages lacking extensive training resources.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73382
  },
  {
    "title": "Augmented Weak Distance for Fast and Accurate Bounds Checking",
    "abstract": "This work advances floating-point program verification by introducing\nAugmented Weak-Distance (AWD), a principled extension of the Weak-Distance (WD)\nframework. WD is a recent approach that reformulates program analysis as a\nnumerical minimization problem, providing correctness guarantees through\nnon-negativity and zero-target correspondence. It consistently outperforms\ntraditional floating-point analysis, often achieving speedups of several orders\nof magnitude. However, WD suffers from ill-conditioned optimization landscapes\nand branching discontinuities, which significantly hinder its practical\neffectiveness. AWD overcomes these limitations with two key contributions.\nFirst, it enforces the Monotonic Convergence Condition (MCC), ensuring a\nstrictly decreasing objective function and mitigating misleading optimization\nstalls. Second, it extends WD with a per-path analysis scheme, preserving the\ncorrectness guarantees of weak-distance theory while integrating execution\npaths into the optimization process. These enhancements construct a\nwell-conditioned optimization landscape, enabling AWD to handle floating-point\nprograms effectively, even in the presence of loops and external functions. We\nevaluate AWD on SV-COMP 2024, a widely used benchmark for software\nverification.Across 40 benchmarks initially selected for bounds checking, AWD\nachieves 100% accuracy, matching the state-of-the-art bounded model checker\nCBMC, one of the most widely used verification tools, while running 170X faster\non average. In contrast, the static analysis tool Astr\\'ee, despite being fast,\nsolves only 17.5% of the benchmarks. These results establish AWD as a highly\nefficient alternative to CBMC for bounds checking, delivering precise\nfloating-point verification without compromising correctness.",
    "text": "Augmented Weak Distance for Fast and Accurate Bounds Checking This work advances floating-point program verification by introducing\nAugmented Weak-Distance (AWD), a principled extension of the Weak-Distance (WD)\nframework. WD is a recent approach that reformulates program analysis as a\nnumerical minimization problem, providing correctness guarantees through\nnon-negativity and zero-target correspondence. It consistently outperforms\ntraditional floating-point analysis, often achieving speedups of several orders\nof magnitude. However, WD suffers from ill-conditioned optimization landscapes\nand branching discontinuities, which significantly hinder its practical\neffectiveness. AWD overcomes these limitations with two key contributions.\nFirst, it enforces the Monotonic Convergence Condition (MCC), ensuring a\nstrictly decreasing objective function and mitigating misleading optimization\nstalls. Second, it extends WD with a per-path analysis scheme, preserving the\ncorrectness guarantees of weak-distance theory while integrating execution\npaths into the optimization process. These enhancements construct a\nwell-conditioned optimization landscape, enabling AWD to handle floating-point\nprograms effectively, even in the presence of loops and external functions. We\nevaluate AWD on SV-COMP 2024, a widely used benchmark for software\nverification.Across 40 benchmarks initially selected for bounds checking, AWD\nachieves 100% accuracy, matching the state-of-the-art bounded model checker\nCBMC, one of the most widely used verification tools, while running 170X faster\non average. In contrast, the static analysis tool Astr\\'ee, despite being fast,\nsolves only 17.5% of the benchmarks. These results establish AWD as a highly\nefficient alternative to CBMC for bounds checking, delivering precise\nfloating-point verification without compromising correctness.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733831
  },
  {
    "title": "Verifying Tree-Manipulating Programs via CHCs",
    "abstract": "Programs that manipulate tree-shaped data structures often require complex,\nspecialized proofs that are difficult to generalize and automate. This paper\nintroduces a unified, foundational approach to verifying such programs. Central\nto our approach is the knitted-tree encoding, modeling each program execution\nas a tree structure capturing input, output, and intermediate states.\nLeveraging the compositional nature of knitted-trees, we encode these\nstructures as constrained Horn clauses (CHCs), reducing verification to CHC\nsatisfiability task. To illustrate our approach, we focus on memory safety and\nshow how it naturally leads to simple, modular invariants.",
    "text": "Verifying Tree-Manipulating Programs via CHCs Programs that manipulate tree-shaped data structures often require complex,\nspecialized proofs that are difficult to generalize and automate. This paper\nintroduces a unified, foundational approach to verifying such programs. Central\nto our approach is the knitted-tree encoding, modeling each program execution\nas a tree structure capturing input, output, and intermediate states.\nLeveraging the compositional nature of knitted-trees, we encode these\nstructures as constrained Horn clauses (CHCs), reducing verification to CHC\nsatisfiability task. To illustrate our approach, we focus on memory safety and\nshow how it naturally leads to simple, modular invariants.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73384
  },
  {
    "title": "Transductively Informed Inductive Program Synthesis",
    "abstract": "Abstraction and reasoning in program synthesis has seen significant progress\nthrough both inductive and transductive paradigms. Inductive approaches\ngenerate a program or latent function from input-output examples, which can\nthen be applied to new inputs. Transductive approaches directly predict output\nvalues for given inputs, effectively serving as the function themselves.\nCurrent approaches combine inductive and transductive models via isolated\nensembling, but they do not explicitly model the interaction between both\nparadigms. In this work, we introduce \\acs{tiips}, a novel framework that\nunifies transductive and inductive strategies by explicitly modeling their\ninteractions through a cooperative mechanism: an inductive model generates\nprograms, while a transductive model constrains, guides, and refines the search\nto improve synthesis accuracy and generalization. We evaluate \\acs{tiips} on\ntwo widely studied program synthesis domains: string and list manipulation. Our\nresults show that \\acs{tiips} solves more tasks and yields functions that more\nclosely match optimal solutions in syntax and semantics, particularly in\nout-of-distribution settings, yielding state-of-the-art performance. We believe\nthat explicitly modeling the synergy between inductive and transductive\nreasoning opens promising avenues for general-purpose program synthesis and\nbroader applications.",
    "text": "Transductively Informed Inductive Program Synthesis Abstraction and reasoning in program synthesis has seen significant progress\nthrough both inductive and transductive paradigms. Inductive approaches\ngenerate a program or latent function from input-output examples, which can\nthen be applied to new inputs. Transductive approaches directly predict output\nvalues for given inputs, effectively serving as the function themselves.\nCurrent approaches combine inductive and transductive models via isolated\nensembling, but they do not explicitly model the interaction between both\nparadigms. In this work, we introduce \\acs{tiips}, a novel framework that\nunifies transductive and inductive strategies by explicitly modeling their\ninteractions through a cooperative mechanism: an inductive model generates\nprograms, while a transductive model constrains, guides, and refines the search\nto improve synthesis accuracy and generalization. We evaluate \\acs{tiips} on\ntwo widely studied program synthesis domains: string and list manipulation. Our\nresults show that \\acs{tiips} solves more tasks and yields functions that more\nclosely match optimal solutions in syntax and semantics, particularly in\nout-of-distribution settings, yielding state-of-the-art performance. We believe\nthat explicitly modeling the synergy between inductive and transductive\nreasoning opens promising avenues for general-purpose program synthesis and\nbroader applications.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.73385
  },
  {
    "title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation",
    "abstract": "We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).",
    "text": "CLEVER: A Curated Benchmark for Formally Verified Code Generation We introduce ${\\rm C{\\small LEVER}}$, a high-quality, curated benchmark of\n161 problems for end-to-end verified code generation in Lean. Each problem\nconsists of (1) the task of generating a specification that matches a held-out\nground-truth specification, and (2) the task of generating a Lean\nimplementation that provably satisfies this specification. Unlike prior\nbenchmarks, ${\\rm C{\\small LEVER}}$ avoids test-case supervision, LLM-generated\nannotations, and specifications that leak implementation logic or allow vacuous\nsolutions. All outputs are verified post-hoc using Lean's type checker to\nensure machine-checkable correctness. We use ${\\rm C{\\small LEVER}}$ to\nevaluate several few-shot and agentic approaches based on state-of-the-art\nlanguage models. These methods all struggle to achieve full verification,\nestablishing it as a challenging frontier benchmark for program synthesis and\nformal reasoning. Our benchmark can be found on\nGitHub(https://github.com/trishullab/clever) as well as\nHuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our\nevaluation code is also available\nonline(https://github.com/trishullab/clever-prover).",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733866
  },
  {
    "title": "Genesis: A Compiler Framework for Hamiltonian Simulation on Hybrid CV-DV\n  Quantum Computers",
    "abstract": "This paper introduces Genesis, the first compiler designed to support\nHamiltonian Simulation on hybrid continuous-variable (CV) and discrete-variable\n(DV) quantum computing systems. Genesis is a two-level compilation system. At\nthe first level, it decomposes an input Hamiltonian into basis gates using the\nnative instruction set of the target hybrid CV-DV quantum computer. At the\nsecond level, it tackles the mapping and routing of qumodes/qubits to implement\nlong-range interactions for the gates decomposed from the first level. Rather\nthan a typical implementation that relies on SWAP primitives similar to\nqubit-based (or DV-only) systems, we propose an integrated design of\nconnectivity-aware gate synthesis and beamsplitter SWAP insertion tailored for\nhybrid CV-DV systems. We also introduce an OpenQASM-like domain-specific\nlanguage (DSL) named CVDV-QASM to represent Hamiltonian in terms of\nPauli-exponentials and basic gate sequences from the hybrid CV-DV gate set.\nGenesis has successfully compiled several important Hamiltonians, including the\nBose-Hubbard model, $\\mathbb{Z}_2-$Higgs model, Hubbard-Holstein model,\nHeisenberg model and Electron-vibration coupling Hamiltonians, which are\ncritical in domains like quantum field theory, condensed matter physics, and\nquantum chemistry. Our implementation is available at\nGenesis-CVDV-Compiler(https://github.com/ruadapt/Genesis-CVDV-Compiler).",
    "text": "Genesis: A Compiler Framework for Hamiltonian Simulation on Hybrid CV-DV\n  Quantum Computers This paper introduces Genesis, the first compiler designed to support\nHamiltonian Simulation on hybrid continuous-variable (CV) and discrete-variable\n(DV) quantum computing systems. Genesis is a two-level compilation system. At\nthe first level, it decomposes an input Hamiltonian into basis gates using the\nnative instruction set of the target hybrid CV-DV quantum computer. At the\nsecond level, it tackles the mapping and routing of qumodes/qubits to implement\nlong-range interactions for the gates decomposed from the first level. Rather\nthan a typical implementation that relies on SWAP primitives similar to\nqubit-based (or DV-only) systems, we propose an integrated design of\nconnectivity-aware gate synthesis and beamsplitter SWAP insertion tailored for\nhybrid CV-DV systems. We also introduce an OpenQASM-like domain-specific\nlanguage (DSL) named CVDV-QASM to represent Hamiltonian in terms of\nPauli-exponentials and basic gate sequences from the hybrid CV-DV gate set.\nGenesis has successfully compiled several important Hamiltonians, including the\nBose-Hubbard model, $\\mathbb{Z}_2-$Higgs model, Hubbard-Holstein model,\nHeisenberg model and Electron-vibration coupling Hamiltonians, which are\ncritical in domains like quantum field theory, condensed matter physics, and\nquantum chemistry. Our implementation is available at\nGenesis-CVDV-Compiler(https://github.com/ruadapt/Genesis-CVDV-Compiler).",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733876
  },
  {
    "title": "NEAT: QCP: A Practical Separation Logic-based C Program Verification\n  Tool",
    "abstract": "As software systems increase in size and complexity dramatically, ensuring\ntheir correctness, security, and reliability becomes an increasingly formidable\nchallenge. Despite significant advancements in verification techniques and\ntools, there still remain %these tools still continue to encounter substantial\ndifficulties when applying these tools to complex, real-world scenarios. To\naddress these difficulties, this paper introduces a novel verification tool,\ncalled \\textbf{Qualified C Programming Verifier (QCP)}. QCP incorporates a\nrefined front-end %syntax of assertion language to enhance user interaction.\nThe proposed assertion language aims to %syntax is designed to lower the entry\nbarrier for verification tools, improve proof efficiency by improving\nautomation, and facilitate a deeper understanding of both the program and its\nverification results.",
    "text": "NEAT: QCP: A Practical Separation Logic-based C Program Verification\n  Tool As software systems increase in size and complexity dramatically, ensuring\ntheir correctness, security, and reliability becomes an increasingly formidable\nchallenge. Despite significant advancements in verification techniques and\ntools, there still remain %these tools still continue to encounter substantial\ndifficulties when applying these tools to complex, real-world scenarios. To\naddress these difficulties, this paper introduces a novel verification tool,\ncalled \\textbf{Qualified C Programming Verifier (QCP)}. QCP incorporates a\nrefined front-end %syntax of assertion language to enhance user interaction.\nThe proposed assertion language aims to %syntax is designed to lower the entry\nbarrier for verification tools, improve proof efficiency by improving\nautomation, and facilitate a deeper understanding of both the program and its\nverification results.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733886
  },
  {
    "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward",
    "abstract": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.",
    "text": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL\n  via Graph Matching and Stepwise Reward Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733896
  },
  {
    "title": "Nonmalleable Progress Leakage",
    "abstract": "Information-flow control systems often enforce progress-insensitive\nnoninterference, as it is simple to understand and enforce. Unfortunately, real\nprograms need to declassify results and endorse inputs, which noninterference\ndisallows, while preventing attackers from controlling leakage, including\nthrough progress channels, which progress-insensitivity ignores.\n  This work combines ideas for progress-sensitive security with secure\ndowngrading (declassification and endorsement) to identify a notion of securely\ndowngrading progress information. We use hyperproperties to distill the\nseparation between progress-sensitive and progress-insensitive noninterference\nand combine it with nonmalleable information flow, an existing\n(progress-insensitive) definition of secure downgrading, to define nonmalleable\nprogress leakage (NMPL). We present the first information-flow type system to\nallow some progress leakage while enforcing NMPL, and we show how to infer the\nlocation of secure progress downgrades. All theorems are verified in Rocq.",
    "text": "Nonmalleable Progress Leakage Information-flow control systems often enforce progress-insensitive\nnoninterference, as it is simple to understand and enforce. Unfortunately, real\nprograms need to declassify results and endorse inputs, which noninterference\ndisallows, while preventing attackers from controlling leakage, including\nthrough progress channels, which progress-insensitivity ignores.\n  This work combines ideas for progress-sensitive security with secure\ndowngrading (declassification and endorsement) to identify a notion of securely\ndowngrading progress information. We use hyperproperties to distill the\nseparation between progress-sensitive and progress-insensitive noninterference\nand combine it with nonmalleable information flow, an existing\n(progress-insensitive) definition of secure downgrading, to define nonmalleable\nprogress leakage (NMPL). We present the first information-flow type system to\nallow some progress leakage while enforcing NMPL, and we show how to infer the\nlocation of secure progress downgrades. All theorems are verified in Rocq.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7339048
  },
  {
    "title": "Introduction to Analytical Software Engineering Design Paradigm",
    "abstract": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics.",
    "text": "Introduction to Analytical Software Engineering Design Paradigm As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733921
  },
  {
    "title": "VeriReason: Reinforcement Learning with Testbench Feedback for\n  Reasoning-Enhanced Verilog Generation",
    "abstract": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason",
    "text": "VeriReason: Reinforcement Learning with Testbench Feedback for\n  Reasoning-Enhanced Verilog Generation Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733931
  },
  {
    "title": "VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning\n  and Formal Verification",
    "abstract": "This paper introduces VeriThoughts, a novel dataset designed for\nreasoning-based Verilog code generation. We establish a new benchmark framework\ngrounded in formal verification methods to evaluate the quality and correctness\nof generated hardware descriptions. Additionally, we present a suite of\nspecialized small-scale models optimized specifically for Verilog generation.\nOur work addresses the growing need for automated hardware design tools that\ncan produce verifiably correct implementations from high-level specifications,\npotentially accelerating the hardware development process while maintaining\nrigorous correctness guarantees. Our code and data are available at\n\\href{https://github.com/wilyub/VeriThoughts}{this URL}.",
    "text": "VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning\n  and Formal Verification This paper introduces VeriThoughts, a novel dataset designed for\nreasoning-based Verilog code generation. We establish a new benchmark framework\ngrounded in formal verification methods to evaluate the quality and correctness\nof generated hardware descriptions. Additionally, we present a suite of\nspecialized small-scale models optimized specifically for Verilog generation.\nOur work addresses the growing need for automated hardware design tools that\ncan produce verifiably correct implementations from high-level specifications,\npotentially accelerating the hardware development process while maintaining\nrigorous correctness guarantees. Our code and data are available at\n\\href{https://github.com/wilyub/VeriThoughts}{this URL}.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733941
  },
  {
    "title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning",
    "abstract": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
    "text": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733957
  },
  {
    "title": "eqsat: An Equality Saturation Dialect for Non-destructive Rewriting",
    "abstract": "With recent algorithmic improvements and easy-to-use libraries, equality\nsaturation is being picked up for hardware design, program synthesis, theorem\nproving, program optimization, and more. Existing work on using equality\nsaturation for program optimization makes use of external equality saturation\nlibraries such as egg, typically generating a single optimized expression. In\nthe context of a compiler, such an approach uses equality saturation to replace\na small number of passes. In this work, we propose an alternative approach that\nrepresents equality saturation natively in the compiler's intermediate\nrepresentation, facilitating the application of constructive compiler passes\nthat maintain the e-graph state throughout the compilation flow. We take LLVM's\nMLIR framework and propose a new MLIR dialect named eqsat that represents\ne-graphs in MLIR code. This not only provides opportunities to rethink\ne-matching and extraction techniques by orchestrating existing MLIR passes,\nsuch as common subexpression elimination, but also avoids translation overhead\nbetween the chosen e-graph library and MLIR. Our eqsat intermediate\nrepresentation (IR) allows programmers to apply equality saturation on\narbitrary domain-specific IRs using the same flow as other compiler\ntransformations in MLIR.",
    "text": "eqsat: An Equality Saturation Dialect for Non-destructive Rewriting With recent algorithmic improvements and easy-to-use libraries, equality\nsaturation is being picked up for hardware design, program synthesis, theorem\nproving, program optimization, and more. Existing work on using equality\nsaturation for program optimization makes use of external equality saturation\nlibraries such as egg, typically generating a single optimized expression. In\nthe context of a compiler, such an approach uses equality saturation to replace\na small number of passes. In this work, we propose an alternative approach that\nrepresents equality saturation natively in the compiler's intermediate\nrepresentation, facilitating the application of constructive compiler passes\nthat maintain the e-graph state throughout the compilation flow. We take LLVM's\nMLIR framework and propose a new MLIR dialect named eqsat that represents\ne-graphs in MLIR code. This not only provides opportunities to rethink\ne-matching and extraction techniques by orchestrating existing MLIR passes,\nsuch as common subexpression elimination, but also avoids translation overhead\nbetween the chosen e-graph library and MLIR. Our eqsat intermediate\nrepresentation (IR) allows programmers to apply equality saturation on\narbitrary domain-specific IRs using the same flow as other compiler\ntransformations in MLIR.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733967
  },
  {
    "title": "AI-Mediated Code Comment Improvement",
    "abstract": "This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.",
    "text": "AI-Mediated Code Comment Improvement This paper describes an approach to improve code comments along different\nquality axes by rewriting those comments with customized Artificial\nIntelligence (AI)-based tools. We conduct an empirical study followed by\ngrounded theory qualitative analysis to determine the quality axes to improve.\nThen we propose a procedure using a Large Language Model (LLM) to rewrite\nexisting code comments along the quality axes. We implement our procedure using\nGPT-4o, then distil the results into a smaller model capable of being run\nin-house, so users can maintain data custody. We evaluate both our approach\nusing GPT-4o and the distilled model versions. We show in an evaluation how our\nprocedure improves code comments along the quality axes. We release all data\nand source code in an online repository for reproducibility.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733977
  },
  {
    "title": "Comparing Parallel Functional Array Languages: Programming and\n  Performance",
    "abstract": "Parallel functional array languages are an emerging class of programming\nlanguages that promise to combine low-effort parallel programming with good\nperformance and performance portability. We systematically compare the designs\nand implementations of five different functional array languages: Accelerate,\nAPL, DaCe, Futhark, and SaC. We demonstrate the expressiveness of functional\narray programming by means of four challenging benchmarks, namely N-body\nsimulation, MultiGrid, Quickhull, and Flash Attention. These benchmarks\nrepresent a range of application domains and parallel computational models. We\nargue that the functional array code is much shorter and more comprehensible\nthan the hand-optimized baseline implementations because it omits\narchitecture-specific aspects. Instead, the language implementations generate\nboth multicore and GPU executables from a single source code base. Hence, we\nfurther argue that functional array code could more easily be ported to, and\noptimized for, new parallel architectures than conventional implementations of\nnumerical kernels. We demonstrate this potential by reporting the performance\nof the five parallel functional array languages on a total of 39 instances of\nthe four benchmarks on both a 32-core AMD EPYC 7313 multicore system and on an\nNVIDIA A30 GPU. We explore in-depth why each language performs well or not so\nwell on each benchmark and architecture. We argue that the results demonstrate\nthat mature functional array languages have the potential to deliver\nperformance competitive with the best available conventional techniques.",
    "text": "Comparing Parallel Functional Array Languages: Programming and\n  Performance Parallel functional array languages are an emerging class of programming\nlanguages that promise to combine low-effort parallel programming with good\nperformance and performance portability. We systematically compare the designs\nand implementations of five different functional array languages: Accelerate,\nAPL, DaCe, Futhark, and SaC. We demonstrate the expressiveness of functional\narray programming by means of four challenging benchmarks, namely N-body\nsimulation, MultiGrid, Quickhull, and Flash Attention. These benchmarks\nrepresent a range of application domains and parallel computational models. We\nargue that the functional array code is much shorter and more comprehensible\nthan the hand-optimized baseline implementations because it omits\narchitecture-specific aspects. Instead, the language implementations generate\nboth multicore and GPU executables from a single source code base. Hence, we\nfurther argue that functional array code could more easily be ported to, and\noptimized for, new parallel architectures than conventional implementations of\nnumerical kernels. We demonstrate this potential by reporting the performance\nof the five parallel functional array languages on a total of 39 instances of\nthe four benchmarks on both a 32-core AMD EPYC 7313 multicore system and on an\nNVIDIA A30 GPU. We explore in-depth why each language performs well or not so\nwell on each benchmark and architecture. We argue that the results demonstrate\nthat mature functional array languages have the potential to deliver\nperformance competitive with the best available conventional techniques.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733987
  },
  {
    "title": "D-Hammer: Efficient Equational Reasoning for Labelled Dirac Notation",
    "abstract": "Labelled Dirac notation is a formalism commonly used by physicists to\nrepresent many-body quantum systems and by computer scientists to assert\nproperties of quantum programs. It is supported by a rich equational theory for\nproving equality between expressions in the language. These proofs are\ntypically carried on pen-and-paper, and can be exceedingly long and\nerror-prone. We introduce D-Hammer, the first tool to support automated\nequational proof for labelled Dirac notation. The salient features of D-Hammer\ninclude: an expressive, higher-order, dependently-typed language for labelled\nDirac notation; an efficient normalization algorithm; and an optimized C++\nimplementation. We evaluate the implementation on representative examples from\nboth plain and labelled Dirac notation. In the case of plain Dirac notation, we\nshow that our implementation significantly outperforms DiracDec.",
    "text": "D-Hammer: Efficient Equational Reasoning for Labelled Dirac Notation Labelled Dirac notation is a formalism commonly used by physicists to\nrepresent many-body quantum systems and by computer scientists to assert\nproperties of quantum programs. It is supported by a rich equational theory for\nproving equality between expressions in the language. These proofs are\ntypically carried on pen-and-paper, and can be exceedingly long and\nerror-prone. We introduce D-Hammer, the first tool to support automated\nequational proof for labelled Dirac notation. The salient features of D-Hammer\ninclude: an expressive, higher-order, dependently-typed language for labelled\nDirac notation; an efficient normalization algorithm; and an optimized C++\nimplementation. We evaluate the implementation on representative examples from\nboth plain and labelled Dirac notation. In the case of plain Dirac notation, we\nshow that our implementation significantly outperforms DiracDec.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.733996
  },
  {
    "title": "Valida ISA Spec, version 1.0: A zk-Optimized Instruction Set\n  Architecture",
    "abstract": "The Valida instruction set architecture is designed for implementation in\nzkVMs to optimize for fast, efficient execution proving. This specification\nintends to guide implementors of zkVMs and compiler toolchains for Valida. It\nprovides an unambiguous definition of the semantics of Valida programs and may\nbe used as a starting point for formalization efforts.",
    "text": "Valida ISA Spec, version 1.0: A zk-Optimized Instruction Set\n  Architecture The Valida instruction set architecture is designed for implementation in\nzkVMs to optimize for fast, efficient execution proving. This specification\nintends to guide implementors of zkVMs and compiler toolchains for Valida. It\nprovides an unambiguous definition of the semantics of Valida programs and may\nbe used as a starting point for formalization efforts.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7340071
  },
  {
    "title": "LEGO: Layout Expression for Generating One-to-one Mapping",
    "abstract": "We describe LEGO, a new approach to optimizing data movement whereby code is\nexpressed as a layout-independent computation and composed with layouts for\ndata and computation. This code generator organization derives complex indexing\nexpressions associated with hierarchical parallel code and data movement for\nGPUs. LEGO maps from layout specification to indexing expressions, and can be\nintegrated into existing compilers and code templates. It facilitates the\nexploration of data layouts in combination with other optimizations. We\ndemonstrate LEGO's integration with the MLIR and Triton compilers, and with\nCUDA templates. We show that LEGO is capable of deriving performance\ncompetitive with Triton, and shows broad applicability in its integration with\nMLIR and CUDA.",
    "text": "LEGO: Layout Expression for Generating One-to-one Mapping We describe LEGO, a new approach to optimizing data movement whereby code is\nexpressed as a layout-independent computation and composed with layouts for\ndata and computation. This code generator organization derives complex indexing\nexpressions associated with hierarchical parallel code and data movement for\nGPUs. LEGO maps from layout specification to indexing expressions, and can be\nintegrated into existing compilers and code templates. It facilitates the\nexploration of data layouts in combination with other optimizations. We\ndemonstrate LEGO's integration with the MLIR and Triton compilers, and with\nCUDA templates. We show that LEGO is capable of deriving performance\ncompetitive with Triton, and shows broad applicability in its integration with\nMLIR and CUDA.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7340171
  },
  {
    "title": "Verified Purely Functional Catenable Real-Time Deques",
    "abstract": "We present OCaml and Rocq implementations of Kaplan and Tarjan's purely\nfunctional, real-time catenable deques. The correctness of our Rocq\nimplementation is machine-checked.",
    "text": "Verified Purely Functional Catenable Real-Time Deques We present OCaml and Rocq implementations of Kaplan and Tarjan's purely\nfunctional, real-time catenable deques. The correctness of our Rocq\nimplementation is machine-checked.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.734027
  },
  {
    "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended\n  Version)",
    "abstract": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice.",
    "text": "A Formally Verified Robustness Certifier for Neural Networks (Extended\n  Version) Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.734049
  },
  {
    "title": "RTL++: Graph-enhanced LLM for RTL Code Generation",
    "abstract": "As hardware design complexity escalates, there is an urgent need for advanced\nautomation in electronic design automation (EDA). Traditional register transfer\nlevel (RTL) design methods are manual, time-consuming, and prone to errors.\nWhile commercial (instruction-tuned) large language models (LLMs) shows\npromising performance for automation, they pose security and privacy concerns.\nOpen-source models offer alternatives; however, they frequently fall short in\nquality/correctness, largely due to limited, high-quality RTL code data\nessential for effective training and generalization. This paper proposes RTL++,\na first-of-its-kind LLM-assisted method for RTL code generation that utilizes\ngraph representations of code structures to enhance the quality of generated\ncode. By encoding RTL code into a textualized control flowgraphs (CFG) and data\nflow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and\nrelationships within the code. This structured graph-based approach enhances\nthe context available to LLMs, enabling them to better understand and generate\ninstructions. By focusing on data generation through graph representations,\nRTL++ addresses the limitations of previous approaches that rely solely on code\nand suffer from lack of diversity. Experimental results demonstrate that RTL++\noutperforms state-of-the-art models fine-tuned for RTL generation, as evaluated\nusing the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1\nmodel, which highlight the effectiveness of graph-enhanced context in advancing\nthe capabilities of LLM-assisted RTL code generation.",
    "text": "RTL++: Graph-enhanced LLM for RTL Code Generation As hardware design complexity escalates, there is an urgent need for advanced\nautomation in electronic design automation (EDA). Traditional register transfer\nlevel (RTL) design methods are manual, time-consuming, and prone to errors.\nWhile commercial (instruction-tuned) large language models (LLMs) shows\npromising performance for automation, they pose security and privacy concerns.\nOpen-source models offer alternatives; however, they frequently fall short in\nquality/correctness, largely due to limited, high-quality RTL code data\nessential for effective training and generalization. This paper proposes RTL++,\na first-of-its-kind LLM-assisted method for RTL code generation that utilizes\ngraph representations of code structures to enhance the quality of generated\ncode. By encoding RTL code into a textualized control flowgraphs (CFG) and data\nflow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and\nrelationships within the code. This structured graph-based approach enhances\nthe context available to LLMs, enabling them to better understand and generate\ninstructions. By focusing on data generation through graph representations,\nRTL++ addresses the limitations of previous approaches that rely solely on code\nand suffer from lack of diversity. Experimental results demonstrate that RTL++\noutperforms state-of-the-art models fine-tuned for RTL generation, as evaluated\nusing the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1\nmodel, which highlight the effectiveness of graph-enhanced context in advancing\nthe capabilities of LLM-assisted RTL code generation.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7340622
  },
  {
    "title": "An Extensive Study on Text Serialization Formats and Methods",
    "abstract": "Text serialization is a fundamental concept in modern computing, enabling the\nconversion of complex data structures into a format that can be easily stored,\ntransmitted, and reconstructed. This paper provides an extensive overview of\ntext serialization, exploring its importance, prevalent formats, underlying\nmethods, and comparative performance characteristics. We dive into the\nadvantages and disadvantages of various text-based serialization formats,\nincluding JSON, XML, YAML, and CSV, examining their structure, readability,\nverbosity, and suitability for different applications. The paper also discusses\nthe common methods involved in the serialization and deserialization processes,\nsuch as parsing techniques and the role of schemas. To illustrate the practical\nimplications of choosing a serialization format, we present hypothetical\nperformance results in the form of tables, comparing formats based on metrics\nlike serialization deserialization speed and resulting data size. The\ndiscussion analyzes these results, highlighting the trade offs involved in\nselecting a text serialization format for specific use cases. This work aims to\nprovide a comprehensive resource for understanding and applying text\nserialization in various computational domains.",
    "text": "An Extensive Study on Text Serialization Formats and Methods Text serialization is a fundamental concept in modern computing, enabling the\nconversion of complex data structures into a format that can be easily stored,\ntransmitted, and reconstructed. This paper provides an extensive overview of\ntext serialization, exploring its importance, prevalent formats, underlying\nmethods, and comparative performance characteristics. We dive into the\nadvantages and disadvantages of various text-based serialization formats,\nincluding JSON, XML, YAML, and CSV, examining their structure, readability,\nverbosity, and suitability for different applications. The paper also discusses\nthe common methods involved in the serialization and deserialization processes,\nsuch as parsing techniques and the role of schemas. To illustrate the practical\nimplications of choosing a serialization format, we present hypothetical\nperformance results in the form of tables, comparing formats based on metrics\nlike serialization deserialization speed and resulting data size. The\ndiscussion analyzes these results, highlighting the trade offs involved in\nselecting a text serialization format for specific use cases. This work aims to\nprovide a comprehensive resource for understanding and applying text\nserialization in various computational domains.",
    "url": "",
    "category": "cs.PL",
    "source": "arxiv",
    "timestamp": 1750404543.7340732
  }
]